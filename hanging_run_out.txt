(lingua_250731) jovyan@w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:~/shared/muchane/lingua-repo/Efficient-LLMs$ PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True torchrun --nproc-per-node 8 -m apps.main.train config=configs/distill_exps/debug.yaml
W0806 20:45:32.514000 1434340 site-packages/torch/distributed/run.py:793]
W0806 20:45:32.514000 1434340 site-packages/torch/distributed/run.py:793] *****************************************
W0806 20:45:32.514000 1434340 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
W0806 20:45:32.514000 1434340 site-packages/torch/distributed/run.py:793] *****************************************
Tensor parallelism has not been tested for a while, use at your own risk
3: WARNING 25-08-06 20:45:41.206607 - 0:00:00 - Signal handler installed.
3: WARNING 25-08-06 20:45:41.206607 - 0:00:00 - Signal handler installed.
3: WARNING 25-08-06 20:45:41.207171 - 0:00:00 - WARNING: Setting MKL_SERVICE_FORCE_INTEL to GNU
3: WARNING 25-08-06 20:45:41.207171 - 0:00:00 - WARNING: Setting MKL_SERVICE_FORCE_INTEL to GNU
3: WARNING 25-08-06 20:45:41.207275 - 0:00:00 - WARNING: Setting MKL_NUM_THREADS to 1
3: WARNING 25-08-06 20:45:41.207275 - 0:00:00 - WARNING: Setting MKL_NUM_THREADS to 1
3: WARNING 25-08-06 20:45:41.207345 - 0:00:00 - WARNING: Setting ENABLE_INTRA_NODE_COMM to 1
3: WARNING 25-08-06 20:45:41.207345 - 0:00:00 - WARNING: Setting ENABLE_INTRA_NODE_COMM to 1
3: WARNING 25-08-06 20:45:41.207416 - 0:00:00 - WARNING: Setting TORCH_NCCL_AVOID_RECORD_STREAMS to 1
3: WARNING 25-08-06 20:45:41.207416 - 0:00:00 - WARNING: Setting TORCH_NCCL_AVOID_RECORD_STREAMS to 1
3: WARNING 25-08-06 20:45:41.207468 - 0:00:00 - WARNING: Setting NCCL_IB_TIMEOUT to 22
3: WARNING 25-08-06 20:45:41.207468 - 0:00:00 - WARNING: Setting NCCL_IB_TIMEOUT to 22
3: WARNING 25-08-06 20:45:41.207517 - 0:00:00 - WARNING: Setting NCCL_DEBUG to INFO
3: WARNING 25-08-06 20:45:41.207517 - 0:00:00 - WARNING: Setting NCCL_DEBUG to INFO
3: WARNING 25-08-06 20:45:41.207566 - 0:00:00 - WARNING: Setting TRITON_CACHE_DIR to /tmp/tmpjgvu80xo
3: WARNING 25-08-06 20:45:41.207566 - 0:00:00 - WARNING: Setting TRITON_CACHE_DIR to /tmp/tmpjgvu80xo
Tensor parallelism has not been tested for a while, use at your own risk
2: WARNING 25-08-06 20:45:41.367750 - 0:00:00 - Signal handler installed.
2: WARNING 25-08-06 20:45:41.367750 - 0:00:00 - Signal handler installed.
2: WARNING 25-08-06 20:45:41.368311 - 0:00:00 - WARNING: Setting MKL_SERVICE_FORCE_INTEL to GNU
2: WARNING 25-08-06 20:45:41.368311 - 0:00:00 - WARNING: Setting MKL_SERVICE_FORCE_INTEL to GNU
2: WARNING 25-08-06 20:45:41.368401 - 0:00:00 - WARNING: Setting MKL_NUM_THREADS to 1
2: WARNING 25-08-06 20:45:41.368401 - 0:00:00 - WARNING: Setting MKL_NUM_THREADS to 1
2: WARNING 25-08-06 20:45:41.368468 - 0:00:00 - WARNING: Setting ENABLE_INTRA_NODE_COMM to 1
2: WARNING 25-08-06 20:45:41.368468 - 0:00:00 - WARNING: Setting ENABLE_INTRA_NODE_COMM to 1
2: WARNING 25-08-06 20:45:41.368539 - 0:00:00 - WARNING: Setting TORCH_NCCL_AVOID_RECORD_STREAMS to 1
2: WARNING 25-08-06 20:45:41.368539 - 0:00:00 - WARNING: Setting TORCH_NCCL_AVOID_RECORD_STREAMS to 1
Tensor parallelism has not been tested for a while, use at your own risk
6: WARNING 25-08-06 20:45:41.369378 - 0:00:00 - Signal handler installed.
6: WARNING 25-08-06 20:45:41.369378 - 0:00:00 - Signal handler installed.
6: WARNING 25-08-06 20:45:41.369929 - 0:00:00 - WARNING: Setting MKL_SERVICE_FORCE_INTEL to GNU
Tensor parallelism has not been tested for a while, use at your own risk
6: WARNING 25-08-06 20:45:41.369929 - 0:00:00 - WARNING: Setting MKL_SERVICE_FORCE_INTEL to GNU
6: WARNING 25-08-06 20:45:41.370013 - 0:00:00 - WARNING: Setting MKL_NUM_THREADS to 1
6: WARNING 25-08-06 20:45:41.370013 - 0:00:00 - WARNING: Setting MKL_NUM_THREADS to 1
6: WARNING 25-08-06 20:45:41.370078 - 0:00:00 - WARNING: Setting ENABLE_INTRA_NODE_COMM to 1
6: WARNING 25-08-06 20:45:41.370078 - 0:00:00 - WARNING: Setting ENABLE_INTRA_NODE_COMM to 1
6: WARNING 25-08-06 20:45:41.370145 - 0:00:00 - WARNING: Setting TORCH_NCCL_AVOID_RECORD_STREAMS to 1
6: WARNING 25-08-06 20:45:41.370145 - 0:00:00 - WARNING: Setting TORCH_NCCL_AVOID_RECORD_STREAMS to 1
6: WARNING 25-08-06 20:45:41.370194 - 0:00:00 - WARNING: Setting NCCL_IB_TIMEOUT to 22
6: WARNING 25-08-06 20:45:41.370194 - 0:00:00 - WARNING: Setting NCCL_IB_TIMEOUT to 22
6: WARNING 25-08-06 20:45:41.370240 - 0:00:00 - WARNING: Setting NCCL_DEBUG to INFO
6: WARNING 25-08-06 20:45:41.370240 - 0:00:00 - WARNING: Setting NCCL_DEBUG to INFO
6: WARNING 25-08-06 20:45:41.370286 - 0:00:00 - WARNING: Setting TRITON_CACHE_DIR to /tmp/tmp7eaf3qhc
6: WARNING 25-08-06 20:45:41.370286 - 0:00:00 - WARNING: Setting TRITON_CACHE_DIR to /tmp/tmp7eaf3qhc
7: WARNING 25-08-06 20:45:41.370254 - 0:00:00 - Signal handler installed.
7: WARNING 25-08-06 20:45:41.370254 - 0:00:00 - Signal handler installed.
2: WARNING 25-08-06 20:45:41.368589 - 0:00:00 - WARNING: Setting NCCL_IB_TIMEOUT to 22
2: WARNING 25-08-06 20:45:41.368589 - 0:00:00 - WARNING: Setting NCCL_IB_TIMEOUT to 22
7: WARNING 25-08-06 20:45:41.370877 - 0:00:00 - WARNING: Setting MKL_SERVICE_FORCE_INTEL to GNU
7: WARNING 25-08-06 20:45:41.370877 - 0:00:00 - WARNING: Setting MKL_SERVICE_FORCE_INTEL to GNU
2: WARNING 25-08-06 20:45:41.370894 - 0:00:00 - WARNING: Setting NCCL_DEBUG to INFO
2: WARNING 25-08-06 20:45:41.370894 - 0:00:00 - WARNING: Setting NCCL_DEBUG to INFO
7: WARNING 25-08-06 20:45:41.370963 - 0:00:00 - WARNING: Setting MKL_NUM_THREADS to 1
2: WARNING 25-08-06 20:45:41.370984 - 0:00:00 - WARNING: Setting TRITON_CACHE_DIR to /tmp/tmpru6t8knu
7: WARNING 25-08-06 20:45:41.370963 - 0:00:00 - WARNING: Setting MKL_NUM_THREADS to 1
2: WARNING 25-08-06 20:45:41.370984 - 0:00:00 - WARNING: Setting TRITON_CACHE_DIR to /tmp/tmpru6t8knu
7: WARNING 25-08-06 20:45:41.371037 - 0:00:00 - WARNING: Setting ENABLE_INTRA_NODE_COMM to 1
7: WARNING 25-08-06 20:45:41.371037 - 0:00:00 - WARNING: Setting ENABLE_INTRA_NODE_COMM to 1
7: WARNING 25-08-06 20:45:41.371107 - 0:00:00 - WARNING: Setting TORCH_NCCL_AVOID_RECORD_STREAMS to 1
7: WARNING 25-08-06 20:45:41.371107 - 0:00:00 - WARNING: Setting TORCH_NCCL_AVOID_RECORD_STREAMS to 1
7: WARNING 25-08-06 20:45:41.371156 - 0:00:00 - WARNING: Setting NCCL_IB_TIMEOUT to 22
7: WARNING 25-08-06 20:45:41.371156 - 0:00:00 - WARNING: Setting NCCL_IB_TIMEOUT to 22
7: WARNING 25-08-06 20:45:41.371203 - 0:00:00 - WARNING: Setting NCCL_DEBUG to INFO
7: WARNING 25-08-06 20:45:41.371203 - 0:00:00 - WARNING: Setting NCCL_DEBUG to INFO
7: WARNING 25-08-06 20:45:41.371250 - 0:00:00 - WARNING: Setting TRITON_CACHE_DIR to /tmp/tmpq0ir1vnd
7: WARNING 25-08-06 20:45:41.371250 - 0:00:00 - WARNING: Setting TRITON_CACHE_DIR to /tmp/tmpq0ir1vnd
Tensor parallelism has not been tested for a while, use at your own risk
4: WARNING 25-08-06 20:45:41.485168 - 0:00:00 - Signal handler installed.
4: WARNING 25-08-06 20:45:41.485168 - 0:00:00 - Signal handler installed.
4: WARNING 25-08-06 20:45:41.485754 - 0:00:00 - WARNING: Setting MKL_SERVICE_FORCE_INTEL to GNU
4: WARNING 25-08-06 20:45:41.485754 - 0:00:00 - WARNING: Setting MKL_SERVICE_FORCE_INTEL to GNU
4: WARNING 25-08-06 20:45:41.485848 - 0:00:00 - WARNING: Setting MKL_NUM_THREADS to 1
4: WARNING 25-08-06 20:45:41.485848 - 0:00:00 - WARNING: Setting MKL_NUM_THREADS to 1
4: WARNING 25-08-06 20:45:41.485915 - 0:00:00 - WARNING: Setting ENABLE_INTRA_NODE_COMM to 1
4: WARNING 25-08-06 20:45:41.485915 - 0:00:00 - WARNING: Setting ENABLE_INTRA_NODE_COMM to 1
4: WARNING 25-08-06 20:45:41.485986 - 0:00:00 - WARNING: Setting TORCH_NCCL_AVOID_RECORD_STREAMS to 1
Tensor parallelism has not been tested for a while, use at your own risk
4: WARNING 25-08-06 20:45:41.485986 - 0:00:00 - WARNING: Setting TORCH_NCCL_AVOID_RECORD_STREAMS to 1
4: WARNING 25-08-06 20:45:41.486038 - 0:00:00 - WARNING: Setting NCCL_IB_TIMEOUT to 22
4: WARNING 25-08-06 20:45:41.486038 - 0:00:00 - WARNING: Setting NCCL_IB_TIMEOUT to 22
4: WARNING 25-08-06 20:45:41.486086 - 0:00:00 - WARNING: Setting NCCL_DEBUG to INFO
4: WARNING 25-08-06 20:45:41.486086 - 0:00:00 - WARNING: Setting NCCL_DEBUG to INFO
4: WARNING 25-08-06 20:45:41.486135 - 0:00:00 - WARNING: Setting TRITON_CACHE_DIR to /tmp/tmp0aqrntuh
4: WARNING 25-08-06 20:45:41.486135 - 0:00:00 - WARNING: Setting TRITON_CACHE_DIR to /tmp/tmp0aqrntuh
1: WARNING 25-08-06 20:45:41.486252 - 0:00:00 - Signal handler installed.
1: WARNING 25-08-06 20:45:41.486252 - 0:00:00 - Signal handler installed.
1: WARNING 25-08-06 20:45:41.486720 - 0:00:00 - WARNING: Setting MKL_SERVICE_FORCE_INTEL to GNU
1: WARNING 25-08-06 20:45:41.486720 - 0:00:00 - WARNING: Setting MKL_SERVICE_FORCE_INTEL to GNU
1: WARNING 25-08-06 20:45:41.486807 - 0:00:00 - WARNING: Setting MKL_NUM_THREADS to 1
1: WARNING 25-08-06 20:45:41.486807 - 0:00:00 - WARNING: Setting MKL_NUM_THREADS to 1
1: WARNING 25-08-06 20:45:41.486874 - 0:00:00 - WARNING: Setting ENABLE_INTRA_NODE_COMM to 1
1: WARNING 25-08-06 20:45:41.486874 - 0:00:00 - WARNING: Setting ENABLE_INTRA_NODE_COMM to 1
1: WARNING 25-08-06 20:45:41.486944 - 0:00:00 - WARNING: Setting TORCH_NCCL_AVOID_RECORD_STREAMS to 1
1: WARNING 25-08-06 20:45:41.486944 - 0:00:00 - WARNING: Setting TORCH_NCCL_AVOID_RECORD_STREAMS to 1
1: WARNING 25-08-06 20:45:41.486994 - 0:00:00 - WARNING: Setting NCCL_IB_TIMEOUT to 22
1: WARNING 25-08-06 20:45:41.486994 - 0:00:00 - WARNING: Setting NCCL_IB_TIMEOUT to 22
1: WARNING 25-08-06 20:45:41.487041 - 0:00:00 - WARNING: Setting NCCL_DEBUG to INFO
1: WARNING 25-08-06 20:45:41.487041 - 0:00:00 - WARNING: Setting NCCL_DEBUG to INFO
1: WARNING 25-08-06 20:45:41.487089 - 0:00:00 - WARNING: Setting TRITON_CACHE_DIR to /tmp/tmpn3f14l7k
1: WARNING 25-08-06 20:45:41.487089 - 0:00:00 - WARNING: Setting TRITON_CACHE_DIR to /tmp/tmpn3f14l7k
Tensor parallelism has not been tested for a while, use at your own risk
0: WARNING 25-08-06 20:45:41.566849 - 0:00:00 - Signal handler installed.
0: WARNING 25-08-06 20:45:41.566849 - 0:00:00 - Signal handler installed.
0: WARNING 25-08-06 20:45:41.567256 - 0:00:00 - WARNING: Setting MKL_SERVICE_FORCE_INTEL to GNU
0: WARNING 25-08-06 20:45:41.567256 - 0:00:00 - WARNING: Setting MKL_SERVICE_FORCE_INTEL to GNU
0: WARNING 25-08-06 20:45:41.567365 - 0:00:00 - WARNING: Setting MKL_NUM_THREADS to 1
0: WARNING 25-08-06 20:45:41.567365 - 0:00:00 - WARNING: Setting MKL_NUM_THREADS to 1
0: WARNING 25-08-06 20:45:41.567457 - 0:00:00 - WARNING: Setting ENABLE_INTRA_NODE_COMM to 1
0: WARNING 25-08-06 20:45:41.567457 - 0:00:00 - WARNING: Setting ENABLE_INTRA_NODE_COMM to 1
0: WARNING 25-08-06 20:45:41.567533 - 0:00:00 - WARNING: Setting TORCH_NCCL_AVOID_RECORD_STREAMS to 1
0: WARNING 25-08-06 20:45:41.567533 - 0:00:00 - WARNING: Setting TORCH_NCCL_AVOID_RECORD_STREAMS to 1
0: WARNING 25-08-06 20:45:41.567596 - 0:00:00 - WARNING: Setting NCCL_IB_TIMEOUT to 22
0: WARNING 25-08-06 20:45:41.567596 - 0:00:00 - WARNING: Setting NCCL_IB_TIMEOUT to 22
0: WARNING 25-08-06 20:45:41.567671 - 0:00:00 - WARNING: Setting NCCL_DEBUG to INFO
0: WARNING 25-08-06 20:45:41.567671 - 0:00:00 - WARNING: Setting NCCL_DEBUG to INFO
0: WARNING 25-08-06 20:45:41.567736 - 0:00:00 - WARNING: Setting TRITON_CACHE_DIR to /tmp/tmpkrnyh_88
0: WARNING 25-08-06 20:45:41.567736 - 0:00:00 - WARNING: Setting TRITON_CACHE_DIR to /tmp/tmpkrnyh_88
Tensor parallelism has not been tested for a while, use at your own risk
5: WARNING 25-08-06 20:45:41.578719 - 0:00:00 - Signal handler installed.
5: WARNING 25-08-06 20:45:41.578719 - 0:00:00 - Signal handler installed.
5: WARNING 25-08-06 20:45:41.579181 - 0:00:00 - WARNING: Setting MKL_SERVICE_FORCE_INTEL to GNU
5: WARNING 25-08-06 20:45:41.579181 - 0:00:00 - WARNING: Setting MKL_SERVICE_FORCE_INTEL to GNU
5: WARNING 25-08-06 20:45:41.579267 - 0:00:00 - WARNING: Setting MKL_NUM_THREADS to 1
5: WARNING 25-08-06 20:45:41.579267 - 0:00:00 - WARNING: Setting MKL_NUM_THREADS to 1
5: WARNING 25-08-06 20:45:41.579335 - 0:00:00 - WARNING: Setting ENABLE_INTRA_NODE_COMM to 1
5: WARNING 25-08-06 20:45:41.579335 - 0:00:00 - WARNING: Setting ENABLE_INTRA_NODE_COMM to 1
5: WARNING 25-08-06 20:45:41.579406 - 0:00:00 - WARNING: Setting TORCH_NCCL_AVOID_RECORD_STREAMS to 1
5: WARNING 25-08-06 20:45:41.579406 - 0:00:00 - WARNING: Setting TORCH_NCCL_AVOID_RECORD_STREAMS to 1
5: WARNING 25-08-06 20:45:41.579457 - 0:00:00 - WARNING: Setting NCCL_IB_TIMEOUT to 22
5: WARNING 25-08-06 20:45:41.579457 - 0:00:00 - WARNING: Setting NCCL_IB_TIMEOUT to 22
5: WARNING 25-08-06 20:45:41.579507 - 0:00:00 - WARNING: Setting NCCL_DEBUG to INFO
5: WARNING 25-08-06 20:45:41.579507 - 0:00:00 - WARNING: Setting NCCL_DEBUG to INFO
5: WARNING 25-08-06 20:45:41.579557 - 0:00:00 - WARNING: Setting TRITON_CACHE_DIR to /tmp/tmp18iaamm4
5: WARNING 25-08-06 20:45:41.579557 - 0:00:00 - WARNING: Setting TRITON_CACHE_DIR to /tmp/tmp18iaamm4
7: INFO    25-08-06 20:45:50.047577 - 0:00:09 - Run launched with torchrun, local rank: 7
7: INFO    25-08-06 20:45:50.047816 - 0:00:09 - ENV: environ({'PYTORCH_CUDA_ALLOC_CONF': 'expandable_segments:True', 'SHELL': '/bin/bash', 'NV_LIBCUBLAS_VERSION': '12.1.0.26-1', 'NVIDIA_VISIBLE_DEVICES': 'GPU-508589a6-66b5-1624-f80f-e697085b4da0,GPU-01b43462-d9a5-ec99-23ae-a6508e99b4b4,GPU-4b906144-246f-1d19-58cd-374bacb12f0c,GPU-555361da-6a81-e45d-31bc-58230c88f8a4,GPU-00889c01-9780-9886-a803-e9a372d610b3,GPU-84db6b97-65f5-9b64-912f-d1901007b9a9,GPU-bd89a13d-0282-cdc9-aa4a-3a2d2ee67969,GPU-9a9f0354-8684-ec3a-f877-d8b160f9582e', 'KUBERNETES_SERVICE_PORT_HTTPS': '443', 'NV_CUDNN_PACKAGE_NAME': 'libcudnn8', 'KUBERNETES_SERVICE_PORT': '443', 'SATURN_USER': 'muchane', 'CONDA_EXE': '/opt/saturncloud/bin/conda', '_CE_M': '', 'BASE_URL': 'https://app.nvidia-oci.saturnenterprise.io', 'CONDA_BIN': '/opt/saturncloud/bin', 'HOSTNAME': 'w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v', 'SATURN_TOKEN': 'eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJhdWQiOlsiYXRsYXMiLCJzYXR1cm4tYXV0aC1wcm94eSJdLCJpc3MiOiJhdGxhcyIsInN1YiI6ImMwMzU4OWFkMTUyOTRjOTJhNmJlNDkwNDY3ZGEzMjVhIiwiaXNfcmVmcmVzaCI6ZmFsc2UsInJlc291cmNlIjoid29ya3NwYWNlOmE4NmVjYmI0MzFkOTQ4MTE4ZTU4M2U1YTllNmI4OWRmIiwic2NvcGUiOiJ3b3Jrc3BhY2U6YTg2ZWNiYjQzMWQ5NDgxMThlNTgzZTVhOWU2Yjg5ZGY6ZGFzazp3cml0ZSIsInVzZXJfaWQiOiIyZGI2MjEwOTc2Yjc0ZTA5YWY4Mzg0YjA1YjFkYjUzNSJ9.VQXvhHCsd23c6X9fbvu4pTmgmtes2i9V1DWJmdyNtJzOl3ym_Grz7DPVyKrzUcu0bjzFXic61JEkCj5NaYZgqFWeA7QnmPS1UvVR70PsAL_qiCaxj6Ns4Ot-RFH2MZ5kQSzfoK10woR3vY_dVzEmoBX3tfv25feAii-tOPLRjIRLBAgSDMyD9869EgneV4YVKVrL-JTnzKPh1y78KoolSM6C5xKBmARPBToswJuypI2XjGRQJWOgd8kvCTHcyi0y8KkhsiOr9VOXdjm9HPVfZKPX-z-3cNEf693H76I0hzrSIzoBQB8XslC6Cm1k66yJSViM4Ya-e7IbhHhzNt_BaZvwkGbuo0NgpqF-fQy8unPN3Uof2MxQxiiRTtcFNoY7kMkGGfV51COduS3Aive5yKwlMn9hX5qauBfznEB4NVzK2o9salob-DRcz7uahSrUrj17wHqWziOhOmKjy-MD-mi5DOoalQ52lU6h_p5O0mL9Vbdh_Ow_fcOANWaCW1Vgp-FLfLPqZmwURIL3k6NoP7rGi7bPHIpgyl7jqgUodaLtoMfvuIW_14VmEKmdAZixsHyTVV4hkKfPslHzC6ikb9itccuoOzxna94vUvXk5adJ-gHdEre38QUJvgu22fP59ANp1mJ3y4wO_isAeKHfurpkgRq-kUcNe7M6i9sW630', 'LANGUAGE': 'en_US.UTF-8', 'SATURN_RESOURCE_TYPE': 'SingleUserServer', 'SATURN_EMAIL': 'muchane@uchicago.edu', 'SATURN_SSH_ENABLED': 'true', 'SATURN_RESOURCE_NAME': 'lingua-run', 'NVIDIA_REQUIRE_CUDA': 'cuda>=12.1 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526', 'SATURN_IS_INTERNAL': 'true', 'SSH_AUTH_SOCK': '/tmp/ssh-XXXX1VrxrN/agent.1416665', 'NV_NVTX_VERSION': '12.1.66-1', 'NB_UID': '1000', 'DASK_DISTRIBUTED__DEPLOY__SCHEDULER_INFO_INTERVAL': '10s', 'NV_LIBCUSPARSE_VERSION': '12.0.2.55-1', 'NV_LIBNPP_VERSION': '12.0.2.50-1', 'NCCL_VERSION': '2.17.1-1', 'SATURN_BASE_URL': 'https://app.nvidia-oci.saturnenterprise.io', 'SATURN_REPO_HOME': '/home/jovyan', 'PWD': '/home/jovyan/shared/muchane/lingua-repo/Efficient-LLMs', 'LOGNAME': 'jovyan', 'NV_CUDNN_PACKAGE': 'libcudnn8=8.9.0.131-1+cuda12.1', 'CONDA_PREFIX': '/opt/saturncloud/envs/lingua_250731', 'KUBERNETES_PORT_12250_TCP_ADDR': '10.96.0.1', 'SATURN_ORG': 'AutomatingDataSciencewithTabularLanguag', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'SATURN_USER_PYTHON': '/opt/saturncloud/envs/saturn/bin/python', 'NV_LIBNPP_PACKAGE': 'libnpp-12-1=12.0.2.50-1', 'SATURN_JUPYTER_BASE_URL': 'http://lingua-8gpu-server.nvidia-oci.saturnenterprise.io', 'NVIDIA_PRODUCT_NAME': 'CUDA', 'SATURN_USERNAME': 'muchane', 'MOTD_SHOWN': 'pam', 'NV_CUDA_CUDART_VERSION': '12.1.55-1', 'HOME': '/home/jovyan', 'LANG': 'en_US.UTF-8', 'KUBERNETES_PORT_443_TCP': 'tcp://10.96.0.1:443', 'SATURN_SYSTEM_PYTHON': '/opt/saturncloud/bin/python', 'SATURN_GIT_SCRATCH_PATH': '/home/jovyan/.git-scratch', 'KUBERNETES_PORT_12250_TCP': 'tcp://10.96.0.1:12250', 'SATURN_PROJECT_NAME': 'lingua-run', 'NVIDIA_CUDA_END_OF_LIFE': '1', 'CUDA_VERSION': '12.1.0', 'BOKEH_ALLOW_WS_ORIGIN': 'lingua-8gpu-server.nvidia-oci.saturnenterprise.io', 'NV_LIBCUBLAS_PACKAGE': 'libcublas-12-1=12.1.0.26-1', 'CONDA_PROMPT_MODIFIER': '(lingua_250731) ', 'SSH_CONNECTION': '192.168.247.122 42216 192.168.45.217 22', 'NV_LIBCUBLAS_PACKAGE_NAME': 'libcublas-12-1', 'JUPYTER_IMAGE_SPEC': 'public.ecr.aws/saturncloud/saturn-python-pytorch:2024.08.01', 'DASK_DISTRIBUTED__DEPLOY__CLUSTER_REPR_INTERVAL': '10s', 'SATURN_VERSION': '2025.02.01-18', 'CONDA_OVERRIDE_CUDA': '12.1', 'TERM': 'xterm', '_CE_CONDA': '', 'KUBERNETES_PORT_12250_TCP_PORT': '12250', 'USER': 'jovyan', 'NV_CUDNN_VERSION': '8.9.0.131', 'CONDA_SHLVL': '1', 'KUBERNETES_SERVICE_PORT_PROXYMUX': '12250', 'SHLVL': '1', 'CONDA_DIR': '/opt/saturncloud', 'NV_CUDA_LIB_VERSION': '12.1.0-1', 'NVARCH': 'x86_64', 'KUBERNETES_PORT_443_TCP_PROTO': 'tcp', 'KUBERNETES_PORT_443_TCP_ADDR': '10.96.0.1', 'NV_CUDA_COMPAT_PACKAGE': 'cuda-compat-12-1', 'CONDA_PYTHON_EXE': '/opt/saturncloud/bin/python', 'NV_LIBNCCL_PACKAGE': 'libnccl2=2.17.1-1+cuda12.1', 'LD_LIBRARY_PATH': '/usr/local/nvidia/lib:/usr/local/nvidia/lib64', 'SATURN_JUPYTER_BASE_DOMAIN': 'lingua-8gpu-server.nvidia-oci.saturnenterprise.io', 'SATURN_IMAGE': 'public.ecr.aws/saturncloud/saturn-python-pytorch:2024.08.01', 'CONDA_INSTALL_DIR': '/opt/saturncloud', 'SSH_CLIENT': '192.168.247.122 42216 22', 'CONDA_DEFAULT_ENV': 'lingua_250731', 'SATURN_GROUP': '', 'SATURN_PYTHON_INIT_SCRIPT': '/home/jovyan/.saturn/setup_jupyterlab.py', 'NB_USER': 'jovyan', 'KUBERNETES_SERVICE_HOST': '10.96.0.1', 'LC_ALL': 'en_US.UTF-8', 'KUBERNETES_PORT': 'tcp://10.96.0.1:443', 'KUBERNETES_PORT_443_TCP_PORT': '443', 'PATH': '/opt/saturncloud/envs/lingua_250731/bin:/opt/saturncloud/condabin:/opt/saturncloud/envs/saturn/bin:/opt/saturncloud/bin:/home/jovyan/.local/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'NV_LIBNCCL_PACKAGE_NAME': 'libnccl2', 'NV_LIBNCCL_PACKAGE_VERSION': '2.17.1-1', 'NB_PYTHON_PREFIX': '/opt/saturncloud/envs/saturn', 'KUBERNETES_PORT_12250_TCP_PROTO': 'tcp', 'SSH_TTY': '/dev/pts/3', 'DEBIAN_FRONTEND': 'noninteractive', 'OLDPWD': '/home/jovyan', '_': '/opt/saturncloud/envs/lingua_250731/bin/torchrun', 'OMP_NUM_THREADS': '1', 'LOCAL_RANK': '7', 'RANK': '7', 'GROUP_RANK': '0', 'ROLE_RANK': '7', 'ROLE_NAME': 'default', 'LOCAL_WORLD_SIZE': '8', 'WORLD_SIZE': '8', 'GROUP_WORLD_SIZE': '1', 'ROLE_WORLD_SIZE': '8', 'MASTER_ADDR': '127.0.0.1', 'MASTER_PORT': '29500', 'TORCHELASTIC_RESTART_COUNT': '0', 'TORCHELASTIC_MAX_RESTARTS': '0', 'TORCHELASTIC_RUN_ID': 'none', 'TORCHELASTIC_USE_AGENT_STORE': 'True', 'TORCH_NCCL_ASYNC_ERROR_HANDLING': '1', 'TORCHELASTIC_ERROR_FILE': '/tmp/torchelastic__o4h9nnb/none_v6wepvnq/attempt_0/7/error.json', 'CUDA_MODULE_LOADING': 'LAZY', 'KMP_DUPLICATE_LIB_OK': 'True', 'KMP_INIT_AT_FORK': 'FALSE', 'MKL_SERVICE_FORCE_INTEL': 'GNU', 'MKL_NUM_THREADS': '1', 'ENABLE_INTRA_NODE_COMM': '1', 'TORCH_NCCL_AVOID_RECORD_STREAMS': '1', 'NCCL_IB_TIMEOUT': '22', 'NCCL_DEBUG': 'INFO', 'TRITON_CACHE_DIR': '/tmp/tmpq0ir1vnd'})
4: INFO    25-08-06 20:45:50.088411 - 0:00:09 - Run launched with torchrun, local rank: 4
4: INFO    25-08-06 20:45:50.088686 - 0:00:09 - ENV: environ({'PYTORCH_CUDA_ALLOC_CONF': 'expandable_segments:True', 'SHELL': '/bin/bash', 'NV_LIBCUBLAS_VERSION': '12.1.0.26-1', 'NVIDIA_VISIBLE_DEVICES': 'GPU-508589a6-66b5-1624-f80f-e697085b4da0,GPU-01b43462-d9a5-ec99-23ae-a6508e99b4b4,GPU-4b906144-246f-1d19-58cd-374bacb12f0c,GPU-555361da-6a81-e45d-31bc-58230c88f8a4,GPU-00889c01-9780-9886-a803-e9a372d610b3,GPU-84db6b97-65f5-9b64-912f-d1901007b9a9,GPU-bd89a13d-0282-cdc9-aa4a-3a2d2ee67969,GPU-9a9f0354-8684-ec3a-f877-d8b160f9582e', 'KUBERNETES_SERVICE_PORT_HTTPS': '443', 'NV_CUDNN_PACKAGE_NAME': 'libcudnn8', 'KUBERNETES_SERVICE_PORT': '443', 'SATURN_USER': 'muchane', 'CONDA_EXE': '/opt/saturncloud/bin/conda', '_CE_M': '', 'BASE_URL': 'https://app.nvidia-oci.saturnenterprise.io', 'CONDA_BIN': '/opt/saturncloud/bin', 'HOSTNAME': 'w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v', 'SATURN_TOKEN': 'eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJhdWQiOlsiYXRsYXMiLCJzYXR1cm4tYXV0aC1wcm94eSJdLCJpc3MiOiJhdGxhcyIsInN1YiI6ImMwMzU4OWFkMTUyOTRjOTJhNmJlNDkwNDY3ZGEzMjVhIiwiaXNfcmVmcmVzaCI6ZmFsc2UsInJlc291cmNlIjoid29ya3NwYWNlOmE4NmVjYmI0MzFkOTQ4MTE4ZTU4M2U1YTllNmI4OWRmIiwic2NvcGUiOiJ3b3Jrc3BhY2U6YTg2ZWNiYjQzMWQ5NDgxMThlNTgzZTVhOWU2Yjg5ZGY6ZGFzazp3cml0ZSIsInVzZXJfaWQiOiIyZGI2MjEwOTc2Yjc0ZTA5YWY4Mzg0YjA1YjFkYjUzNSJ9.VQXvhHCsd23c6X9fbvu4pTmgmtes2i9V1DWJmdyNtJzOl3ym_Grz7DPVyKrzUcu0bjzFXic61JEkCj5NaYZgqFWeA7QnmPS1UvVR70PsAL_qiCaxj6Ns4Ot-RFH2MZ5kQSzfoK10woR3vY_dVzEmoBX3tfv25feAii-tOPLRjIRLBAgSDMyD9869EgneV4YVKVrL-JTnzKPh1y78KoolSM6C5xKBmARPBToswJuypI2XjGRQJWOgd8kvCTHcyi0y8KkhsiOr9VOXdjm9HPVfZKPX-z-3cNEf693H76I0hzrSIzoBQB8XslC6Cm1k66yJSViM4Ya-e7IbhHhzNt_BaZvwkGbuo0NgpqF-fQy8unPN3Uof2MxQxiiRTtcFNoY7kMkGGfV51COduS3Aive5yKwlMn9hX5qauBfznEB4NVzK2o9salob-DRcz7uahSrUrj17wHqWziOhOmKjy-MD-mi5DOoalQ52lU6h_p5O0mL9Vbdh_Ow_fcOANWaCW1Vgp-FLfLPqZmwURIL3k6NoP7rGi7bPHIpgyl7jqgUodaLtoMfvuIW_14VmEKmdAZixsHyTVV4hkKfPslHzC6ikb9itccuoOzxna94vUvXk5adJ-gHdEre38QUJvgu22fP59ANp1mJ3y4wO_isAeKHfurpkgRq-kUcNe7M6i9sW630', 'LANGUAGE': 'en_US.UTF-8', 'SATURN_RESOURCE_TYPE': 'SingleUserServer', 'SATURN_EMAIL': 'muchane@uchicago.edu', 'SATURN_SSH_ENABLED': 'true', 'SATURN_RESOURCE_NAME': 'lingua-run', 'NVIDIA_REQUIRE_CUDA': 'cuda>=12.1 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526', 'SATURN_IS_INTERNAL': 'true', 'SSH_AUTH_SOCK': '/tmp/ssh-XXXX1VrxrN/agent.1416665', 'NV_NVTX_VERSION': '12.1.66-1', 'NB_UID': '1000', 'DASK_DISTRIBUTED__DEPLOY__SCHEDULER_INFO_INTERVAL': '10s', 'NV_LIBCUSPARSE_VERSION': '12.0.2.55-1', 'NV_LIBNPP_VERSION': '12.0.2.50-1', 'NCCL_VERSION': '2.17.1-1', 'SATURN_BASE_URL': 'https://app.nvidia-oci.saturnenterprise.io', 'SATURN_REPO_HOME': '/home/jovyan', 'PWD': '/home/jovyan/shared/muchane/lingua-repo/Efficient-LLMs', 'LOGNAME': 'jovyan', 'NV_CUDNN_PACKAGE': 'libcudnn8=8.9.0.131-1+cuda12.1', 'CONDA_PREFIX': '/opt/saturncloud/envs/lingua_250731', 'KUBERNETES_PORT_12250_TCP_ADDR': '10.96.0.1', 'SATURN_ORG': 'AutomatingDataSciencewithTabularLanguag', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'SATURN_USER_PYTHON': '/opt/saturncloud/envs/saturn/bin/python', 'NV_LIBNPP_PACKAGE': 'libnpp-12-1=12.0.2.50-1', 'SATURN_JUPYTER_BASE_URL': 'http://lingua-8gpu-server.nvidia-oci.saturnenterprise.io', 'NVIDIA_PRODUCT_NAME': 'CUDA', 'SATURN_USERNAME': 'muchane', 'MOTD_SHOWN': 'pam', 'NV_CUDA_CUDART_VERSION': '12.1.55-1', 'HOME': '/home/jovyan', 'LANG': 'en_US.UTF-8', 'KUBERNETES_PORT_443_TCP': 'tcp://10.96.0.1:443', 'SATURN_SYSTEM_PYTHON': '/opt/saturncloud/bin/python', 'SATURN_GIT_SCRATCH_PATH': '/home/jovyan/.git-scratch', 'KUBERNETES_PORT_12250_TCP': 'tcp://10.96.0.1:12250', 'SATURN_PROJECT_NAME': 'lingua-run', 'NVIDIA_CUDA_END_OF_LIFE': '1', 'CUDA_VERSION': '12.1.0', 'BOKEH_ALLOW_WS_ORIGIN': 'lingua-8gpu-server.nvidia-oci.saturnenterprise.io', 'NV_LIBCUBLAS_PACKAGE': 'libcublas-12-1=12.1.0.26-1', 'CONDA_PROMPT_MODIFIER': '(lingua_250731) ', 'SSH_CONNECTION': '192.168.247.122 42216 192.168.45.217 22', 'NV_LIBCUBLAS_PACKAGE_NAME': 'libcublas-12-1', 'JUPYTER_IMAGE_SPEC': 'public.ecr.aws/saturncloud/saturn-python-pytorch:2024.08.01', 'DASK_DISTRIBUTED__DEPLOY__CLUSTER_REPR_INTERVAL': '10s', 'SATURN_VERSION': '2025.02.01-18', 'CONDA_OVERRIDE_CUDA': '12.1', 'TERM': 'xterm', '_CE_CONDA': '', 'KUBERNETES_PORT_12250_TCP_PORT': '12250', 'USER': 'jovyan', 'NV_CUDNN_VERSION': '8.9.0.131', 'CONDA_SHLVL': '1', 'KUBERNETES_SERVICE_PORT_PROXYMUX': '12250', 'SHLVL': '1', 'CONDA_DIR': '/opt/saturncloud', 'NV_CUDA_LIB_VERSION': '12.1.0-1', 'NVARCH': 'x86_64', 'KUBERNETES_PORT_443_TCP_PROTO': 'tcp', 'KUBERNETES_PORT_443_TCP_ADDR': '10.96.0.1', 'NV_CUDA_COMPAT_PACKAGE': 'cuda-compat-12-1', 'CONDA_PYTHON_EXE': '/opt/saturncloud/bin/python', 'NV_LIBNCCL_PACKAGE': 'libnccl2=2.17.1-1+cuda12.1', 'LD_LIBRARY_PATH': '/usr/local/nvidia/lib:/usr/local/nvidia/lib64', 'SATURN_JUPYTER_BASE_DOMAIN': 'lingua-8gpu-server.nvidia-oci.saturnenterprise.io', 'SATURN_IMAGE': 'public.ecr.aws/saturncloud/saturn-python-pytorch:2024.08.01', 'CONDA_INSTALL_DIR': '/opt/saturncloud', 'SSH_CLIENT': '192.168.247.122 42216 22', 'CONDA_DEFAULT_ENV': 'lingua_250731', 'SATURN_GROUP': '', 'SATURN_PYTHON_INIT_SCRIPT': '/home/jovyan/.saturn/setup_jupyterlab.py', 'NB_USER': 'jovyan', 'KUBERNETES_SERVICE_HOST': '10.96.0.1', 'LC_ALL': 'en_US.UTF-8', 'KUBERNETES_PORT': 'tcp://10.96.0.1:443', 'KUBERNETES_PORT_443_TCP_PORT': '443', 'PATH': '/opt/saturncloud/envs/lingua_250731/bin:/opt/saturncloud/condabin:/opt/saturncloud/envs/saturn/bin:/opt/saturncloud/bin:/home/jovyan/.local/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'NV_LIBNCCL_PACKAGE_NAME': 'libnccl2', 'NV_LIBNCCL_PACKAGE_VERSION': '2.17.1-1', 'NB_PYTHON_PREFIX': '/opt/saturncloud/envs/saturn', 'KUBERNETES_PORT_12250_TCP_PROTO': 'tcp', 'SSH_TTY': '/dev/pts/3', 'DEBIAN_FRONTEND': 'noninteractive', 'OLDPWD': '/home/jovyan', '_': '/opt/saturncloud/envs/lingua_250731/bin/torchrun', 'OMP_NUM_THREADS': '1', 'LOCAL_RANK': '4', 'RANK': '4', 'GROUP_RANK': '0', 'ROLE_RANK': '4', 'ROLE_NAME': 'default', 'LOCAL_WORLD_SIZE': '8', 'WORLD_SIZE': '8', 'GROUP_WORLD_SIZE': '1', 'ROLE_WORLD_SIZE': '8', 'MASTER_ADDR': '127.0.0.1', 'MASTER_PORT': '29500', 'TORCHELASTIC_RESTART_COUNT': '0', 'TORCHELASTIC_MAX_RESTARTS': '0', 'TORCHELASTIC_RUN_ID': 'none', 'TORCHELASTIC_USE_AGENT_STORE': 'True', 'TORCH_NCCL_ASYNC_ERROR_HANDLING': '1', 'TORCHELASTIC_ERROR_FILE': '/tmp/torchelastic__o4h9nnb/none_v6wepvnq/attempt_0/4/error.json', 'CUDA_MODULE_LOADING': 'LAZY', 'KMP_DUPLICATE_LIB_OK': 'True', 'KMP_INIT_AT_FORK': 'FALSE', 'MKL_SERVICE_FORCE_INTEL': 'GNU', 'MKL_NUM_THREADS': '1', 'ENABLE_INTRA_NODE_COMM': '1', 'TORCH_NCCL_AVOID_RECORD_STREAMS': '1', 'NCCL_IB_TIMEOUT': '22', 'NCCL_DEBUG': 'INFO', 'TRITON_CACHE_DIR': '/tmp/tmp0aqrntuh'})
3: INFO    25-08-06 20:45:50.142390 - 0:00:09 - Run launched with torchrun, local rank: 3
3: INFO    25-08-06 20:45:50.142664 - 0:00:09 - ENV: environ({'PYTORCH_CUDA_ALLOC_CONF': 'expandable_segments:True', 'SHELL': '/bin/bash', 'NV_LIBCUBLAS_VERSION': '12.1.0.26-1', 'NVIDIA_VISIBLE_DEVICES': 'GPU-508589a6-66b5-1624-f80f-e697085b4da0,GPU-01b43462-d9a5-ec99-23ae-a6508e99b4b4,GPU-4b906144-246f-1d19-58cd-374bacb12f0c,GPU-555361da-6a81-e45d-31bc-58230c88f8a4,GPU-00889c01-9780-9886-a803-e9a372d610b3,GPU-84db6b97-65f5-9b64-912f-d1901007b9a9,GPU-bd89a13d-0282-cdc9-aa4a-3a2d2ee67969,GPU-9a9f0354-8684-ec3a-f877-d8b160f9582e', 'KUBERNETES_SERVICE_PORT_HTTPS': '443', 'NV_CUDNN_PACKAGE_NAME': 'libcudnn8', 'KUBERNETES_SERVICE_PORT': '443', 'SATURN_USER': 'muchane', 'CONDA_EXE': '/opt/saturncloud/bin/conda', '_CE_M': '', 'BASE_URL': 'https://app.nvidia-oci.saturnenterprise.io', 'CONDA_BIN': '/opt/saturncloud/bin', 'HOSTNAME': 'w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v', 'SATURN_TOKEN': 'eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJhdWQiOlsiYXRsYXMiLCJzYXR1cm4tYXV0aC1wcm94eSJdLCJpc3MiOiJhdGxhcyIsInN1YiI6ImMwMzU4OWFkMTUyOTRjOTJhNmJlNDkwNDY3ZGEzMjVhIiwiaXNfcmVmcmVzaCI6ZmFsc2UsInJlc291cmNlIjoid29ya3NwYWNlOmE4NmVjYmI0MzFkOTQ4MTE4ZTU4M2U1YTllNmI4OWRmIiwic2NvcGUiOiJ3b3Jrc3BhY2U6YTg2ZWNiYjQzMWQ5NDgxMThlNTgzZTVhOWU2Yjg5ZGY6ZGFzazp3cml0ZSIsInVzZXJfaWQiOiIyZGI2MjEwOTc2Yjc0ZTA5YWY4Mzg0YjA1YjFkYjUzNSJ9.VQXvhHCsd23c6X9fbvu4pTmgmtes2i9V1DWJmdyNtJzOl3ym_Grz7DPVyKrzUcu0bjzFXic61JEkCj5NaYZgqFWeA7QnmPS1UvVR70PsAL_qiCaxj6Ns4Ot-RFH2MZ5kQSzfoK10woR3vY_dVzEmoBX3tfv25feAii-tOPLRjIRLBAgSDMyD9869EgneV4YVKVrL-JTnzKPh1y78KoolSM6C5xKBmARPBToswJuypI2XjGRQJWOgd8kvCTHcyi0y8KkhsiOr9VOXdjm9HPVfZKPX-z-3cNEf693H76I0hzrSIzoBQB8XslC6Cm1k66yJSViM4Ya-e7IbhHhzNt_BaZvwkGbuo0NgpqF-fQy8unPN3Uof2MxQxiiRTtcFNoY7kMkGGfV51COduS3Aive5yKwlMn9hX5qauBfznEB4NVzK2o9salob-DRcz7uahSrUrj17wHqWziOhOmKjy-MD-mi5DOoalQ52lU6h_p5O0mL9Vbdh_Ow_fcOANWaCW1Vgp-FLfLPqZmwURIL3k6NoP7rGi7bPHIpgyl7jqgUodaLtoMfvuIW_14VmEKmdAZixsHyTVV4hkKfPslHzC6ikb9itccuoOzxna94vUvXk5adJ-gHdEre38QUJvgu22fP59ANp1mJ3y4wO_isAeKHfurpkgRq-kUcNe7M6i9sW630', 'LANGUAGE': 'en_US.UTF-8', 'SATURN_RESOURCE_TYPE': 'SingleUserServer', 'SATURN_EMAIL': 'muchane@uchicago.edu', 'SATURN_SSH_ENABLED': 'true', 'SATURN_RESOURCE_NAME': 'lingua-run', 'NVIDIA_REQUIRE_CUDA': 'cuda>=12.1 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526', 'SATURN_IS_INTERNAL': 'true', 'SSH_AUTH_SOCK': '/tmp/ssh-XXXX1VrxrN/agent.1416665', 'NV_NVTX_VERSION': '12.1.66-1', 'NB_UID': '1000', 'DASK_DISTRIBUTED__DEPLOY__SCHEDULER_INFO_INTERVAL': '10s', 'NV_LIBCUSPARSE_VERSION': '12.0.2.55-1', 'NV_LIBNPP_VERSION': '12.0.2.50-1', 'NCCL_VERSION': '2.17.1-1', 'SATURN_BASE_URL': 'https://app.nvidia-oci.saturnenterprise.io', 'SATURN_REPO_HOME': '/home/jovyan', 'PWD': '/home/jovyan/shared/muchane/lingua-repo/Efficient-LLMs', 'LOGNAME': 'jovyan', 'NV_CUDNN_PACKAGE': 'libcudnn8=8.9.0.131-1+cuda12.1', 'CONDA_PREFIX': '/opt/saturncloud/envs/lingua_250731', 'KUBERNETES_PORT_12250_TCP_ADDR': '10.96.0.1', 'SATURN_ORG': 'AutomatingDataSciencewithTabularLanguag', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'SATURN_USER_PYTHON': '/opt/saturncloud/envs/saturn/bin/python', 'NV_LIBNPP_PACKAGE': 'libnpp-12-1=12.0.2.50-1', 'SATURN_JUPYTER_BASE_URL': 'http://lingua-8gpu-server.nvidia-oci.saturnenterprise.io', 'NVIDIA_PRODUCT_NAME': 'CUDA', 'SATURN_USERNAME': 'muchane', 'MOTD_SHOWN': 'pam', 'NV_CUDA_CUDART_VERSION': '12.1.55-1', 'HOME': '/home/jovyan', 'LANG': 'en_US.UTF-8', 'KUBERNETES_PORT_443_TCP': 'tcp://10.96.0.1:443', 'SATURN_SYSTEM_PYTHON': '/opt/saturncloud/bin/python', 'SATURN_GIT_SCRATCH_PATH': '/home/jovyan/.git-scratch', 'KUBERNETES_PORT_12250_TCP': 'tcp://10.96.0.1:12250', 'SATURN_PROJECT_NAME': 'lingua-run', 'NVIDIA_CUDA_END_OF_LIFE': '1', 'CUDA_VERSION': '12.1.0', 'BOKEH_ALLOW_WS_ORIGIN': 'lingua-8gpu-server.nvidia-oci.saturnenterprise.io', 'NV_LIBCUBLAS_PACKAGE': 'libcublas-12-1=12.1.0.26-1', 'CONDA_PROMPT_MODIFIER': '(lingua_250731) ', 'SSH_CONNECTION': '192.168.247.122 42216 192.168.45.217 22', 'NV_LIBCUBLAS_PACKAGE_NAME': 'libcublas-12-1', 'JUPYTER_IMAGE_SPEC': 'public.ecr.aws/saturncloud/saturn-python-pytorch:2024.08.01', 'DASK_DISTRIBUTED__DEPLOY__CLUSTER_REPR_INTERVAL': '10s', 'SATURN_VERSION': '2025.02.01-18', 'CONDA_OVERRIDE_CUDA': '12.1', 'TERM': 'xterm', '_CE_CONDA': '', 'KUBERNETES_PORT_12250_TCP_PORT': '12250', 'USER': 'jovyan', 'NV_CUDNN_VERSION': '8.9.0.131', 'CONDA_SHLVL': '1', 'KUBERNETES_SERVICE_PORT_PROXYMUX': '12250', 'SHLVL': '1', 'CONDA_DIR': '/opt/saturncloud', 'NV_CUDA_LIB_VERSION': '12.1.0-1', 'NVARCH': 'x86_64', 'KUBERNETES_PORT_443_TCP_PROTO': 'tcp', 'KUBERNETES_PORT_443_TCP_ADDR': '10.96.0.1', 'NV_CUDA_COMPAT_PACKAGE': 'cuda-compat-12-1', 'CONDA_PYTHON_EXE': '/opt/saturncloud/bin/python', 'NV_LIBNCCL_PACKAGE': 'libnccl2=2.17.1-1+cuda12.1', 'LD_LIBRARY_PATH': '/usr/local/nvidia/lib:/usr/local/nvidia/lib64', 'SATURN_JUPYTER_BASE_DOMAIN': 'lingua-8gpu-server.nvidia-oci.saturnenterprise.io', 'SATURN_IMAGE': 'public.ecr.aws/saturncloud/saturn-python-pytorch:2024.08.01', 'CONDA_INSTALL_DIR': '/opt/saturncloud', 'SSH_CLIENT': '192.168.247.122 42216 22', 'CONDA_DEFAULT_ENV': 'lingua_250731', 'SATURN_GROUP': '', 'SATURN_PYTHON_INIT_SCRIPT': '/home/jovyan/.saturn/setup_jupyterlab.py', 'NB_USER': 'jovyan', 'KUBERNETES_SERVICE_HOST': '10.96.0.1', 'LC_ALL': 'en_US.UTF-8', 'KUBERNETES_PORT': 'tcp://10.96.0.1:443', 'KUBERNETES_PORT_443_TCP_PORT': '443', 'PATH': '/opt/saturncloud/envs/lingua_250731/bin:/opt/saturncloud/condabin:/opt/saturncloud/envs/saturn/bin:/opt/saturncloud/bin:/home/jovyan/.local/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'NV_LIBNCCL_PACKAGE_NAME': 'libnccl2', 'NV_LIBNCCL_PACKAGE_VERSION': '2.17.1-1', 'NB_PYTHON_PREFIX': '/opt/saturncloud/envs/saturn', 'KUBERNETES_PORT_12250_TCP_PROTO': 'tcp', 'SSH_TTY': '/dev/pts/3', 'DEBIAN_FRONTEND': 'noninteractive', 'OLDPWD': '/home/jovyan', '_': '/opt/saturncloud/envs/lingua_250731/bin/torchrun', 'OMP_NUM_THREADS': '1', 'LOCAL_RANK': '3', 'RANK': '3', 'GROUP_RANK': '0', 'ROLE_RANK': '3', 'ROLE_NAME': 'default', 'LOCAL_WORLD_SIZE': '8', 'WORLD_SIZE': '8', 'GROUP_WORLD_SIZE': '1', 'ROLE_WORLD_SIZE': '8', 'MASTER_ADDR': '127.0.0.1', 'MASTER_PORT': '29500', 'TORCHELASTIC_RESTART_COUNT': '0', 'TORCHELASTIC_MAX_RESTARTS': '0', 'TORCHELASTIC_RUN_ID': 'none', 'TORCHELASTIC_USE_AGENT_STORE': 'True', 'TORCH_NCCL_ASYNC_ERROR_HANDLING': '1', 'TORCHELASTIC_ERROR_FILE': '/tmp/torchelastic__o4h9nnb/none_v6wepvnq/attempt_0/3/error.json', 'CUDA_MODULE_LOADING': 'LAZY', 'KMP_DUPLICATE_LIB_OK': 'True', 'KMP_INIT_AT_FORK': 'FALSE', 'MKL_SERVICE_FORCE_INTEL': 'GNU', 'MKL_NUM_THREADS': '1', 'ENABLE_INTRA_NODE_COMM': '1', 'TORCH_NCCL_AVOID_RECORD_STREAMS': '1', 'NCCL_IB_TIMEOUT': '22', 'NCCL_DEBUG': 'INFO', 'TRITON_CACHE_DIR': '/tmp/tmpjgvu80xo'})
5: INFO    25-08-06 20:45:50.151235 - 0:00:09 - Run launched with torchrun, local rank: 5
5: INFO    25-08-06 20:45:50.151569 - 0:00:09 - ENV: environ({'PYTORCH_CUDA_ALLOC_CONF': 'expandable_segments:True', 'SHELL': '/bin/bash', 'NV_LIBCUBLAS_VERSION': '12.1.0.26-1', 'NVIDIA_VISIBLE_DEVICES': 'GPU-508589a6-66b5-1624-f80f-e697085b4da0,GPU-01b43462-d9a5-ec99-23ae-a6508e99b4b4,GPU-4b906144-246f-1d19-58cd-374bacb12f0c,GPU-555361da-6a81-e45d-31bc-58230c88f8a4,GPU-00889c01-9780-9886-a803-e9a372d610b3,GPU-84db6b97-65f5-9b64-912f-d1901007b9a9,GPU-bd89a13d-0282-cdc9-aa4a-3a2d2ee67969,GPU-9a9f0354-8684-ec3a-f877-d8b160f9582e', 'KUBERNETES_SERVICE_PORT_HTTPS': '443', 'NV_CUDNN_PACKAGE_NAME': 'libcudnn8', 'KUBERNETES_SERVICE_PORT': '443', 'SATURN_USER': 'muchane', 'CONDA_EXE': '/opt/saturncloud/bin/conda', '_CE_M': '', 'BASE_URL': 'https://app.nvidia-oci.saturnenterprise.io', 'CONDA_BIN': '/opt/saturncloud/bin', 'HOSTNAME': 'w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v', 'SATURN_TOKEN': 'eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJhdWQiOlsiYXRsYXMiLCJzYXR1cm4tYXV0aC1wcm94eSJdLCJpc3MiOiJhdGxhcyIsInN1YiI6ImMwMzU4OWFkMTUyOTRjOTJhNmJlNDkwNDY3ZGEzMjVhIiwiaXNfcmVmcmVzaCI6ZmFsc2UsInJlc291cmNlIjoid29ya3NwYWNlOmE4NmVjYmI0MzFkOTQ4MTE4ZTU4M2U1YTllNmI4OWRmIiwic2NvcGUiOiJ3b3Jrc3BhY2U6YTg2ZWNiYjQzMWQ5NDgxMThlNTgzZTVhOWU2Yjg5ZGY6ZGFzazp3cml0ZSIsInVzZXJfaWQiOiIyZGI2MjEwOTc2Yjc0ZTA5YWY4Mzg0YjA1YjFkYjUzNSJ9.VQXvhHCsd23c6X9fbvu4pTmgmtes2i9V1DWJmdyNtJzOl3ym_Grz7DPVyKrzUcu0bjzFXic61JEkCj5NaYZgqFWeA7QnmPS1UvVR70PsAL_qiCaxj6Ns4Ot-RFH2MZ5kQSzfoK10woR3vY_dVzEmoBX3tfv25feAii-tOPLRjIRLBAgSDMyD9869EgneV4YVKVrL-JTnzKPh1y78KoolSM6C5xKBmARPBToswJuypI2XjGRQJWOgd8kvCTHcyi0y8KkhsiOr9VOXdjm9HPVfZKPX-z-3cNEf693H76I0hzrSIzoBQB8XslC6Cm1k66yJSViM4Ya-e7IbhHhzNt_BaZvwkGbuo0NgpqF-fQy8unPN3Uof2MxQxiiRTtcFNoY7kMkGGfV51COduS3Aive5yKwlMn9hX5qauBfznEB4NVzK2o9salob-DRcz7uahSrUrj17wHqWziOhOmKjy-MD-mi5DOoalQ52lU6h_p5O0mL9Vbdh_Ow_fcOANWaCW1Vgp-FLfLPqZmwURIL3k6NoP7rGi7bPHIpgyl7jqgUodaLtoMfvuIW_14VmEKmdAZixsHyTVV4hkKfPslHzC6ikb9itccuoOzxna94vUvXk5adJ-gHdEre38QUJvgu22fP59ANp1mJ3y4wO_isAeKHfurpkgRq-kUcNe7M6i9sW630', 'LANGUAGE': 'en_US.UTF-8', 'SATURN_RESOURCE_TYPE': 'SingleUserServer', 'SATURN_EMAIL': 'muchane@uchicago.edu', 'SATURN_SSH_ENABLED': 'true', 'SATURN_RESOURCE_NAME': 'lingua-run', 'NVIDIA_REQUIRE_CUDA': 'cuda>=12.1 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526', 'SATURN_IS_INTERNAL': 'true', 'SSH_AUTH_SOCK': '/tmp/ssh-XXXX1VrxrN/agent.1416665', 'NV_NVTX_VERSION': '12.1.66-1', 'NB_UID': '1000', 'DASK_DISTRIBUTED__DEPLOY__SCHEDULER_INFO_INTERVAL': '10s', 'NV_LIBCUSPARSE_VERSION': '12.0.2.55-1', 'NV_LIBNPP_VERSION': '12.0.2.50-1', 'NCCL_VERSION': '2.17.1-1', 'SATURN_BASE_URL': 'https://app.nvidia-oci.saturnenterprise.io', 'SATURN_REPO_HOME': '/home/jovyan', 'PWD': '/home/jovyan/shared/muchane/lingua-repo/Efficient-LLMs', 'LOGNAME': 'jovyan', 'NV_CUDNN_PACKAGE': 'libcudnn8=8.9.0.131-1+cuda12.1', 'CONDA_PREFIX': '/opt/saturncloud/envs/lingua_250731', 'KUBERNETES_PORT_12250_TCP_ADDR': '10.96.0.1', 'SATURN_ORG': 'AutomatingDataSciencewithTabularLanguag', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'SATURN_USER_PYTHON': '/opt/saturncloud/envs/saturn/bin/python', 'NV_LIBNPP_PACKAGE': 'libnpp-12-1=12.0.2.50-1', 'SATURN_JUPYTER_BASE_URL': 'http://lingua-8gpu-server.nvidia-oci.saturnenterprise.io', 'NVIDIA_PRODUCT_NAME': 'CUDA', 'SATURN_USERNAME': 'muchane', 'MOTD_SHOWN': 'pam', 'NV_CUDA_CUDART_VERSION': '12.1.55-1', 'HOME': '/home/jovyan', 'LANG': 'en_US.UTF-8', 'KUBERNETES_PORT_443_TCP': 'tcp://10.96.0.1:443', 'SATURN_SYSTEM_PYTHON': '/opt/saturncloud/bin/python', 'SATURN_GIT_SCRATCH_PATH': '/home/jovyan/.git-scratch', 'KUBERNETES_PORT_12250_TCP': 'tcp://10.96.0.1:12250', 'SATURN_PROJECT_NAME': 'lingua-run', 'NVIDIA_CUDA_END_OF_LIFE': '1', 'CUDA_VERSION': '12.1.0', 'BOKEH_ALLOW_WS_ORIGIN': 'lingua-8gpu-server.nvidia-oci.saturnenterprise.io', 'NV_LIBCUBLAS_PACKAGE': 'libcublas-12-1=12.1.0.26-1', 'CONDA_PROMPT_MODIFIER': '(lingua_250731) ', 'SSH_CONNECTION': '192.168.247.122 42216 192.168.45.217 22', 'NV_LIBCUBLAS_PACKAGE_NAME': 'libcublas-12-1', 'JUPYTER_IMAGE_SPEC': 'public.ecr.aws/saturncloud/saturn-python-pytorch:2024.08.01', 'DASK_DISTRIBUTED__DEPLOY__CLUSTER_REPR_INTERVAL': '10s', 'SATURN_VERSION': '2025.02.01-18', 'CONDA_OVERRIDE_CUDA': '12.1', 'TERM': 'xterm', '_CE_CONDA': '', 'KUBERNETES_PORT_12250_TCP_PORT': '12250', 'USER': 'jovyan', 'NV_CUDNN_VERSION': '8.9.0.131', 'CONDA_SHLVL': '1', 'KUBERNETES_SERVICE_PORT_PROXYMUX': '12250', 'SHLVL': '1', 'CONDA_DIR': '/opt/saturncloud', 'NV_CUDA_LIB_VERSION': '12.1.0-1', 'NVARCH': 'x86_64', 'KUBERNETES_PORT_443_TCP_PROTO': 'tcp', 'KUBERNETES_PORT_443_TCP_ADDR': '10.96.0.1', 'NV_CUDA_COMPAT_PACKAGE': 'cuda-compat-12-1', 'CONDA_PYTHON_EXE': '/opt/saturncloud/bin/python', 'NV_LIBNCCL_PACKAGE': 'libnccl2=2.17.1-1+cuda12.1', 'LD_LIBRARY_PATH': '/usr/local/nvidia/lib:/usr/local/nvidia/lib64', 'SATURN_JUPYTER_BASE_DOMAIN': 'lingua-8gpu-server.nvidia-oci.saturnenterprise.io', 'SATURN_IMAGE': 'public.ecr.aws/saturncloud/saturn-python-pytorch:2024.08.01', 'CONDA_INSTALL_DIR': '/opt/saturncloud', 'SSH_CLIENT': '192.168.247.122 42216 22', 'CONDA_DEFAULT_ENV': 'lingua_250731', 'SATURN_GROUP': '', 'SATURN_PYTHON_INIT_SCRIPT': '/home/jovyan/.saturn/setup_jupyterlab.py', 'NB_USER': 'jovyan', 'KUBERNETES_SERVICE_HOST': '10.96.0.1', 'LC_ALL': 'en_US.UTF-8', 'KUBERNETES_PORT': 'tcp://10.96.0.1:443', 'KUBERNETES_PORT_443_TCP_PORT': '443', 'PATH': '/opt/saturncloud/envs/lingua_250731/bin:/opt/saturncloud/condabin:/opt/saturncloud/envs/saturn/bin:/opt/saturncloud/bin:/home/jovyan/.local/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'NV_LIBNCCL_PACKAGE_NAME': 'libnccl2', 'NV_LIBNCCL_PACKAGE_VERSION': '2.17.1-1', 'NB_PYTHON_PREFIX': '/opt/saturncloud/envs/saturn', 'KUBERNETES_PORT_12250_TCP_PROTO': 'tcp', 'SSH_TTY': '/dev/pts/3', 'DEBIAN_FRONTEND': 'noninteractive', 'OLDPWD': '/home/jovyan', '_': '/opt/saturncloud/envs/lingua_250731/bin/torchrun', 'OMP_NUM_THREADS': '1', 'LOCAL_RANK': '5', 'RANK': '5', 'GROUP_RANK': '0', 'ROLE_RANK': '5', 'ROLE_NAME': 'default', 'LOCAL_WORLD_SIZE': '8', 'WORLD_SIZE': '8', 'GROUP_WORLD_SIZE': '1', 'ROLE_WORLD_SIZE': '8', 'MASTER_ADDR': '127.0.0.1', 'MASTER_PORT': '29500', 'TORCHELASTIC_RESTART_COUNT': '0', 'TORCHELASTIC_MAX_RESTARTS': '0', 'TORCHELASTIC_RUN_ID': 'none', 'TORCHELASTIC_USE_AGENT_STORE': 'True', 'TORCH_NCCL_ASYNC_ERROR_HANDLING': '1', 'TORCHELASTIC_ERROR_FILE': '/tmp/torchelastic__o4h9nnb/none_v6wepvnq/attempt_0/5/error.json', 'CUDA_MODULE_LOADING': 'LAZY', 'KMP_DUPLICATE_LIB_OK': 'True', 'KMP_INIT_AT_FORK': 'FALSE', 'MKL_SERVICE_FORCE_INTEL': 'GNU', 'MKL_NUM_THREADS': '1', 'ENABLE_INTRA_NODE_COMM': '1', 'TORCH_NCCL_AVOID_RECORD_STREAMS': '1', 'NCCL_IB_TIMEOUT': '22', 'NCCL_DEBUG': 'INFO', 'TRITON_CACHE_DIR': '/tmp/tmp18iaamm4'})
0: INFO    25-08-06 20:45:50.188010 - 0:00:09 - Run launched with torchrun, local rank: 0
0: INFO    25-08-06 20:45:50.188331 - 0:00:09 - ENV: environ({'PYTORCH_CUDA_ALLOC_CONF': 'expandable_segments:True', 'SHELL': '/bin/bash', 'NV_LIBCUBLAS_VERSION': '12.1.0.26-1', 'NVIDIA_VISIBLE_DEVICES': 'GPU-508589a6-66b5-1624-f80f-e697085b4da0,GPU-01b43462-d9a5-ec99-23ae-a6508e99b4b4,GPU-4b906144-246f-1d19-58cd-374bacb12f0c,GPU-555361da-6a81-e45d-31bc-58230c88f8a4,GPU-00889c01-9780-9886-a803-e9a372d610b3,GPU-84db6b97-65f5-9b64-912f-d1901007b9a9,GPU-bd89a13d-0282-cdc9-aa4a-3a2d2ee67969,GPU-9a9f0354-8684-ec3a-f877-d8b160f9582e', 'KUBERNETES_SERVICE_PORT_HTTPS': '443', 'NV_CUDNN_PACKAGE_NAME': 'libcudnn8', 'KUBERNETES_SERVICE_PORT': '443', 'SATURN_USER': 'muchane', 'CONDA_EXE': '/opt/saturncloud/bin/conda', '_CE_M': '', 'BASE_URL': 'https://app.nvidia-oci.saturnenterprise.io', 'CONDA_BIN': '/opt/saturncloud/bin', 'HOSTNAME': 'w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v', 'SATURN_TOKEN': 'eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJhdWQiOlsiYXRsYXMiLCJzYXR1cm4tYXV0aC1wcm94eSJdLCJpc3MiOiJhdGxhcyIsInN1YiI6ImMwMzU4OWFkMTUyOTRjOTJhNmJlNDkwNDY3ZGEzMjVhIiwiaXNfcmVmcmVzaCI6ZmFsc2UsInJlc291cmNlIjoid29ya3NwYWNlOmE4NmVjYmI0MzFkOTQ4MTE4ZTU4M2U1YTllNmI4OWRmIiwic2NvcGUiOiJ3b3Jrc3BhY2U6YTg2ZWNiYjQzMWQ5NDgxMThlNTgzZTVhOWU2Yjg5ZGY6ZGFzazp3cml0ZSIsInVzZXJfaWQiOiIyZGI2MjEwOTc2Yjc0ZTA5YWY4Mzg0YjA1YjFkYjUzNSJ9.VQXvhHCsd23c6X9fbvu4pTmgmtes2i9V1DWJmdyNtJzOl3ym_Grz7DPVyKrzUcu0bjzFXic61JEkCj5NaYZgqFWeA7QnmPS1UvVR70PsAL_qiCaxj6Ns4Ot-RFH2MZ5kQSzfoK10woR3vY_dVzEmoBX3tfv25feAii-tOPLRjIRLBAgSDMyD9869EgneV4YVKVrL-JTnzKPh1y78KoolSM6C5xKBmARPBToswJuypI2XjGRQJWOgd8kvCTHcyi0y8KkhsiOr9VOXdjm9HPVfZKPX-z-3cNEf693H76I0hzrSIzoBQB8XslC6Cm1k66yJSViM4Ya-e7IbhHhzNt_BaZvwkGbuo0NgpqF-fQy8unPN3Uof2MxQxiiRTtcFNoY7kMkGGfV51COduS3Aive5yKwlMn9hX5qauBfznEB4NVzK2o9salob-DRcz7uahSrUrj17wHqWziOhOmKjy-MD-mi5DOoalQ52lU6h_p5O0mL9Vbdh_Ow_fcOANWaCW1Vgp-FLfLPqZmwURIL3k6NoP7rGi7bPHIpgyl7jqgUodaLtoMfvuIW_14VmEKmdAZixsHyTVV4hkKfPslHzC6ikb9itccuoOzxna94vUvXk5adJ-gHdEre38QUJvgu22fP59ANp1mJ3y4wO_isAeKHfurpkgRq-kUcNe7M6i9sW630', 'LANGUAGE': 'en_US.UTF-8', 'SATURN_RESOURCE_TYPE': 'SingleUserServer', 'SATURN_EMAIL': 'muchane@uchicago.edu', 'SATURN_SSH_ENABLED': 'true', 'SATURN_RESOURCE_NAME': 'lingua-run', 'NVIDIA_REQUIRE_CUDA': 'cuda>=12.1 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526', 'SATURN_IS_INTERNAL': 'true', 'SSH_AUTH_SOCK': '/tmp/ssh-XXXX1VrxrN/agent.1416665', 'NV_NVTX_VERSION': '12.1.66-1', 'NB_UID': '1000', 'DASK_DISTRIBUTED__DEPLOY__SCHEDULER_INFO_INTERVAL': '10s', 'NV_LIBCUSPARSE_VERSION': '12.0.2.55-1', 'NV_LIBNPP_VERSION': '12.0.2.50-1', 'NCCL_VERSION': '2.17.1-1', 'SATURN_BASE_URL': 'https://app.nvidia-oci.saturnenterprise.io', 'SATURN_REPO_HOME': '/home/jovyan', 'PWD': '/home/jovyan/shared/muchane/lingua-repo/Efficient-LLMs', 'LOGNAME': 'jovyan', 'NV_CUDNN_PACKAGE': 'libcudnn8=8.9.0.131-1+cuda12.1', 'CONDA_PREFIX': '/opt/saturncloud/envs/lingua_250731', 'KUBERNETES_PORT_12250_TCP_ADDR': '10.96.0.1', 'SATURN_ORG': 'AutomatingDataSciencewithTabularLanguag', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'SATURN_USER_PYTHON': '/opt/saturncloud/envs/saturn/bin/python', 'NV_LIBNPP_PACKAGE': 'libnpp-12-1=12.0.2.50-1', 'SATURN_JUPYTER_BASE_URL': 'http://lingua-8gpu-server.nvidia-oci.saturnenterprise.io', 'NVIDIA_PRODUCT_NAME': 'CUDA', 'SATURN_USERNAME': 'muchane', 'MOTD_SHOWN': 'pam', 'NV_CUDA_CUDART_VERSION': '12.1.55-1', 'HOME': '/home/jovyan', 'LANG': 'en_US.UTF-8', 'KUBERNETES_PORT_443_TCP': 'tcp://10.96.0.1:443', 'SATURN_SYSTEM_PYTHON': '/opt/saturncloud/bin/python', 'SATURN_GIT_SCRATCH_PATH': '/home/jovyan/.git-scratch', 'KUBERNETES_PORT_12250_TCP': 'tcp://10.96.0.1:12250', 'SATURN_PROJECT_NAME': 'lingua-run', 'NVIDIA_CUDA_END_OF_LIFE': '1', 'CUDA_VERSION': '12.1.0', 'BOKEH_ALLOW_WS_ORIGIN': 'lingua-8gpu-server.nvidia-oci.saturnenterprise.io', 'NV_LIBCUBLAS_PACKAGE': 'libcublas-12-1=12.1.0.26-1', 'CONDA_PROMPT_MODIFIER': '(lingua_250731) ', 'SSH_CONNECTION': '192.168.247.122 42216 192.168.45.217 22', 'NV_LIBCUBLAS_PACKAGE_NAME': 'libcublas-12-1', 'JUPYTER_IMAGE_SPEC': 'public.ecr.aws/saturncloud/saturn-python-pytorch:2024.08.01', 'DASK_DISTRIBUTED__DEPLOY__CLUSTER_REPR_INTERVAL': '10s', 'SATURN_VERSION': '2025.02.01-18', 'CONDA_OVERRIDE_CUDA': '12.1', 'TERM': 'xterm', '_CE_CONDA': '', 'KUBERNETES_PORT_12250_TCP_PORT': '12250', 'USER': 'jovyan', 'NV_CUDNN_VERSION': '8.9.0.131', 'CONDA_SHLVL': '1', 'KUBERNETES_SERVICE_PORT_PROXYMUX': '12250', 'SHLVL': '1', 'CONDA_DIR': '/opt/saturncloud', 'NV_CUDA_LIB_VERSION': '12.1.0-1', 'NVARCH': 'x86_64', 'KUBERNETES_PORT_443_TCP_PROTO': 'tcp', 'KUBERNETES_PORT_443_TCP_ADDR': '10.96.0.1', 'NV_CUDA_COMPAT_PACKAGE': 'cuda-compat-12-1', 'CONDA_PYTHON_EXE': '/opt/saturncloud/bin/python', 'NV_LIBNCCL_PACKAGE': 'libnccl2=2.17.1-1+cuda12.1', 'LD_LIBRARY_PATH': '/usr/local/nvidia/lib:/usr/local/nvidia/lib64', 'SATURN_JUPYTER_BASE_DOMAIN': 'lingua-8gpu-server.nvidia-oci.saturnenterprise.io', 'SATURN_IMAGE': 'public.ecr.aws/saturncloud/saturn-python-pytorch:2024.08.01', 'CONDA_INSTALL_DIR': '/opt/saturncloud', 'SSH_CLIENT': '192.168.247.122 42216 22', 'CONDA_DEFAULT_ENV': 'lingua_250731', 'SATURN_GROUP': '', 'SATURN_PYTHON_INIT_SCRIPT': '/home/jovyan/.saturn/setup_jupyterlab.py', 'NB_USER': 'jovyan', 'KUBERNETES_SERVICE_HOST': '10.96.0.1', 'LC_ALL': 'en_US.UTF-8', 'KUBERNETES_PORT': 'tcp://10.96.0.1:443', 'KUBERNETES_PORT_443_TCP_PORT': '443', 'PATH': '/opt/saturncloud/envs/lingua_250731/bin:/opt/saturncloud/condabin:/opt/saturncloud/envs/saturn/bin:/opt/saturncloud/bin:/home/jovyan/.local/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'NV_LIBNCCL_PACKAGE_NAME': 'libnccl2', 'NV_LIBNCCL_PACKAGE_VERSION': '2.17.1-1', 'NB_PYTHON_PREFIX': '/opt/saturncloud/envs/saturn', 'KUBERNETES_PORT_12250_TCP_PROTO': 'tcp', 'SSH_TTY': '/dev/pts/3', 'DEBIAN_FRONTEND': 'noninteractive', 'OLDPWD': '/home/jovyan', '_': '/opt/saturncloud/envs/lingua_250731/bin/torchrun', 'OMP_NUM_THREADS': '1', 'LOCAL_RANK': '0', 'RANK': '0', 'GROUP_RANK': '0', 'ROLE_RANK': '0', 'ROLE_NAME': 'default', 'LOCAL_WORLD_SIZE': '8', 'WORLD_SIZE': '8', 'GROUP_WORLD_SIZE': '1', 'ROLE_WORLD_SIZE': '8', 'MASTER_ADDR': '127.0.0.1', 'MASTER_PORT': '29500', 'TORCHELASTIC_RESTART_COUNT': '0', 'TORCHELASTIC_MAX_RESTARTS': '0', 'TORCHELASTIC_RUN_ID': 'none', 'TORCHELASTIC_USE_AGENT_STORE': 'True', 'TORCH_NCCL_ASYNC_ERROR_HANDLING': '1', 'TORCHELASTIC_ERROR_FILE': '/tmp/torchelastic__o4h9nnb/none_v6wepvnq/attempt_0/0/error.json', 'CUDA_MODULE_LOADING': 'LAZY', 'KMP_DUPLICATE_LIB_OK': 'True', 'KMP_INIT_AT_FORK': 'FALSE', 'MKL_SERVICE_FORCE_INTEL': 'GNU', 'MKL_NUM_THREADS': '1', 'ENABLE_INTRA_NODE_COMM': '1', 'TORCH_NCCL_AVOID_RECORD_STREAMS': '1', 'NCCL_IB_TIMEOUT': '22', 'NCCL_DEBUG': 'INFO', 'TRITON_CACHE_DIR': '/tmp/tmpkrnyh_88'})
0: INFO    25-08-06 20:45:50.188540 - 0:00:09 - Initializing global process group with all 8 ranks
6: INFO    25-08-06 20:45:50.388198 - 0:00:09 - Run launched with torchrun, local rank: 6
6: INFO    25-08-06 20:45:50.388559 - 0:00:09 - ENV: environ({'PYTORCH_CUDA_ALLOC_CONF': 'expandable_segments:True', 'SHELL': '/bin/bash', 'NV_LIBCUBLAS_VERSION': '12.1.0.26-1', 'NVIDIA_VISIBLE_DEVICES': 'GPU-508589a6-66b5-1624-f80f-e697085b4da0,GPU-01b43462-d9a5-ec99-23ae-a6508e99b4b4,GPU-4b906144-246f-1d19-58cd-374bacb12f0c,GPU-555361da-6a81-e45d-31bc-58230c88f8a4,GPU-00889c01-9780-9886-a803-e9a372d610b3,GPU-84db6b97-65f5-9b64-912f-d1901007b9a9,GPU-bd89a13d-0282-cdc9-aa4a-3a2d2ee67969,GPU-9a9f0354-8684-ec3a-f877-d8b160f9582e', 'KUBERNETES_SERVICE_PORT_HTTPS': '443', 'NV_CUDNN_PACKAGE_NAME': 'libcudnn8', 'KUBERNETES_SERVICE_PORT': '443', 'SATURN_USER': 'muchane', 'CONDA_EXE': '/opt/saturncloud/bin/conda', '_CE_M': '', 'BASE_URL': 'https://app.nvidia-oci.saturnenterprise.io', 'CONDA_BIN': '/opt/saturncloud/bin', 'HOSTNAME': 'w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v', 'SATURN_TOKEN': 'eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJhdWQiOlsiYXRsYXMiLCJzYXR1cm4tYXV0aC1wcm94eSJdLCJpc3MiOiJhdGxhcyIsInN1YiI6ImMwMzU4OWFkMTUyOTRjOTJhNmJlNDkwNDY3ZGEzMjVhIiwiaXNfcmVmcmVzaCI6ZmFsc2UsInJlc291cmNlIjoid29ya3NwYWNlOmE4NmVjYmI0MzFkOTQ4MTE4ZTU4M2U1YTllNmI4OWRmIiwic2NvcGUiOiJ3b3Jrc3BhY2U6YTg2ZWNiYjQzMWQ5NDgxMThlNTgzZTVhOWU2Yjg5ZGY6ZGFzazp3cml0ZSIsInVzZXJfaWQiOiIyZGI2MjEwOTc2Yjc0ZTA5YWY4Mzg0YjA1YjFkYjUzNSJ9.VQXvhHCsd23c6X9fbvu4pTmgmtes2i9V1DWJmdyNtJzOl3ym_Grz7DPVyKrzUcu0bjzFXic61JEkCj5NaYZgqFWeA7QnmPS1UvVR70PsAL_qiCaxj6Ns4Ot-RFH2MZ5kQSzfoK10woR3vY_dVzEmoBX3tfv25feAii-tOPLRjIRLBAgSDMyD9869EgneV4YVKVrL-JTnzKPh1y78KoolSM6C5xKBmARPBToswJuypI2XjGRQJWOgd8kvCTHcyi0y8KkhsiOr9VOXdjm9HPVfZKPX-z-3cNEf693H76I0hzrSIzoBQB8XslC6Cm1k66yJSViM4Ya-e7IbhHhzNt_BaZvwkGbuo0NgpqF-fQy8unPN3Uof2MxQxiiRTtcFNoY7kMkGGfV51COduS3Aive5yKwlMn9hX5qauBfznEB4NVzK2o9salob-DRcz7uahSrUrj17wHqWziOhOmKjy-MD-mi5DOoalQ52lU6h_p5O0mL9Vbdh_Ow_fcOANWaCW1Vgp-FLfLPqZmwURIL3k6NoP7rGi7bPHIpgyl7jqgUodaLtoMfvuIW_14VmEKmdAZixsHyTVV4hkKfPslHzC6ikb9itccuoOzxna94vUvXk5adJ-gHdEre38QUJvgu22fP59ANp1mJ3y4wO_isAeKHfurpkgRq-kUcNe7M6i9sW630', 'LANGUAGE': 'en_US.UTF-8', 'SATURN_RESOURCE_TYPE': 'SingleUserServer', 'SATURN_EMAIL': 'muchane@uchicago.edu', 'SATURN_SSH_ENABLED': 'true', 'SATURN_RESOURCE_NAME': 'lingua-run', 'NVIDIA_REQUIRE_CUDA': 'cuda>=12.1 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526', 'SATURN_IS_INTERNAL': 'true', 'SSH_AUTH_SOCK': '/tmp/ssh-XXXX1VrxrN/agent.1416665', 'NV_NVTX_VERSION': '12.1.66-1', 'NB_UID': '1000', 'DASK_DISTRIBUTED__DEPLOY__SCHEDULER_INFO_INTERVAL': '10s', 'NV_LIBCUSPARSE_VERSION': '12.0.2.55-1', 'NV_LIBNPP_VERSION': '12.0.2.50-1', 'NCCL_VERSION': '2.17.1-1', 'SATURN_BASE_URL': 'https://app.nvidia-oci.saturnenterprise.io', 'SATURN_REPO_HOME': '/home/jovyan', 'PWD': '/home/jovyan/shared/muchane/lingua-repo/Efficient-LLMs', 'LOGNAME': 'jovyan', 'NV_CUDNN_PACKAGE': 'libcudnn8=8.9.0.131-1+cuda12.1', 'CONDA_PREFIX': '/opt/saturncloud/envs/lingua_250731', 'KUBERNETES_PORT_12250_TCP_ADDR': '10.96.0.1', 'SATURN_ORG': 'AutomatingDataSciencewithTabularLanguag', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'SATURN_USER_PYTHON': '/opt/saturncloud/envs/saturn/bin/python', 'NV_LIBNPP_PACKAGE': 'libnpp-12-1=12.0.2.50-1', 'SATURN_JUPYTER_BASE_URL': 'http://lingua-8gpu-server.nvidia-oci.saturnenterprise.io', 'NVIDIA_PRODUCT_NAME': 'CUDA', 'SATURN_USERNAME': 'muchane', 'MOTD_SHOWN': 'pam', 'NV_CUDA_CUDART_VERSION': '12.1.55-1', 'HOME': '/home/jovyan', 'LANG': 'en_US.UTF-8', 'KUBERNETES_PORT_443_TCP': 'tcp://10.96.0.1:443', 'SATURN_SYSTEM_PYTHON': '/opt/saturncloud/bin/python', 'SATURN_GIT_SCRATCH_PATH': '/home/jovyan/.git-scratch', 'KUBERNETES_PORT_12250_TCP': 'tcp://10.96.0.1:12250', 'SATURN_PROJECT_NAME': 'lingua-run', 'NVIDIA_CUDA_END_OF_LIFE': '1', 'CUDA_VERSION': '12.1.0', 'BOKEH_ALLOW_WS_ORIGIN': 'lingua-8gpu-server.nvidia-oci.saturnenterprise.io', 'NV_LIBCUBLAS_PACKAGE': 'libcublas-12-1=12.1.0.26-1', 'CONDA_PROMPT_MODIFIER': '(lingua_250731) ', 'SSH_CONNECTION': '192.168.247.122 42216 192.168.45.217 22', 'NV_LIBCUBLAS_PACKAGE_NAME': 'libcublas-12-1', 'JUPYTER_IMAGE_SPEC': 'public.ecr.aws/saturncloud/saturn-python-pytorch:2024.08.01', 'DASK_DISTRIBUTED__DEPLOY__CLUSTER_REPR_INTERVAL': '10s', 'SATURN_VERSION': '2025.02.01-18', 'CONDA_OVERRIDE_CUDA': '12.1', 'TERM': 'xterm', '_CE_CONDA': '', 'KUBERNETES_PORT_12250_TCP_PORT': '12250', 'USER': 'jovyan', 'NV_CUDNN_VERSION': '8.9.0.131', 'CONDA_SHLVL': '1', 'KUBERNETES_SERVICE_PORT_PROXYMUX': '12250', 'SHLVL': '1', 'CONDA_DIR': '/opt/saturncloud', 'NV_CUDA_LIB_VERSION': '12.1.0-1', 'NVARCH': 'x86_64', 'KUBERNETES_PORT_443_TCP_PROTO': 'tcp', 'KUBERNETES_PORT_443_TCP_ADDR': '10.96.0.1', 'NV_CUDA_COMPAT_PACKAGE': 'cuda-compat-12-1', 'CONDA_PYTHON_EXE': '/opt/saturncloud/bin/python', 'NV_LIBNCCL_PACKAGE': 'libnccl2=2.17.1-1+cuda12.1', 'LD_LIBRARY_PATH': '/usr/local/nvidia/lib:/usr/local/nvidia/lib64', 'SATURN_JUPYTER_BASE_DOMAIN': 'lingua-8gpu-server.nvidia-oci.saturnenterprise.io', 'SATURN_IMAGE': 'public.ecr.aws/saturncloud/saturn-python-pytorch:2024.08.01', 'CONDA_INSTALL_DIR': '/opt/saturncloud', 'SSH_CLIENT': '192.168.247.122 42216 22', 'CONDA_DEFAULT_ENV': 'lingua_250731', 'SATURN_GROUP': '', 'SATURN_PYTHON_INIT_SCRIPT': '/home/jovyan/.saturn/setup_jupyterlab.py', 'NB_USER': 'jovyan', 'KUBERNETES_SERVICE_HOST': '10.96.0.1', 'LC_ALL': 'en_US.UTF-8', 'KUBERNETES_PORT': 'tcp://10.96.0.1:443', 'KUBERNETES_PORT_443_TCP_PORT': '443', 'PATH': '/opt/saturncloud/envs/lingua_250731/bin:/opt/saturncloud/condabin:/opt/saturncloud/envs/saturn/bin:/opt/saturncloud/bin:/home/jovyan/.local/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'NV_LIBNCCL_PACKAGE_NAME': 'libnccl2', 'NV_LIBNCCL_PACKAGE_VERSION': '2.17.1-1', 'NB_PYTHON_PREFIX': '/opt/saturncloud/envs/saturn', 'KUBERNETES_PORT_12250_TCP_PROTO': 'tcp', 'SSH_TTY': '/dev/pts/3', 'DEBIAN_FRONTEND': 'noninteractive', 'OLDPWD': '/home/jovyan', '_': '/opt/saturncloud/envs/lingua_250731/bin/torchrun', 'OMP_NUM_THREADS': '1', 'LOCAL_RANK': '6', 'RANK': '6', 'GROUP_RANK': '0', 'ROLE_RANK': '6', 'ROLE_NAME': 'default', 'LOCAL_WORLD_SIZE': '8', 'WORLD_SIZE': '8', 'GROUP_WORLD_SIZE': '1', 'ROLE_WORLD_SIZE': '8', 'MASTER_ADDR': '127.0.0.1', 'MASTER_PORT': '29500', 'TORCHELASTIC_RESTART_COUNT': '0', 'TORCHELASTIC_MAX_RESTARTS': '0', 'TORCHELASTIC_RUN_ID': 'none', 'TORCHELASTIC_USE_AGENT_STORE': 'True', 'TORCH_NCCL_ASYNC_ERROR_HANDLING': '1', 'TORCHELASTIC_ERROR_FILE': '/tmp/torchelastic__o4h9nnb/none_v6wepvnq/attempt_0/6/error.json', 'CUDA_MODULE_LOADING': 'LAZY', 'KMP_DUPLICATE_LIB_OK': 'True', 'KMP_INIT_AT_FORK': 'FALSE', 'MKL_SERVICE_FORCE_INTEL': 'GNU', 'MKL_NUM_THREADS': '1', 'ENABLE_INTRA_NODE_COMM': '1', 'TORCH_NCCL_AVOID_RECORD_STREAMS': '1', 'NCCL_IB_TIMEOUT': '22', 'NCCL_DEBUG': 'INFO', 'TRITON_CACHE_DIR': '/tmp/tmp7eaf3qhc'})
2: INFO    25-08-06 20:45:50.499807 - 0:00:09 - Run launched with torchrun, local rank: 2
2: INFO    25-08-06 20:45:50.500072 - 0:00:09 - ENV: environ({'PYTORCH_CUDA_ALLOC_CONF': 'expandable_segments:True', 'SHELL': '/bin/bash', 'NV_LIBCUBLAS_VERSION': '12.1.0.26-1', 'NVIDIA_VISIBLE_DEVICES': 'GPU-508589a6-66b5-1624-f80f-e697085b4da0,GPU-01b43462-d9a5-ec99-23ae-a6508e99b4b4,GPU-4b906144-246f-1d19-58cd-374bacb12f0c,GPU-555361da-6a81-e45d-31bc-58230c88f8a4,GPU-00889c01-9780-9886-a803-e9a372d610b3,GPU-84db6b97-65f5-9b64-912f-d1901007b9a9,GPU-bd89a13d-0282-cdc9-aa4a-3a2d2ee67969,GPU-9a9f0354-8684-ec3a-f877-d8b160f9582e', 'KUBERNETES_SERVICE_PORT_HTTPS': '443', 'NV_CUDNN_PACKAGE_NAME': 'libcudnn8', 'KUBERNETES_SERVICE_PORT': '443', 'SATURN_USER': 'muchane', 'CONDA_EXE': '/opt/saturncloud/bin/conda', '_CE_M': '', 'BASE_URL': 'https://app.nvidia-oci.saturnenterprise.io', 'CONDA_BIN': '/opt/saturncloud/bin', 'HOSTNAME': 'w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v', 'SATURN_TOKEN': 'eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJhdWQiOlsiYXRsYXMiLCJzYXR1cm4tYXV0aC1wcm94eSJdLCJpc3MiOiJhdGxhcyIsInN1YiI6ImMwMzU4OWFkMTUyOTRjOTJhNmJlNDkwNDY3ZGEzMjVhIiwiaXNfcmVmcmVzaCI6ZmFsc2UsInJlc291cmNlIjoid29ya3NwYWNlOmE4NmVjYmI0MzFkOTQ4MTE4ZTU4M2U1YTllNmI4OWRmIiwic2NvcGUiOiJ3b3Jrc3BhY2U6YTg2ZWNiYjQzMWQ5NDgxMThlNTgzZTVhOWU2Yjg5ZGY6ZGFzazp3cml0ZSIsInVzZXJfaWQiOiIyZGI2MjEwOTc2Yjc0ZTA5YWY4Mzg0YjA1YjFkYjUzNSJ9.VQXvhHCsd23c6X9fbvu4pTmgmtes2i9V1DWJmdyNtJzOl3ym_Grz7DPVyKrzUcu0bjzFXic61JEkCj5NaYZgqFWeA7QnmPS1UvVR70PsAL_qiCaxj6Ns4Ot-RFH2MZ5kQSzfoK10woR3vY_dVzEmoBX3tfv25feAii-tOPLRjIRLBAgSDMyD9869EgneV4YVKVrL-JTnzKPh1y78KoolSM6C5xKBmARPBToswJuypI2XjGRQJWOgd8kvCTHcyi0y8KkhsiOr9VOXdjm9HPVfZKPX-z-3cNEf693H76I0hzrSIzoBQB8XslC6Cm1k66yJSViM4Ya-e7IbhHhzNt_BaZvwkGbuo0NgpqF-fQy8unPN3Uof2MxQxiiRTtcFNoY7kMkGGfV51COduS3Aive5yKwlMn9hX5qauBfznEB4NVzK2o9salob-DRcz7uahSrUrj17wHqWziOhOmKjy-MD-mi5DOoalQ52lU6h_p5O0mL9Vbdh_Ow_fcOANWaCW1Vgp-FLfLPqZmwURIL3k6NoP7rGi7bPHIpgyl7jqgUodaLtoMfvuIW_14VmEKmdAZixsHyTVV4hkKfPslHzC6ikb9itccuoOzxna94vUvXk5adJ-gHdEre38QUJvgu22fP59ANp1mJ3y4wO_isAeKHfurpkgRq-kUcNe7M6i9sW630', 'LANGUAGE': 'en_US.UTF-8', 'SATURN_RESOURCE_TYPE': 'SingleUserServer', 'SATURN_EMAIL': 'muchane@uchicago.edu', 'SATURN_SSH_ENABLED': 'true', 'SATURN_RESOURCE_NAME': 'lingua-run', 'NVIDIA_REQUIRE_CUDA': 'cuda>=12.1 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526', 'SATURN_IS_INTERNAL': 'true', 'SSH_AUTH_SOCK': '/tmp/ssh-XXXX1VrxrN/agent.1416665', 'NV_NVTX_VERSION': '12.1.66-1', 'NB_UID': '1000', 'DASK_DISTRIBUTED__DEPLOY__SCHEDULER_INFO_INTERVAL': '10s', 'NV_LIBCUSPARSE_VERSION': '12.0.2.55-1', 'NV_LIBNPP_VERSION': '12.0.2.50-1', 'NCCL_VERSION': '2.17.1-1', 'SATURN_BASE_URL': 'https://app.nvidia-oci.saturnenterprise.io', 'SATURN_REPO_HOME': '/home/jovyan', 'PWD': '/home/jovyan/shared/muchane/lingua-repo/Efficient-LLMs', 'LOGNAME': 'jovyan', 'NV_CUDNN_PACKAGE': 'libcudnn8=8.9.0.131-1+cuda12.1', 'CONDA_PREFIX': '/opt/saturncloud/envs/lingua_250731', 'KUBERNETES_PORT_12250_TCP_ADDR': '10.96.0.1', 'SATURN_ORG': 'AutomatingDataSciencewithTabularLanguag', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'SATURN_USER_PYTHON': '/opt/saturncloud/envs/saturn/bin/python', 'NV_LIBNPP_PACKAGE': 'libnpp-12-1=12.0.2.50-1', 'SATURN_JUPYTER_BASE_URL': 'http://lingua-8gpu-server.nvidia-oci.saturnenterprise.io', 'NVIDIA_PRODUCT_NAME': 'CUDA', 'SATURN_USERNAME': 'muchane', 'MOTD_SHOWN': 'pam', 'NV_CUDA_CUDART_VERSION': '12.1.55-1', 'HOME': '/home/jovyan', 'LANG': 'en_US.UTF-8', 'KUBERNETES_PORT_443_TCP': 'tcp://10.96.0.1:443', 'SATURN_SYSTEM_PYTHON': '/opt/saturncloud/bin/python', 'SATURN_GIT_SCRATCH_PATH': '/home/jovyan/.git-scratch', 'KUBERNETES_PORT_12250_TCP': 'tcp://10.96.0.1:12250', 'SATURN_PROJECT_NAME': 'lingua-run', 'NVIDIA_CUDA_END_OF_LIFE': '1', 'CUDA_VERSION': '12.1.0', 'BOKEH_ALLOW_WS_ORIGIN': 'lingua-8gpu-server.nvidia-oci.saturnenterprise.io', 'NV_LIBCUBLAS_PACKAGE': 'libcublas-12-1=12.1.0.26-1', 'CONDA_PROMPT_MODIFIER': '(lingua_250731) ', 'SSH_CONNECTION': '192.168.247.122 42216 192.168.45.217 22', 'NV_LIBCUBLAS_PACKAGE_NAME': 'libcublas-12-1', 'JUPYTER_IMAGE_SPEC': 'public.ecr.aws/saturncloud/saturn-python-pytorch:2024.08.01', 'DASK_DISTRIBUTED__DEPLOY__CLUSTER_REPR_INTERVAL': '10s', 'SATURN_VERSION': '2025.02.01-18', 'CONDA_OVERRIDE_CUDA': '12.1', 'TERM': 'xterm', '_CE_CONDA': '', 'KUBERNETES_PORT_12250_TCP_PORT': '12250', 'USER': 'jovyan', 'NV_CUDNN_VERSION': '8.9.0.131', 'CONDA_SHLVL': '1', 'KUBERNETES_SERVICE_PORT_PROXYMUX': '12250', 'SHLVL': '1', 'CONDA_DIR': '/opt/saturncloud', 'NV_CUDA_LIB_VERSION': '12.1.0-1', 'NVARCH': 'x86_64', 'KUBERNETES_PORT_443_TCP_PROTO': 'tcp', 'KUBERNETES_PORT_443_TCP_ADDR': '10.96.0.1', 'NV_CUDA_COMPAT_PACKAGE': 'cuda-compat-12-1', 'CONDA_PYTHON_EXE': '/opt/saturncloud/bin/python', 'NV_LIBNCCL_PACKAGE': 'libnccl2=2.17.1-1+cuda12.1', 'LD_LIBRARY_PATH': '/usr/local/nvidia/lib:/usr/local/nvidia/lib64', 'SATURN_JUPYTER_BASE_DOMAIN': 'lingua-8gpu-server.nvidia-oci.saturnenterprise.io', 'SATURN_IMAGE': 'public.ecr.aws/saturncloud/saturn-python-pytorch:2024.08.01', 'CONDA_INSTALL_DIR': '/opt/saturncloud', 'SSH_CLIENT': '192.168.247.122 42216 22', 'CONDA_DEFAULT_ENV': 'lingua_250731', 'SATURN_GROUP': '', 'SATURN_PYTHON_INIT_SCRIPT': '/home/jovyan/.saturn/setup_jupyterlab.py', 'NB_USER': 'jovyan', 'KUBERNETES_SERVICE_HOST': '10.96.0.1', 'LC_ALL': 'en_US.UTF-8', 'KUBERNETES_PORT': 'tcp://10.96.0.1:443', 'KUBERNETES_PORT_443_TCP_PORT': '443', 'PATH': '/opt/saturncloud/envs/lingua_250731/bin:/opt/saturncloud/condabin:/opt/saturncloud/envs/saturn/bin:/opt/saturncloud/bin:/home/jovyan/.local/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'NV_LIBNCCL_PACKAGE_NAME': 'libnccl2', 'NV_LIBNCCL_PACKAGE_VERSION': '2.17.1-1', 'NB_PYTHON_PREFIX': '/opt/saturncloud/envs/saturn', 'KUBERNETES_PORT_12250_TCP_PROTO': 'tcp', 'SSH_TTY': '/dev/pts/3', 'DEBIAN_FRONTEND': 'noninteractive', 'OLDPWD': '/home/jovyan', '_': '/opt/saturncloud/envs/lingua_250731/bin/torchrun', 'OMP_NUM_THREADS': '1', 'LOCAL_RANK': '2', 'RANK': '2', 'GROUP_RANK': '0', 'ROLE_RANK': '2', 'ROLE_NAME': 'default', 'LOCAL_WORLD_SIZE': '8', 'WORLD_SIZE': '8', 'GROUP_WORLD_SIZE': '1', 'ROLE_WORLD_SIZE': '8', 'MASTER_ADDR': '127.0.0.1', 'MASTER_PORT': '29500', 'TORCHELASTIC_RESTART_COUNT': '0', 'TORCHELASTIC_MAX_RESTARTS': '0', 'TORCHELASTIC_RUN_ID': 'none', 'TORCHELASTIC_USE_AGENT_STORE': 'True', 'TORCH_NCCL_ASYNC_ERROR_HANDLING': '1', 'TORCHELASTIC_ERROR_FILE': '/tmp/torchelastic__o4h9nnb/none_v6wepvnq/attempt_0/2/error.json', 'CUDA_MODULE_LOADING': 'LAZY', 'KMP_DUPLICATE_LIB_OK': 'True', 'KMP_INIT_AT_FORK': 'FALSE', 'MKL_SERVICE_FORCE_INTEL': 'GNU', 'MKL_NUM_THREADS': '1', 'ENABLE_INTRA_NODE_COMM': '1', 'TORCH_NCCL_AVOID_RECORD_STREAMS': '1', 'NCCL_IB_TIMEOUT': '22', 'NCCL_DEBUG': 'INFO', 'TRITON_CACHE_DIR': '/tmp/tmpru6t8knu'})
1: INFO    25-08-06 20:45:50.528514 - 0:00:09 - Run launched with torchrun, local rank: 1
1: INFO    25-08-06 20:45:50.528789 - 0:00:09 - ENV: environ({'PYTORCH_CUDA_ALLOC_CONF': 'expandable_segments:True', 'SHELL': '/bin/bash', 'NV_LIBCUBLAS_VERSION': '12.1.0.26-1', 'NVIDIA_VISIBLE_DEVICES': 'GPU-508589a6-66b5-1624-f80f-e697085b4da0,GPU-01b43462-d9a5-ec99-23ae-a6508e99b4b4,GPU-4b906144-246f-1d19-58cd-374bacb12f0c,GPU-555361da-6a81-e45d-31bc-58230c88f8a4,GPU-00889c01-9780-9886-a803-e9a372d610b3,GPU-84db6b97-65f5-9b64-912f-d1901007b9a9,GPU-bd89a13d-0282-cdc9-aa4a-3a2d2ee67969,GPU-9a9f0354-8684-ec3a-f877-d8b160f9582e', 'KUBERNETES_SERVICE_PORT_HTTPS': '443', 'NV_CUDNN_PACKAGE_NAME': 'libcudnn8', 'KUBERNETES_SERVICE_PORT': '443', 'SATURN_USER': 'muchane', 'CONDA_EXE': '/opt/saturncloud/bin/conda', '_CE_M': '', 'BASE_URL': 'https://app.nvidia-oci.saturnenterprise.io', 'CONDA_BIN': '/opt/saturncloud/bin', 'HOSTNAME': 'w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v', 'SATURN_TOKEN': 'eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJhdWQiOlsiYXRsYXMiLCJzYXR1cm4tYXV0aC1wcm94eSJdLCJpc3MiOiJhdGxhcyIsInN1YiI6ImMwMzU4OWFkMTUyOTRjOTJhNmJlNDkwNDY3ZGEzMjVhIiwiaXNfcmVmcmVzaCI6ZmFsc2UsInJlc291cmNlIjoid29ya3NwYWNlOmE4NmVjYmI0MzFkOTQ4MTE4ZTU4M2U1YTllNmI4OWRmIiwic2NvcGUiOiJ3b3Jrc3BhY2U6YTg2ZWNiYjQzMWQ5NDgxMThlNTgzZTVhOWU2Yjg5ZGY6ZGFzazp3cml0ZSIsInVzZXJfaWQiOiIyZGI2MjEwOTc2Yjc0ZTA5YWY4Mzg0YjA1YjFkYjUzNSJ9.VQXvhHCsd23c6X9fbvu4pTmgmtes2i9V1DWJmdyNtJzOl3ym_Grz7DPVyKrzUcu0bjzFXic61JEkCj5NaYZgqFWeA7QnmPS1UvVR70PsAL_qiCaxj6Ns4Ot-RFH2MZ5kQSzfoK10woR3vY_dVzEmoBX3tfv25feAii-tOPLRjIRLBAgSDMyD9869EgneV4YVKVrL-JTnzKPh1y78KoolSM6C5xKBmARPBToswJuypI2XjGRQJWOgd8kvCTHcyi0y8KkhsiOr9VOXdjm9HPVfZKPX-z-3cNEf693H76I0hzrSIzoBQB8XslC6Cm1k66yJSViM4Ya-e7IbhHhzNt_BaZvwkGbuo0NgpqF-fQy8unPN3Uof2MxQxiiRTtcFNoY7kMkGGfV51COduS3Aive5yKwlMn9hX5qauBfznEB4NVzK2o9salob-DRcz7uahSrUrj17wHqWziOhOmKjy-MD-mi5DOoalQ52lU6h_p5O0mL9Vbdh_Ow_fcOANWaCW1Vgp-FLfLPqZmwURIL3k6NoP7rGi7bPHIpgyl7jqgUodaLtoMfvuIW_14VmEKmdAZixsHyTVV4hkKfPslHzC6ikb9itccuoOzxna94vUvXk5adJ-gHdEre38QUJvgu22fP59ANp1mJ3y4wO_isAeKHfurpkgRq-kUcNe7M6i9sW630', 'LANGUAGE': 'en_US.UTF-8', 'SATURN_RESOURCE_TYPE': 'SingleUserServer', 'SATURN_EMAIL': 'muchane@uchicago.edu', 'SATURN_SSH_ENABLED': 'true', 'SATURN_RESOURCE_NAME': 'lingua-run', 'NVIDIA_REQUIRE_CUDA': 'cuda>=12.1 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526', 'SATURN_IS_INTERNAL': 'true', 'SSH_AUTH_SOCK': '/tmp/ssh-XXXX1VrxrN/agent.1416665', 'NV_NVTX_VERSION': '12.1.66-1', 'NB_UID': '1000', 'DASK_DISTRIBUTED__DEPLOY__SCHEDULER_INFO_INTERVAL': '10s', 'NV_LIBCUSPARSE_VERSION': '12.0.2.55-1', 'NV_LIBNPP_VERSION': '12.0.2.50-1', 'NCCL_VERSION': '2.17.1-1', 'SATURN_BASE_URL': 'https://app.nvidia-oci.saturnenterprise.io', 'SATURN_REPO_HOME': '/home/jovyan', 'PWD': '/home/jovyan/shared/muchane/lingua-repo/Efficient-LLMs', 'LOGNAME': 'jovyan', 'NV_CUDNN_PACKAGE': 'libcudnn8=8.9.0.131-1+cuda12.1', 'CONDA_PREFIX': '/opt/saturncloud/envs/lingua_250731', 'KUBERNETES_PORT_12250_TCP_ADDR': '10.96.0.1', 'SATURN_ORG': 'AutomatingDataSciencewithTabularLanguag', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'SATURN_USER_PYTHON': '/opt/saturncloud/envs/saturn/bin/python', 'NV_LIBNPP_PACKAGE': 'libnpp-12-1=12.0.2.50-1', 'SATURN_JUPYTER_BASE_URL': 'http://lingua-8gpu-server.nvidia-oci.saturnenterprise.io', 'NVIDIA_PRODUCT_NAME': 'CUDA', 'SATURN_USERNAME': 'muchane', 'MOTD_SHOWN': 'pam', 'NV_CUDA_CUDART_VERSION': '12.1.55-1', 'HOME': '/home/jovyan', 'LANG': 'en_US.UTF-8', 'KUBERNETES_PORT_443_TCP': 'tcp://10.96.0.1:443', 'SATURN_SYSTEM_PYTHON': '/opt/saturncloud/bin/python', 'SATURN_GIT_SCRATCH_PATH': '/home/jovyan/.git-scratch', 'KUBERNETES_PORT_12250_TCP': 'tcp://10.96.0.1:12250', 'SATURN_PROJECT_NAME': 'lingua-run', 'NVIDIA_CUDA_END_OF_LIFE': '1', 'CUDA_VERSION': '12.1.0', 'BOKEH_ALLOW_WS_ORIGIN': 'lingua-8gpu-server.nvidia-oci.saturnenterprise.io', 'NV_LIBCUBLAS_PACKAGE': 'libcublas-12-1=12.1.0.26-1', 'CONDA_PROMPT_MODIFIER': '(lingua_250731) ', 'SSH_CONNECTION': '192.168.247.122 42216 192.168.45.217 22', 'NV_LIBCUBLAS_PACKAGE_NAME': 'libcublas-12-1', 'JUPYTER_IMAGE_SPEC': 'public.ecr.aws/saturncloud/saturn-python-pytorch:2024.08.01', 'DASK_DISTRIBUTED__DEPLOY__CLUSTER_REPR_INTERVAL': '10s', 'SATURN_VERSION': '2025.02.01-18', 'CONDA_OVERRIDE_CUDA': '12.1', 'TERM': 'xterm', '_CE_CONDA': '', 'KUBERNETES_PORT_12250_TCP_PORT': '12250', 'USER': 'jovyan', 'NV_CUDNN_VERSION': '8.9.0.131', 'CONDA_SHLVL': '1', 'KUBERNETES_SERVICE_PORT_PROXYMUX': '12250', 'SHLVL': '1', 'CONDA_DIR': '/opt/saturncloud', 'NV_CUDA_LIB_VERSION': '12.1.0-1', 'NVARCH': 'x86_64', 'KUBERNETES_PORT_443_TCP_PROTO': 'tcp', 'KUBERNETES_PORT_443_TCP_ADDR': '10.96.0.1', 'NV_CUDA_COMPAT_PACKAGE': 'cuda-compat-12-1', 'CONDA_PYTHON_EXE': '/opt/saturncloud/bin/python', 'NV_LIBNCCL_PACKAGE': 'libnccl2=2.17.1-1+cuda12.1', 'LD_LIBRARY_PATH': '/usr/local/nvidia/lib:/usr/local/nvidia/lib64', 'SATURN_JUPYTER_BASE_DOMAIN': 'lingua-8gpu-server.nvidia-oci.saturnenterprise.io', 'SATURN_IMAGE': 'public.ecr.aws/saturncloud/saturn-python-pytorch:2024.08.01', 'CONDA_INSTALL_DIR': '/opt/saturncloud', 'SSH_CLIENT': '192.168.247.122 42216 22', 'CONDA_DEFAULT_ENV': 'lingua_250731', 'SATURN_GROUP': '', 'SATURN_PYTHON_INIT_SCRIPT': '/home/jovyan/.saturn/setup_jupyterlab.py', 'NB_USER': 'jovyan', 'KUBERNETES_SERVICE_HOST': '10.96.0.1', 'LC_ALL': 'en_US.UTF-8', 'KUBERNETES_PORT': 'tcp://10.96.0.1:443', 'KUBERNETES_PORT_443_TCP_PORT': '443', 'PATH': '/opt/saturncloud/envs/lingua_250731/bin:/opt/saturncloud/condabin:/opt/saturncloud/envs/saturn/bin:/opt/saturncloud/bin:/home/jovyan/.local/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'NV_LIBNCCL_PACKAGE_NAME': 'libnccl2', 'NV_LIBNCCL_PACKAGE_VERSION': '2.17.1-1', 'NB_PYTHON_PREFIX': '/opt/saturncloud/envs/saturn', 'KUBERNETES_PORT_12250_TCP_PROTO': 'tcp', 'SSH_TTY': '/dev/pts/3', 'DEBIAN_FRONTEND': 'noninteractive', 'OLDPWD': '/home/jovyan', '_': '/opt/saturncloud/envs/lingua_250731/bin/torchrun', 'OMP_NUM_THREADS': '1', 'LOCAL_RANK': '1', 'RANK': '1', 'GROUP_RANK': '0', 'ROLE_RANK': '1', 'ROLE_NAME': 'default', 'LOCAL_WORLD_SIZE': '8', 'WORLD_SIZE': '8', 'GROUP_WORLD_SIZE': '1', 'ROLE_WORLD_SIZE': '8', 'MASTER_ADDR': '127.0.0.1', 'MASTER_PORT': '29500', 'TORCHELASTIC_RESTART_COUNT': '0', 'TORCHELASTIC_MAX_RESTARTS': '0', 'TORCHELASTIC_RUN_ID': 'none', 'TORCHELASTIC_USE_AGENT_STORE': 'True', 'TORCH_NCCL_ASYNC_ERROR_HANDLING': '1', 'TORCHELASTIC_ERROR_FILE': '/tmp/torchelastic__o4h9nnb/none_v6wepvnq/attempt_0/1/error.json', 'CUDA_MODULE_LOADING': 'LAZY', 'KMP_DUPLICATE_LIB_OK': 'True', 'KMP_INIT_AT_FORK': 'FALSE', 'MKL_SERVICE_FORCE_INTEL': 'GNU', 'MKL_NUM_THREADS': '1', 'ENABLE_INTRA_NODE_COMM': '1', 'TORCH_NCCL_AVOID_RECORD_STREAMS': '1', 'NCCL_IB_TIMEOUT': '22', 'NCCL_DEBUG': 'INFO', 'TRITON_CACHE_DIR': '/tmp/tmpn3f14l7k'})
4: INFO    25-08-06 20:45:51.307850 - 0:00:10 - Initializing global process group with all 8 ranks
7: INFO    25-08-06 20:45:51.308077 - 0:00:10 - Initializing global process group with all 8 ranks
3: INFO    25-08-06 20:45:51.314714 - 0:00:10 - Initializing global process group with all 8 ranks
5: INFO    25-08-06 20:45:51.319308 - 0:00:10 - Initializing global process group with all 8 ranks
6: INFO    25-08-06 20:45:51.331806 - 0:00:10 - Initializing global process group with all 8 ranks
1: INFO    25-08-06 20:45:51.353313 - 0:00:10 - Initializing global process group with all 8 ranks
2: INFO    25-08-06 20:45:51.359314 - 0:00:10 - Initializing global process group with all 8 ranks
[W806 20:45:52.682946186 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W806 20:45:52.682971506 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W806 20:45:52.683126509 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W806 20:45:52.683663130 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W806 20:45:52.683715491 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W806 20:45:52.683734581 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W806 20:45:52.683875764 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
[W806 20:45:52.684086808 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())
7: INFO    25-08-06 20:45:52.271565 - 0:00:11 - Rank 7 is part of the distillation group. Distill ranks: [7]
7: INFO    25-08-06 20:45:52.271782 - 0:00:11 - [InterProcessComm] Initialized. is_training=False, distill_rank=7, training_world_size=7
7: INFO    25-08-06 20:45:52.271831 - 0:00:11 - Rank 7 starting distillation process
7: INFO    25-08-06 20:45:52.271881 - 0:00:11 - [DISTILL] Rank 7 initializing distillation process
7: INFO    25-08-06 20:45:52.271912 - 0:00:11 - [DISTILL] Loading HuggingFace model...
5: INFO    25-08-06 20:45:52.751425 - 0:00:11 - Rank 5 is part of the training group. Training ranks: [0, 1, 2, 3, 4, 5, 6]
4: INFO    25-08-06 20:45:52.751460 - 0:00:11 - Rank 4 is part of the training group. Training ranks: [0, 1, 2, 3, 4, 5, 6]
0: INFO    25-08-06 20:45:52.751553 - 0:00:11 - Rank 0 is part of the training group. Training ranks: [0, 1, 2, 3, 4, 5, 6]
5: INFO    25-08-06 20:45:52.751692 - 0:00:11 - [InterProcessComm] Initialized. is_training=True, distill_rank=7, training_world_size=7
4: INFO    25-08-06 20:45:52.751711 - 0:00:11 - [InterProcessComm] Initialized. is_training=True, distill_rank=7, training_world_size=7
5: INFO    25-08-06 20:45:52.751744 - 0:00:11 - Rank 5 starting training process
4: INFO    25-08-06 20:45:52.751762 - 0:00:11 - Rank 4 starting training process
0: INFO    25-08-06 20:45:52.751870 - 0:00:11 - [InterProcessComm] Initialized. is_training=True, distill_rank=7, training_world_size=7
0: INFO    25-08-06 20:45:52.751944 - 0:00:11 - Rank 0 starting training process
2: INFO    25-08-06 20:45:52.751907 - 0:00:11 - Rank 2 is part of the training group. Training ranks: [0, 1, 2, 3, 4, 5, 6]
1: INFO    25-08-06 20:45:52.751988 - 0:00:11 - Rank 1 is part of the training group. Training ranks: [0, 1, 2, 3, 4, 5, 6]
2: INFO    25-08-06 20:45:52.752142 - 0:00:11 - [InterProcessComm] Initialized. is_training=True, distill_rank=7, training_world_size=7
2: INFO    25-08-06 20:45:52.752206 - 0:00:11 - Rank 2 starting training process
6: INFO    25-08-06 20:45:52.752139 - 0:00:11 - Rank 6 is part of the training group. Training ranks: [0, 1, 2, 3, 4, 5, 6]
1: INFO    25-08-06 20:45:52.752221 - 0:00:11 - [InterProcessComm] Initialized. is_training=True, distill_rank=7, training_world_size=7
1: INFO    25-08-06 20:45:52.752285 - 0:00:11 - Rank 1 starting training process
6: INFO    25-08-06 20:45:52.752427 - 0:00:11 - [InterProcessComm] Initialized. is_training=True, distill_rank=7, training_world_size=7
6: INFO    25-08-06 20:45:52.752500 - 0:00:11 - Rank 6 starting training process
3: INFO    25-08-06 20:45:52.758387 - 0:00:12 - Rank 3 is part of the training group. Training ranks: [0, 1, 2, 3, 4, 5, 6]
3: INFO    25-08-06 20:45:52.758611 - 0:00:12 - [InterProcessComm] Initialized. is_training=True, distill_rank=7, training_world_size=7
3: INFO    25-08-06 20:45:52.758664 - 0:00:12 - Rank 3 starting training process
5: INFO    25-08-06 20:45:53.251276 - 0:00:12 - Starting job: Baseline
4: INFO    25-08-06 20:45:53.251338 - 0:00:12 - Starting job: Baseline
0: INFO    25-08-06 20:45:53.251400 - 0:00:12 - Starting job: Baseline
3: INFO    25-08-06 20:45:53.251449 - 0:00:12 - Starting job: Baseline
6: INFO    25-08-06 20:45:53.251522 - 0:00:12 - Starting job: Baseline
5: INFO    25-08-06 20:45:53.251801 - 0:00:12 - Running on dp rank : 5
4: INFO    25-08-06 20:45:53.251842 - 0:00:12 - Running on dp rank : 4
5: INFO    25-08-06 20:45:53.251856 - 0:00:12 - Running on dp size : 7
4: INFO    25-08-06 20:45:53.251898 - 0:00:12 - Running on dp size : 7
3: INFO    25-08-06 20:45:53.251908 - 0:00:12 - Running on dp rank : 3
0: INFO    25-08-06 20:45:53.251931 - 0:00:12 - Running on dp rank : 0
3: INFO    25-08-06 20:45:53.251962 - 0:00:12 - Running on dp size : 7
0: INFO    25-08-06 20:45:53.252008 - 0:00:12 - Running on dp size : 7
6: INFO    25-08-06 20:45:53.252134 - 0:00:12 - Running on dp rank : 6
6: INFO    25-08-06 20:45:53.252191 - 0:00:12 - Running on dp size : 7
2: INFO    25-08-06 20:45:53.252298 - 0:00:12 - Starting job: Baseline
2: INFO    25-08-06 20:45:53.253050 - 0:00:12 - Running on dp rank : 2
2: INFO    25-08-06 20:45:53.253111 - 0:00:12 - Running on dp size : 7
4: INFO    25-08-06 20:45:53.253104 - 0:00:12 - Building model
3: INFO    25-08-06 20:45:53.253104 - 0:00:12 - Building model
0: INFO    25-08-06 20:45:53.253112 - 0:00:12 - Building model
5: INFO    25-08-06 20:45:53.253190 - 0:00:12 - Building model
6: INFO    25-08-06 20:45:53.253503 - 0:00:12 - Building model
2: INFO    25-08-06 20:45:53.254569 - 0:00:12 - Building model
1: INFO    25-08-06 20:45:53.257761 - 0:00:12 - Starting job: Baseline
1: INFO    25-08-06 20:45:53.258235 - 0:00:12 - Running on dp rank : 1
1: INFO    25-08-06 20:45:53.258289 - 0:00:12 - Running on dp size : 7
1: INFO    25-08-06 20:45:53.259459 - 0:00:12 - Building model
0: INFO    25-08-06 20:45:53.276989 - 0:00:12 - Model is built !
4: INFO    25-08-06 20:45:53.277302 - 0:00:12 - Model is built !
3: INFO    25-08-06 20:45:53.277497 - 0:00:12 - Model is built !
6: INFO    25-08-06 20:45:53.278045 - 0:00:12 - Model is built !
5: INFO    25-08-06 20:45:53.278053 - 0:00:12 - Model is built !
2: INFO    25-08-06 20:45:53.282105 - 0:00:12 - Model is built !
1: INFO    25-08-06 20:45:53.283565 - 0:00:12 - Model is built !
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1434414 [0] NCCL INFO Bootstrap : Using eth0:192.168.45.217<0>
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1434414 [0] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1434414 [0] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1434414 [0] NCCL INFO NET/Plugin: Using internal network plugin.
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1434414 [0] NCCL INFO cudaDriverVersion 12060
NCCL version 2.21.5+cuda12.4
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1434419 [5] NCCL INFO cudaDriverVersion 12060
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1434419 [5] NCCL INFO Bootstrap : Using eth0:192.168.45.217<0>
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1434419 [5] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1434419 [5] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1434419 [5] NCCL INFO NET/Plugin: Using internal network plugin.
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1434416 [2] NCCL INFO cudaDriverVersion 12060
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1434416 [2] NCCL INFO Bootstrap : Using eth0:192.168.45.217<0>
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1434416 [2] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1434416 [2] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1434416 [2] NCCL INFO NET/Plugin: Using internal network plugin.
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1434418 [4] NCCL INFO cudaDriverVersion 12060
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1434418 [4] NCCL INFO Bootstrap : Using eth0:192.168.45.217<0>
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1434418 [4] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1434418 [4] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1434418 [4] NCCL INFO NET/Plugin: Using internal network plugin.
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1434420 [6] NCCL INFO cudaDriverVersion 12060
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1434417 [3] NCCL INFO cudaDriverVersion 12060
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1434420 [6] NCCL INFO Bootstrap : Using eth0:192.168.45.217<0>
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1434417 [3] NCCL INFO Bootstrap : Using eth0:192.168.45.217<0>
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1434415 [1] NCCL INFO cudaDriverVersion 12060
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1434417 [3] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1434417 [3] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1434417 [3] NCCL INFO NET/Plugin: Using internal network plugin.
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1434420 [6] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1434420 [6] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1434420 [6] NCCL INFO NET/Plugin: Using internal network plugin.
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1434415 [1] NCCL INFO Bootstrap : Using eth0:192.168.45.217<0>
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1434415 [1] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1434415 [1] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1434415 [1] NCCL INFO NET/Plugin: Using internal network plugin.
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Failed to open libibverbs.so[.1]
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO NET/Socket : Using [0]eth0:192.168.45.217<0>
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Using non-device net plugin version 0
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Using network Socket
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Failed to open libibverbs.so[.1]
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO NET/Socket : Using [0]eth0:192.168.45.217<0>
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Using non-device net plugin version 0
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Using network Socket
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Failed to open libibverbs.so[.1]
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO NET/Socket : Using [0]eth0:192.168.45.217<0>
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Using non-device net plugin version 0
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Using network Socket
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Failed to open libibverbs.so[.1]
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO NET/Socket : Using [0]eth0:192.168.45.217<0>
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Using non-device net plugin version 0
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Using network Socket
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Failed to open libibverbs.so[.1]
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO NET/Socket : Using [0]eth0:192.168.45.217<0>
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Using non-device net plugin version 0
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Using network Socket
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Failed to open libibverbs.so[.1]
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO NET/Socket : Using [0]eth0:192.168.45.217<0>
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Using non-device net plugin version 0
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Using network Socket
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Failed to open libibverbs.so[.1]
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO NET/Socket : Using [0]eth0:192.168.45.217<0>
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Using non-device net plugin version 0
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Using network Socket
7: INFO    25-08-06 20:45:54.272042 - 0:00:13 - [DISTILL] Model loaded successfully!
7: INFO    25-08-06 20:45:54.272175 - 0:00:13 - [DISTILL] Signaling model ready to training ranks
7: INFO    25-08-06 20:45:54.272244 - 0:00:13 - [InterProcessComm.sync_model_loaded] Rank 7 entering sync
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1434421 [7] NCCL INFO cudaDriverVersion 12060
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1434421 [7] NCCL INFO Bootstrap : Using eth0:192.168.45.217<0>
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1434421 [7] NCCL INFO NET/Plugin: No plugin found (libnccl-net.so)
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1434421 [7] NCCL INFO NET/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-net.so
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1434421 [7] NCCL INFO NET/Plugin: Using internal network plugin.
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Failed to open libibverbs.so[.1]
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO NET/Socket : Using [0]eth0:192.168.45.217<0>
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Using non-device net plugin version 0
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Using network Socket
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO ncclCommInitRank comm 0xe5db810 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId f000 commId 0xfcdc102c68dfe030 - Init START
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO ncclCommInitRank comm 0xf2deb10 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 53000 commId 0xfcdc102c68dfe030 - Init START
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO ncclCommInitRank comm 0x97beaf0 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId da000 commId 0xfcdc102c68dfe030 - Init START
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO ncclCommInitRank comm 0xef68550 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId 8c000 commId 0xfcdc102c68dfe030 - Init START
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO ncclCommInitRank comm 0xf8a4840 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId d6000 commId 0xfcdc102c68dfe030 - Init START
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO ncclCommInitRank comm 0x10185690 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId 91000 commId 0xfcdc102c68dfe030 - Init START
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO ncclCommInitRank comm 0xed136d0 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 50000 commId 0xfcdc102c68dfe030 - Init START
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO ncclCommInitRank comm 0xf31c9a0 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 15000 commId 0xfcdc102c68dfe030 - Init START
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Setting affinity for GPU 1 to ffff0000,00000000,00000000,00000000,ffff0000,00000000
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO NVLS multicast support is not available on dev 1
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Setting affinity for GPU 7 to ffff0000,00000000,00000000,00000000,ffff0000,00000000,00000000
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO NVLS multicast support is not available on dev 7
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Setting affinity for GPU 6 to ffff0000,00000000,00000000,00000000,ffff0000,00000000,00000000
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO NVLS multicast support is not available on dev 6
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Setting affinity for GPU 0 to ffff0000,00000000,00000000,00000000,ffff0000,00000000
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Setting affinity for GPU 2 to ffff0000,00000000,00000000,00000000,ffff0000
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO NVLS multicast support is not available on dev 0
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO NVLS multicast support is not available on dev 2
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Setting affinity for GPU 4 to 7fff0000,00000000,00000000,00000000,ffff0000,00000000,00000000,00000000
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO NVLS multicast support is not available on dev 4
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Setting affinity for GPU 5 to 7fff0000,00000000,00000000,00000000,ffff0000,00000000,00000000,00000000
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Setting affinity for GPU 3 to ffff0000,00000000,00000000,00000000,ffff0000
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO NVLS multicast support is not available on dev 5
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO NVLS multicast support is not available on dev 3
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO comm 0x97beaf0 rank 7 nRanks 8 nNodes 1 localRanks 8 localRank 7 MNNVL 0
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO comm 0xf8a4840 rank 6 nRanks 8 nNodes 1 localRanks 8 localRank 6 MNNVL 0
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO comm 0x10185690 rank 5 nRanks 8 nNodes 1 localRanks 8 localRank 5 MNNVL 0
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO comm 0xed136d0 rank 2 nRanks 8 nNodes 1 localRanks 8 localRank 2 MNNVL 0
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO comm 0xef68550 rank 4 nRanks 8 nNodes 1 localRanks 8 localRank 4 MNNVL 0
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] -1/-1/-1->7->6 [3] -1/-1/-1->7->6 [4] -1/-1/-1->7->6 [5] -1/-1/-1->7->6 [6] -1/-1/-1->7->6 [7] -1/-1/-1->7->6 [8] -1/-1/-1->7->6 [9] -1/-1/-1->7->6 [10] -1/-1/-1->7->6 [11] -1/-1/-1->7->6 [12] -1/-1/-1->7->6 [13] -1/-1/-1->7->6 [14] -1/-1/-1->7->6 [15] -1/-1/-1->7->6 [16] -1/-1/-1->7->6 [17] -1/-1/-1->7->6 [18] -1/-1/-1->7->6 [19] -1/-1/-1->7->6 [20] -1/-1/-1->7->6 [21] -1/-1/-1->7->6 [22] -1/-1/-1->7->6 [23] -1/-1/-1->7->6
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/-1/-1->6->5 [4] 7/-1/-1->6->5 [5] 7/-1/-1->6->5 [6] 7/-1/-1->6->5 [7] 7/-1/-1->6->5 [8] 7/-1/-1->6->5 [9] 7/-1/-1->6->5 [10] 7/-1/-1->6->5 [11] 7/-1/-1->6->5 [12] 7/-1/-1->6->5 [13] 7/-1/-1->6->5 [14] 7/-1/-1->6->5 [15] 7/-1/-1->6->5 [16] 7/-1/-1->6->5 [17] 7/-1/-1->6->5 [18] 7/-1/-1->6->5 [19] 7/-1/-1->6->5 [20] 7/-1/-1->6->5 [21] 7/-1/-1->6->5 [22] 7/-1/-1->6->5 [23] 7/-1/-1->6->5
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] 6/-1/-1->5->4 [4] 6/-1/-1->5->4 [5] 6/-1/-1->5->4 [6] 6/-1/-1->5->4 [7] 6/-1/-1->5->4 [8] 6/-1/-1->5->4 [9] 6/-1/-1->5->4 [10] 6/-1/-1->5->4 [11] 6/-1/-1->5->4 [12] 6/-1/-1->5->4 [13] 6/-1/-1->5->4 [14] 6/-1/-1->5->4 [15] 6/-1/-1->5->4 [16] 6/-1/-1->5->4 [17] 6/-1/-1->5->4 [18] 6/-1/-1->5->4 [19] 6/-1/-1->5->4 [20] 6/-1/-1->5->4 [21] 6/-1/-1->5->4 [22] 6/-1/-1->5->4 [23] 6/-1/-1->5->4
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO comm 0xf2deb10 rank 3 nRanks 8 nNodes 1 localRanks 8 localRank 3 MNNVL 0
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO P2P Chunksize set to 524288
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO comm 0xf31c9a0 rank 1 nRanks 8 nNodes 1 localRanks 8 localRank 1 MNNVL 0
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO P2P Chunksize set to 524288
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO P2P Chunksize set to 524288
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO comm 0xe5db810 rank 0 nRanks 8 nNodes 1 localRanks 8 localRank 0 MNNVL 0
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3 [2] 5/-1/-1->4->3 [3] 5/-1/-1->4->3 [4] 5/-1/-1->4->3 [5] 5/-1/-1->4->3 [6] 5/-1/-1->4->3 [7] 5/-1/-1->4->3 [8] 5/-1/-1->4->3 [9] 5/-1/-1->4->3 [10] 5/-1/-1->4->3 [11] 5/-1/-1->4->3 [12] 5/-1/-1->4->3 [13] 5/-1/-1->4->3 [14] 5/-1/-1->4->3 [15] 5/-1/-1->4->3 [16] 5/-1/-1->4->3 [17] 5/-1/-1->4->3 [18] 5/-1/-1->4->3 [19] 5/-1/-1->4->3 [20] 5/-1/-1->4->3 [21] 5/-1/-1->4->3 [22] 5/-1/-1->4->3 [23] 5/-1/-1->4->3
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] 3/-1/-1->2->1 [11] 3/-1/-1->2->1 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1 [16] 3/-1/-1->2->1 [17] 3/-1/-1->2->1 [18] 3/-1/-1->2->1 [19] 3/-1/-1->2->1 [20] 3/-1/-1->2->1 [21] 3/-1/-1->2->1 [22] 3/-1/-1->2->1 [23] 3/-1/-1->2->1
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2 [2] 4/-1/-1->3->2 [3] 4/-1/-1->3->2 [4] 4/-1/-1->3->2 [5] 4/-1/-1->3->2 [6] 4/-1/-1->3->2 [7] 4/-1/-1->3->2 [8] 4/-1/-1->3->2 [9] 4/-1/-1->3->2 [10] 4/-1/-1->3->2 [11] 4/-1/-1->3->2 [12] 4/-1/-1->3->2 [13] 4/-1/-1->3->2 [14] 4/-1/-1->3->2 [15] 4/-1/-1->3->2 [16] 4/-1/-1->3->2 [17] 4/-1/-1->3->2 [18] 4/-1/-1->3->2 [19] 4/-1/-1->3->2 [20] 4/-1/-1->3->2 [21] 4/-1/-1->3->2 [22] 4/-1/-1->3->2 [23] 4/-1/-1->3->2
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO P2P Chunksize set to 524288
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->0 [10] 2/-1/-1->1->0 [11] 2/-1/-1->1->0 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0 [16] 2/-1/-1->1->0 [17] 2/-1/-1->1->0 [18] 2/-1/-1->1->0 [19] 2/-1/-1->1->0 [20] 2/-1/-1->1->0 [21] 2/-1/-1->1->0 [22] 2/-1/-1->1->0 [23] 2/-1/-1->1->0
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO P2P Chunksize set to 524288
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO P2P Chunksize set to 524288
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO P2P Chunksize set to 524288
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Channel 00/24 :    0   1   2   3   4   5   6   7
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Channel 01/24 :    0   1   2   3   4   5   6   7
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Channel 02/24 :    0   1   2   3   4   5   6   7
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Channel 03/24 :    0   1   2   3   4   5   6   7
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Channel 04/24 :    0   1   2   3   4   5   6   7
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Channel 05/24 :    0   1   2   3   4   5   6   7
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Channel 06/24 :    0   1   2   3   4   5   6   7
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Channel 07/24 :    0   1   2   3   4   5   6   7
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Channel 08/24 :    0   1   2   3   4   5   6   7
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Channel 09/24 :    0   1   2   3   4   5   6   7
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Channel 10/24 :    0   1   2   3   4   5   6   7
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Channel 11/24 :    0   1   2   3   4   5   6   7
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Channel 12/24 :    0   1   2   3   4   5   6   7
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Channel 13/24 :    0   1   2   3   4   5   6   7
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Channel 14/24 :    0   1   2   3   4   5   6   7
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Channel 15/24 :    0   1   2   3   4   5   6   7
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Channel 16/24 :    0   1   2   3   4   5   6   7
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Channel 17/24 :    0   1   2   3   4   5   6   7
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Channel 18/24 :    0   1   2   3   4   5   6   7
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Channel 19/24 :    0   1   2   3   4   5   6   7
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Channel 20/24 :    0   1   2   3   4   5   6   7
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Channel 21/24 :    0   1   2   3   4   5   6   7
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Channel 22/24 :    0   1   2   3   4   5   6   7
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Channel 23/24 :    0   1   2   3   4   5   6   7
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 1/-1/-1->0->-1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 1/-1/-1->0->-1 [17] 1/-1/-1->0->-1 [18] 1/-1/-1->0->-1 [19] 1/-1/-1->0->-1 [20] 1/-1/-1->0->-1 [21] 1/-1/-1->0->-1 [22] 1/-1/-1->0->-1 [23] 1/-1/-1->0->-1
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO P2P Chunksize set to 524288
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Channel 02/0 : 1[1] -> 2[2] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Channel 03/0 : 1[1] -> 2[2] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Channel 04/0 : 1[1] -> 2[2] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Channel 00/0 : 7[7] -> 0[0] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Channel 05/0 : 1[1] -> 2[2] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Channel 01/0 : 7[7] -> 0[0] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Channel 06/0 : 1[1] -> 2[2] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Channel 02/0 : 7[7] -> 0[0] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Channel 07/0 : 1[1] -> 2[2] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Channel 03/0 : 7[7] -> 0[0] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Channel 00/0 : 6[6] -> 7[7] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Channel 08/0 : 1[1] -> 2[2] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Channel 04/0 : 7[7] -> 0[0] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Channel 01/0 : 6[6] -> 7[7] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Channel 00/0 : 4[4] -> 5[5] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Channel 09/0 : 1[1] -> 2[2] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Channel 05/0 : 7[7] -> 0[0] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Channel 02/0 : 6[6] -> 7[7] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Channel 02/0 : 2[2] -> 3[3] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Channel 01/0 : 4[4] -> 5[5] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Channel 10/0 : 1[1] -> 2[2] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Channel 06/0 : 7[7] -> 0[0] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Channel 03/0 : 6[6] -> 7[7] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Channel 03/0 : 2[2] -> 3[3] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Channel 02/0 : 4[4] -> 5[5] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Channel 11/0 : 1[1] -> 2[2] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Channel 07/0 : 7[7] -> 0[0] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Channel 04/0 : 6[6] -> 7[7] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Channel 04/0 : 2[2] -> 3[3] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Channel 03/0 : 4[4] -> 5[5] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Channel 12/0 : 1[1] -> 2[2] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Channel 08/0 : 7[7] -> 0[0] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Channel 05/0 : 6[6] -> 7[7] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Channel 05/0 : 2[2] -> 3[3] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Channel 04/0 : 4[4] -> 5[5] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Channel 13/0 : 1[1] -> 2[2] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Channel 00/0 : 5[5] -> 6[6] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Channel 09/0 : 7[7] -> 0[0] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Channel 06/0 : 6[6] -> 7[7] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Channel 05/0 : 4[4] -> 5[5] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Channel 06/0 : 2[2] -> 3[3] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Channel 14/0 : 1[1] -> 2[2] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Channel 01/0 : 5[5] -> 6[6] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Channel 10/0 : 7[7] -> 0[0] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Channel 07/0 : 6[6] -> 7[7] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Channel 06/0 : 4[4] -> 5[5] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Channel 07/0 : 2[2] -> 3[3] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Channel 15/0 : 1[1] -> 2[2] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Channel 02/0 : 5[5] -> 6[6] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Channel 11/0 : 7[7] -> 0[0] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Channel 08/0 : 6[6] -> 7[7] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Channel 07/0 : 4[4] -> 5[5] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Channel 08/0 : 2[2] -> 3[3] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Channel 03/0 : 5[5] -> 6[6] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Channel 16/0 : 1[1] -> 2[2] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Channel 12/0 : 7[7] -> 0[0] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Channel 09/0 : 6[6] -> 7[7] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Channel 00/0 : 3[3] -> 4[4] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Channel 08/0 : 4[4] -> 5[5] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Channel 09/0 : 2[2] -> 3[3] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Channel 04/0 : 5[5] -> 6[6] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Channel 17/0 : 1[1] -> 2[2] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Channel 13/0 : 7[7] -> 0[0] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Channel 10/0 : 6[6] -> 7[7] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Channel 01/0 : 3[3] -> 4[4] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Channel 09/0 : 4[4] -> 5[5] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Channel 10/0 : 2[2] -> 3[3] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Channel 05/0 : 5[5] -> 6[6] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Channel 18/0 : 1[1] -> 2[2] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Channel 14/0 : 7[7] -> 0[0] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Channel 11/0 : 6[6] -> 7[7] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Channel 02/0 : 3[3] -> 4[4] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Channel 10/0 : 4[4] -> 5[5] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Channel 11/0 : 2[2] -> 3[3] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Channel 06/0 : 5[5] -> 6[6] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Channel 19/0 : 1[1] -> 2[2] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Channel 15/0 : 7[7] -> 0[0] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Channel 12/0 : 6[6] -> 7[7] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Channel 03/0 : 3[3] -> 4[4] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Channel 11/0 : 4[4] -> 5[5] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Channel 12/0 : 2[2] -> 3[3] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Channel 07/0 : 5[5] -> 6[6] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Channel 20/0 : 1[1] -> 2[2] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Channel 16/0 : 7[7] -> 0[0] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Channel 13/0 : 6[6] -> 7[7] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Channel 04/0 : 3[3] -> 4[4] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Channel 12/0 : 4[4] -> 5[5] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Channel 13/0 : 2[2] -> 3[3] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Channel 08/0 : 5[5] -> 6[6] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Channel 21/0 : 1[1] -> 2[2] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Channel 14/0 : 6[6] -> 7[7] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Channel 05/0 : 3[3] -> 4[4] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Channel 13/0 : 4[4] -> 5[5] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Channel 14/0 : 2[2] -> 3[3] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Channel 09/0 : 5[5] -> 6[6] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Channel 22/0 : 1[1] -> 2[2] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Channel 15/0 : 6[6] -> 7[7] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Channel 06/0 : 3[3] -> 4[4] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Channel 14/0 : 4[4] -> 5[5] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Channel 15/0 : 2[2] -> 3[3] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Channel 10/0 : 5[5] -> 6[6] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Channel 23/0 : 1[1] -> 2[2] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Channel 16/0 : 6[6] -> 7[7] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Channel 07/0 : 3[3] -> 4[4] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Channel 04/0 : 0[0] -> 1[1] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Channel 16/0 : 2[2] -> 3[3] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Channel 11/0 : 5[5] -> 6[6] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Channel 17/0 : 6[6] -> 7[7] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Channel 08/0 : 3[3] -> 4[4] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Channel 05/0 : 0[0] -> 1[1] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Channel 17/0 : 2[2] -> 3[3] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Channel 12/0 : 5[5] -> 6[6] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Channel 17/0 : 7[7] -> 0[0] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Channel 18/0 : 6[6] -> 7[7] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Channel 09/0 : 3[3] -> 4[4] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Channel 06/0 : 0[0] -> 1[1] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Channel 18/0 : 2[2] -> 3[3] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Channel 13/0 : 5[5] -> 6[6] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Channel 18/0 : 7[7] -> 0[0] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Channel 19/0 : 6[6] -> 7[7] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Channel 10/0 : 3[3] -> 4[4] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Channel 07/0 : 0[0] -> 1[1] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Channel 19/0 : 2[2] -> 3[3] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Channel 14/0 : 5[5] -> 6[6] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Channel 19/0 : 7[7] -> 0[0] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Channel 20/0 : 6[6] -> 7[7] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Channel 11/0 : 3[3] -> 4[4] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Channel 08/0 : 0[0] -> 1[1] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Channel 20/0 : 2[2] -> 3[3] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Channel 15/0 : 5[5] -> 6[6] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Channel 20/0 : 7[7] -> 0[0] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Channel 21/0 : 6[6] -> 7[7] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Channel 12/0 : 3[3] -> 4[4] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Channel 09/0 : 0[0] -> 1[1] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Channel 21/0 : 2[2] -> 3[3] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Channel 15/0 : 4[4] -> 5[5] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Channel 16/0 : 5[5] -> 6[6] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Channel 21/0 : 7[7] -> 0[0] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Channel 22/0 : 6[6] -> 7[7] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Channel 13/0 : 3[3] -> 4[4] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Channel 10/0 : 0[0] -> 1[1] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Channel 22/0 : 2[2] -> 3[3] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Channel 16/0 : 4[4] -> 5[5] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Channel 17/0 : 5[5] -> 6[6] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Channel 22/0 : 7[7] -> 0[0] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Channel 23/0 : 6[6] -> 7[7] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Channel 14/0 : 3[3] -> 4[4] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Channel 11/0 : 0[0] -> 1[1] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Channel 23/0 : 2[2] -> 3[3] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Channel 17/0 : 4[4] -> 5[5] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Channel 18/0 : 5[5] -> 6[6] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Channel 23/0 : 7[7] -> 0[0] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Channel 15/0 : 3[3] -> 4[4] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Channel 12/0 : 0[0] -> 1[1] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Channel 18/0 : 4[4] -> 5[5] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Channel 19/0 : 5[5] -> 6[6] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Channel 16/0 : 3[3] -> 4[4] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Channel 13/0 : 0[0] -> 1[1] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Channel 19/0 : 4[4] -> 5[5] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Channel 20/0 : 5[5] -> 6[6] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Channel 17/0 : 3[3] -> 4[4] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Channel 14/0 : 0[0] -> 1[1] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Channel 20/0 : 4[4] -> 5[5] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Channel 21/0 : 5[5] -> 6[6] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Channel 18/0 : 3[3] -> 4[4] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Channel 15/0 : 0[0] -> 1[1] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Channel 21/0 : 4[4] -> 5[5] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Channel 22/0 : 5[5] -> 6[6] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Channel 19/0 : 3[3] -> 4[4] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Channel 16/0 : 0[0] -> 1[1] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Channel 22/0 : 4[4] -> 5[5] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Channel 23/0 : 5[5] -> 6[6] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Channel 20/0 : 3[3] -> 4[4] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Channel 17/0 : 0[0] -> 1[1] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Channel 23/0 : 4[4] -> 5[5] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Channel 21/0 : 3[3] -> 4[4] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Channel 18/0 : 0[0] -> 1[1] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Channel 22/0 : 3[3] -> 4[4] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Channel 19/0 : 0[0] -> 1[1] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Channel 23/0 : 3[3] -> 4[4] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Channel 20/0 : 0[0] -> 1[1] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Channel 21/0 : 0[0] -> 1[1] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Channel 22/0 : 0[0] -> 1[1] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Channel 23/0 : 0[0] -> 1[1] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Connected all rings
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Connected all rings
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Connected all rings
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Connected all rings
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Connected all rings
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Channel 00/0 : 4[4] -> 3[3] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Channel 01/0 : 4[4] -> 3[3] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Channel 02/0 : 3[3] -> 2[2] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Channel 02/0 : 2[2] -> 1[1] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Channel 03/0 : 3[3] -> 2[2] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Channel 03/0 : 2[2] -> 1[1] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Channel 04/0 : 3[3] -> 2[2] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Channel 04/0 : 2[2] -> 1[1] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Channel 05/0 : 3[3] -> 2[2] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Channel 02/0 : 4[4] -> 3[3] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Channel 05/0 : 2[2] -> 1[1] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Channel 06/0 : 3[3] -> 2[2] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Channel 03/0 : 4[4] -> 3[3] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Channel 06/0 : 2[2] -> 1[1] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Channel 07/0 : 3[3] -> 2[2] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Channel 04/0 : 4[4] -> 3[3] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Channel 08/0 : 3[3] -> 2[2] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Channel 09/0 : 3[3] -> 2[2] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Channel 07/0 : 2[2] -> 1[1] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Channel 10/0 : 3[3] -> 2[2] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Channel 05/0 : 4[4] -> 3[3] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Channel 08/0 : 2[2] -> 1[1] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Channel 11/0 : 3[3] -> 2[2] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Channel 06/0 : 4[4] -> 3[3] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Channel 04/0 : 1[1] -> 0[0] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Channel 09/0 : 2[2] -> 1[1] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Channel 12/0 : 3[3] -> 2[2] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Connected all rings
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Channel 00/0 : 7[7] -> 6[6] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Channel 07/0 : 4[4] -> 3[3] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Connected all rings
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Connected all rings
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Channel 10/0 : 2[2] -> 1[1] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Channel 05/0 : 1[1] -> 0[0] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Channel 13/0 : 3[3] -> 2[2] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Channel 08/0 : 4[4] -> 3[3] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Channel 11/0 : 2[2] -> 1[1] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Channel 06/0 : 1[1] -> 0[0] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Channel 14/0 : 3[3] -> 2[2] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Channel 09/0 : 4[4] -> 3[3] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Channel 12/0 : 2[2] -> 1[1] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Channel 07/0 : 1[1] -> 0[0] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Channel 15/0 : 3[3] -> 2[2] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Channel 10/0 : 4[4] -> 3[3] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Channel 13/0 : 2[2] -> 1[1] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Channel 08/0 : 1[1] -> 0[0] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Channel 16/0 : 3[3] -> 2[2] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Channel 11/0 : 4[4] -> 3[3] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Channel 14/0 : 2[2] -> 1[1] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Channel 09/0 : 1[1] -> 0[0] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Channel 17/0 : 3[3] -> 2[2] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Channel 12/0 : 4[4] -> 3[3] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Channel 15/0 : 2[2] -> 1[1] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Channel 01/0 : 7[7] -> 6[6] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Channel 10/0 : 1[1] -> 0[0] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Channel 18/0 : 3[3] -> 2[2] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Channel 13/0 : 4[4] -> 3[3] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Channel 16/0 : 2[2] -> 1[1] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Channel 02/0 : 7[7] -> 6[6] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Channel 11/0 : 1[1] -> 0[0] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Channel 19/0 : 3[3] -> 2[2] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Channel 14/0 : 4[4] -> 3[3] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Channel 17/0 : 2[2] -> 1[1] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Channel 03/0 : 7[7] -> 6[6] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Channel 12/0 : 1[1] -> 0[0] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Channel 20/0 : 3[3] -> 2[2] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Channel 15/0 : 4[4] -> 3[3] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Channel 18/0 : 2[2] -> 1[1] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Channel 04/0 : 7[7] -> 6[6] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Channel 13/0 : 1[1] -> 0[0] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Channel 21/0 : 3[3] -> 2[2] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Channel 16/0 : 4[4] -> 3[3] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Channel 19/0 : 2[2] -> 1[1] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Channel 05/0 : 7[7] -> 6[6] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Channel 14/0 : 1[1] -> 0[0] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Channel 22/0 : 3[3] -> 2[2] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Channel 17/0 : 4[4] -> 3[3] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Channel 20/0 : 2[2] -> 1[1] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Channel 06/0 : 7[7] -> 6[6] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Channel 15/0 : 1[1] -> 0[0] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Channel 23/0 : 3[3] -> 2[2] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Channel 18/0 : 4[4] -> 3[3] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Channel 21/0 : 2[2] -> 1[1] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Channel 07/0 : 7[7] -> 6[6] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Channel 16/0 : 1[1] -> 0[0] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Channel 19/0 : 4[4] -> 3[3] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Channel 22/0 : 2[2] -> 1[1] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Channel 08/0 : 7[7] -> 6[6] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Channel 17/0 : 1[1] -> 0[0] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Channel 20/0 : 4[4] -> 3[3] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Channel 23/0 : 2[2] -> 1[1] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Channel 09/0 : 7[7] -> 6[6] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Channel 18/0 : 1[1] -> 0[0] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Channel 21/0 : 4[4] -> 3[3] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Channel 10/0 : 7[7] -> 6[6] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Channel 19/0 : 1[1] -> 0[0] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Channel 22/0 : 4[4] -> 3[3] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Channel 11/0 : 7[7] -> 6[6] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Channel 20/0 : 1[1] -> 0[0] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Channel 23/0 : 4[4] -> 3[3] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Channel 12/0 : 7[7] -> 6[6] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Channel 21/0 : 1[1] -> 0[0] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Channel 22/0 : 1[1] -> 0[0] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Channel 13/0 : 7[7] -> 6[6] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Channel 23/0 : 1[1] -> 0[0] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Channel 14/0 : 7[7] -> 6[6] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Channel 15/0 : 7[7] -> 6[6] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Channel 16/0 : 7[7] -> 6[6] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Channel 17/0 : 7[7] -> 6[6] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Channel 18/0 : 7[7] -> 6[6] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Channel 19/0 : 7[7] -> 6[6] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Channel 00/0 : 6[6] -> 5[5] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Channel 20/0 : 7[7] -> 6[6] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Channel 01/0 : 6[6] -> 5[5] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Channel 21/0 : 7[7] -> 6[6] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Channel 02/0 : 6[6] -> 5[5] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Channel 22/0 : 7[7] -> 6[6] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Channel 03/0 : 6[6] -> 5[5] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Channel 23/0 : 7[7] -> 6[6] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Channel 04/0 : 6[6] -> 5[5] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Channel 00/0 : 5[5] -> 4[4] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Channel 05/0 : 6[6] -> 5[5] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Channel 01/0 : 5[5] -> 4[4] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Channel 02/0 : 5[5] -> 4[4] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Channel 06/0 : 6[6] -> 5[5] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Channel 03/0 : 5[5] -> 4[4] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Channel 07/0 : 6[6] -> 5[5] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Channel 04/0 : 5[5] -> 4[4] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Channel 08/0 : 6[6] -> 5[5] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Channel 05/0 : 5[5] -> 4[4] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Channel 09/0 : 6[6] -> 5[5] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Channel 06/0 : 5[5] -> 4[4] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Channel 10/0 : 6[6] -> 5[5] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Channel 11/0 : 6[6] -> 5[5] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Channel 07/0 : 5[5] -> 4[4] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Channel 08/0 : 5[5] -> 4[4] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Channel 09/0 : 5[5] -> 4[4] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Channel 12/0 : 6[6] -> 5[5] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Channel 10/0 : 5[5] -> 4[4] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Channel 13/0 : 6[6] -> 5[5] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Channel 11/0 : 5[5] -> 4[4] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Channel 12/0 : 5[5] -> 4[4] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Channel 13/0 : 5[5] -> 4[4] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Channel 14/0 : 6[6] -> 5[5] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Channel 14/0 : 5[5] -> 4[4] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Channel 15/0 : 6[6] -> 5[5] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Channel 16/0 : 6[6] -> 5[5] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Channel 15/0 : 5[5] -> 4[4] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Channel 17/0 : 6[6] -> 5[5] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Channel 16/0 : 5[5] -> 4[4] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Channel 18/0 : 6[6] -> 5[5] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Channel 17/0 : 5[5] -> 4[4] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Channel 18/0 : 5[5] -> 4[4] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Channel 19/0 : 5[5] -> 4[4] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Channel 19/0 : 6[6] -> 5[5] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Channel 20/0 : 6[6] -> 5[5] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Channel 20/0 : 5[5] -> 4[4] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Channel 21/0 : 6[6] -> 5[5] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Channel 21/0 : 5[5] -> 4[4] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Channel 22/0 : 6[6] -> 5[5] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Channel 22/0 : 5[5] -> 4[4] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Channel 23/0 : 6[6] -> 5[5] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Channel 23/0 : 5[5] -> 4[4] via P2P/CUMEM/read
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO Connected all trees
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO Connected all trees
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO Connected all trees
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO Connected all trees
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO Connected all trees
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO Connected all trees
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO Connected all trees
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO Connected all trees
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO TUNER/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434421:1435234 [7] NCCL INFO ncclCommInitRank comm 0x97beaf0 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId da000 commId 0xfcdc102c68dfe030 - Init COMPLETE
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO TUNER/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435228 [5] NCCL INFO ncclCommInitRank comm 0x10185690 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId 91000 commId 0xfcdc102c68dfe030 - Init COMPLETE
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO TUNER/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO TUNER/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435231 [3] NCCL INFO ncclCommInitRank comm 0xf2deb10 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 53000 commId 0xfcdc102c68dfe030 - Init COMPLETE
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435233 [1] NCCL INFO ncclCommInitRank comm 0xf31c9a0 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 15000 commId 0xfcdc102c68dfe030 - Init COMPLETE
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO TUNER/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435232 [6] NCCL INFO ncclCommInitRank comm 0xf8a4840 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId d6000 commId 0xfcdc102c68dfe030 - Init COMPLETE
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO TUNER/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435230 [4] NCCL INFO ncclCommInitRank comm 0xef68550 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId 8c000 commId 0xfcdc102c68dfe030 - Init COMPLETE
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO TUNER/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435227 [0] NCCL INFO ncclCommInitRank comm 0xe5db810 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId f000 commId 0xfcdc102c68dfe030 - Init COMPLETE
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO TUNER/Plugin: Plugin load returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory : when loading libnccl-tuner.so
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO TUNER/Plugin: Using internal tuner plugin.
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435229 [2] NCCL INFO ncclCommInitRank comm 0xed136d0 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 50000 commId 0xfcdc102c68dfe030 - Init COMPLETE
7: INFO    25-08-06 20:45:56.830141 - 0:00:15 - [InterProcessComm.sync_model_loaded] Rank 7 sync complete
7: INFO    25-08-06 20:45:56.830469 - 0:00:15 - [DISTILL] Step 0: Waiting for batch from training ranks
7: INFO    25-08-06 20:45:56.830533 - 0:00:15 - [InterProcessComm.gather_batch] Rank 7 starting gather
7: INFO    25-08-06 20:45:56.830569 - 0:00:15 - [InterProcessComm.gather_batch] Input shape: None
7: INFO    25-08-06 20:45:56.833070 - 0:00:15 - [InterProcessComm.gather_batch] Distill rank receiving batches of shape (42, 0)
1: WARNING 25-08-06 20:45:56.979606 - 0:00:15 - Model parameter layers.0.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:56.979606 - 0:00:15 - Model parameter layers.0.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:56.980381 - 0:00:15 - Model parameter layers.0.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:56.980381 - 0:00:15 - Model parameter layers.0.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:56.980320 - 0:00:16 - Model parameter layers.0.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:56.980320 - 0:00:16 - Model parameter layers.0.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:56.980964 - 0:00:15 - Model parameter layers.0.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:56.980964 - 0:00:15 - Model parameter layers.0.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:56.981050 - 0:00:16 - Model parameter layers.0.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:56.981050 - 0:00:16 - Model parameter layers.0.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:56.981618 - 0:00:16 - Model parameter layers.0.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:56.981618 - 0:00:16 - Model parameter layers.0.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:56.984424 - 0:00:15 - Model parameter layers.1.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:56.984424 - 0:00:15 - Model parameter layers.1.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:56.984937 - 0:00:16 - Model parameter layers.1.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:56.984937 - 0:00:16 - Model parameter layers.1.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:56.985000 - 0:00:15 - Model parameter layers.1.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:56.985000 - 0:00:15 - Model parameter layers.1.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:56.984965 - 0:00:15 - Model parameter layers.0.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:56.984965 - 0:00:15 - Model parameter layers.0.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:56.985486 - 0:00:16 - Model parameter layers.1.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:56.985486 - 0:00:16 - Model parameter layers.1.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:56.985573 - 0:00:15 - Model parameter layers.1.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:56.985573 - 0:00:15 - Model parameter layers.1.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:56.985885 - 0:00:15 - Model parameter layers.0.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:56.985885 - 0:00:15 - Model parameter layers.0.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:56.986066 - 0:00:16 - Model parameter layers.1.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:56.986066 - 0:00:16 - Model parameter layers.1.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:56.986560 - 0:00:15 - Model parameter layers.0.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:56.986560 - 0:00:15 - Model parameter layers.0.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:56.988504 - 0:00:16 - Model parameter layers.2.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:56.988504 - 0:00:16 - Model parameter layers.2.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:56.988863 - 0:00:16 - Model parameter layers.2.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:56.988863 - 0:00:16 - Model parameter layers.2.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:56.989077 - 0:00:16 - Model parameter layers.2.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:56.989077 - 0:00:16 - Model parameter layers.2.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:56.989409 - 0:00:16 - Model parameter layers.2.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:56.989409 - 0:00:16 - Model parameter layers.2.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:56.989639 - 0:00:16 - Model parameter layers.2.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:56.989639 - 0:00:16 - Model parameter layers.2.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:56.989961 - 0:00:16 - Model parameter layers.2.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:56.989961 - 0:00:16 - Model parameter layers.2.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:56.990106 - 0:00:15 - Model parameter layers.1.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:56.990106 - 0:00:15 - Model parameter layers.1.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:56.990713 - 0:00:15 - Model parameter layers.1.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:56.990713 - 0:00:15 - Model parameter layers.1.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:56.991333 - 0:00:15 - Model parameter layers.1.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:56.991333 - 0:00:15 - Model parameter layers.1.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:56.992486 - 0:00:16 - Model parameter layers.3.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:56.992486 - 0:00:16 - Model parameter layers.3.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:56.992745 - 0:00:16 - Model parameter layers.3.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:56.992745 - 0:00:16 - Model parameter layers.3.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:56.993052 - 0:00:16 - Model parameter layers.3.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:56.993052 - 0:00:16 - Model parameter layers.3.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:56.993290 - 0:00:16 - Model parameter layers.3.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:56.993290 - 0:00:16 - Model parameter layers.3.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:56.993616 - 0:00:16 - Model parameter layers.3.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:56.993616 - 0:00:16 - Model parameter layers.3.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:56.993837 - 0:00:16 - Model parameter layers.3.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:56.993837 - 0:00:16 - Model parameter layers.3.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:56.994262 - 0:00:15 - Model parameter layers.2.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:56.994262 - 0:00:15 - Model parameter layers.2.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:56.994864 - 0:00:15 - Model parameter layers.2.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:56.994864 - 0:00:15 - Model parameter layers.2.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:56.995454 - 0:00:15 - Model parameter layers.2.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:56.995454 - 0:00:15 - Model parameter layers.2.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:56.996486 - 0:00:16 - Model parameter layers.4.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:56.996486 - 0:00:16 - Model parameter layers.4.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:56.996642 - 0:00:16 - Model parameter layers.4.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:56.996642 - 0:00:16 - Model parameter layers.4.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:56.997054 - 0:00:16 - Model parameter layers.4.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:56.997054 - 0:00:16 - Model parameter layers.4.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:56.997186 - 0:00:16 - Model parameter layers.4.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:56.997186 - 0:00:16 - Model parameter layers.4.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:56.997617 - 0:00:16 - Model parameter layers.4.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:56.997617 - 0:00:16 - Model parameter layers.4.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:56.997737 - 0:00:16 - Model parameter layers.4.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:56.997737 - 0:00:16 - Model parameter layers.4.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:56.998413 - 0:00:15 - Model parameter layers.3.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:56.998413 - 0:00:15 - Model parameter layers.3.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:56.999027 - 0:00:15 - Model parameter layers.3.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:56.999027 - 0:00:15 - Model parameter layers.3.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:56.999631 - 0:00:15 - Model parameter layers.3.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:56.999631 - 0:00:15 - Model parameter layers.3.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:57.000519 - 0:00:16 - Model parameter layers.5.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:57.000530 - 0:00:16 - Model parameter layers.5.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:57.000519 - 0:00:16 - Model parameter layers.5.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:57.000530 - 0:00:16 - Model parameter layers.5.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:57.001082 - 0:00:16 - Model parameter layers.5.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:57.001088 - 0:00:16 - Model parameter layers.5.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:57.001082 - 0:00:16 - Model parameter layers.5.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:57.001088 - 0:00:16 - Model parameter layers.5.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:57.001654 - 0:00:16 - Model parameter layers.5.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:57.001672 - 0:00:16 - Model parameter layers.5.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:57.001654 - 0:00:16 - Model parameter layers.5.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:57.001672 - 0:00:16 - Model parameter layers.5.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:57.002594 - 0:00:15 - Model parameter layers.4.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:57.002594 - 0:00:15 - Model parameter layers.4.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:57.003201 - 0:00:15 - Model parameter layers.4.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:57.003201 - 0:00:15 - Model parameter layers.4.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:57.003834 - 0:00:15 - Model parameter layers.4.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:57.003834 - 0:00:15 - Model parameter layers.4.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:57.004439 - 0:00:16 - Model parameter layers.6.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:57.004439 - 0:00:16 - Model parameter layers.6.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:57.004526 - 0:00:16 - Model parameter layers.6.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:57.004526 - 0:00:16 - Model parameter layers.6.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:57.004993 - 0:00:16 - Model parameter layers.6.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:57.004993 - 0:00:16 - Model parameter layers.6.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:57.005095 - 0:00:16 - Model parameter layers.6.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:57.005095 - 0:00:16 - Model parameter layers.6.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:57.005535 - 0:00:16 - Model parameter layers.6.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:57.005535 - 0:00:16 - Model parameter layers.6.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:57.005659 - 0:00:16 - Model parameter layers.6.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:57.005659 - 0:00:16 - Model parameter layers.6.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:57.006775 - 0:00:15 - Model parameter layers.5.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:57.006775 - 0:00:15 - Model parameter layers.5.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:57.007374 - 0:00:15 - Model parameter layers.5.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:57.007374 - 0:00:15 - Model parameter layers.5.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:57.007971 - 0:00:15 - Model parameter layers.5.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:57.007971 - 0:00:15 - Model parameter layers.5.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:57.008331 - 0:00:16 - Model parameter layers.7.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:57.008331 - 0:00:16 - Model parameter layers.7.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:57.008531 - 0:00:16 - Model parameter layers.7.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:57.008531 - 0:00:16 - Model parameter layers.7.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:57.008880 - 0:00:16 - Model parameter layers.7.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:57.008880 - 0:00:16 - Model parameter layers.7.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:57.009099 - 0:00:16 - Model parameter layers.7.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:57.009099 - 0:00:16 - Model parameter layers.7.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:57.009419 - 0:00:16 - Model parameter layers.7.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:57.009419 - 0:00:16 - Model parameter layers.7.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:57.009665 - 0:00:16 - Model parameter layers.7.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:57.009665 - 0:00:16 - Model parameter layers.7.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:57.010917 - 0:00:15 - Model parameter layers.6.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:57.010917 - 0:00:15 - Model parameter layers.6.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:57.011519 - 0:00:15 - Model parameter layers.6.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:57.011519 - 0:00:15 - Model parameter layers.6.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:57.012151 - 0:00:15 - Model parameter layers.6.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:57.012151 - 0:00:15 - Model parameter layers.6.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:57.012195 - 0:00:16 - Model parameter layers.8.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:57.012195 - 0:00:16 - Model parameter layers.8.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:57.012533 - 0:00:16 - Model parameter layers.8.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:57.012533 - 0:00:16 - Model parameter layers.8.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:57.012764 - 0:00:16 - Model parameter layers.8.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:57.012764 - 0:00:16 - Model parameter layers.8.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:57.013105 - 0:00:16 - Model parameter layers.8.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:57.013105 - 0:00:16 - Model parameter layers.8.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:57.013305 - 0:00:16 - Model parameter layers.8.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:57.013305 - 0:00:16 - Model parameter layers.8.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:57.013674 - 0:00:16 - Model parameter layers.8.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:57.013674 - 0:00:16 - Model parameter layers.8.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:57.015087 - 0:00:15 - Model parameter layers.7.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:57.015087 - 0:00:15 - Model parameter layers.7.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:57.015697 - 0:00:15 - Model parameter layers.7.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:57.015697 - 0:00:15 - Model parameter layers.7.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:57.016099 - 0:00:16 - Model parameter layers.9.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:57.016099 - 0:00:16 - Model parameter layers.9.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:57.016285 - 0:00:15 - Model parameter layers.7.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:57.016285 - 0:00:15 - Model parameter layers.7.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:57.016555 - 0:00:16 - Model parameter layers.9.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:57.016555 - 0:00:16 - Model parameter layers.9.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:57.016658 - 0:00:16 - Model parameter layers.9.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:57.016658 - 0:00:16 - Model parameter layers.9.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:57.017128 - 0:00:16 - Model parameter layers.9.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:57.017128 - 0:00:16 - Model parameter layers.9.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:57.017222 - 0:00:16 - Model parameter layers.9.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:57.017222 - 0:00:16 - Model parameter layers.9.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:57.017683 - 0:00:16 - Model parameter layers.9.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:57.017683 - 0:00:16 - Model parameter layers.9.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:57.019274 - 0:00:15 - Model parameter layers.8.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:57.019274 - 0:00:15 - Model parameter layers.8.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:57.019900 - 0:00:15 - Model parameter layers.8.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:57.019900 - 0:00:15 - Model parameter layers.8.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:57.020098 - 0:00:16 - Model parameter layers.10.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:57.020098 - 0:00:16 - Model parameter layers.10.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:57.020498 - 0:00:15 - Model parameter layers.8.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:57.020498 - 0:00:15 - Model parameter layers.8.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:57.020533 - 0:00:16 - Model parameter layers.10.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:57.020533 - 0:00:16 - Model parameter layers.10.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:57.020658 - 0:00:16 - Model parameter layers.10.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:57.020658 - 0:00:16 - Model parameter layers.10.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:57.021100 - 0:00:16 - Model parameter layers.10.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:57.021100 - 0:00:16 - Model parameter layers.10.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:57.021200 - 0:00:16 - Model parameter layers.10.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:57.021200 - 0:00:16 - Model parameter layers.10.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:57.021666 - 0:00:16 - Model parameter layers.10.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:57.021666 - 0:00:16 - Model parameter layers.10.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:57.023478 - 0:00:15 - Model parameter layers.9.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:57.023478 - 0:00:15 - Model parameter layers.9.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:57.023972 - 0:00:16 - Model parameter layers.11.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:57.023972 - 0:00:16 - Model parameter layers.11.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:57.024088 - 0:00:15 - Model parameter layers.9.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:57.024088 - 0:00:15 - Model parameter layers.9.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:57.024514 - 0:00:16 - Model parameter layers.11.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:57.024525 - 0:00:16 - Model parameter layers.11.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:57.024514 - 0:00:16 - Model parameter layers.11.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:57.024525 - 0:00:16 - Model parameter layers.11.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:57.024684 - 0:00:15 - Model parameter layers.9.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:57.024684 - 0:00:15 - Model parameter layers.9.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:57.025064 - 0:00:16 - Model parameter layers.11.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: WARNING 25-08-06 20:45:57.025064 - 0:00:16 - Model parameter layers.11.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:57.025091 - 0:00:16 - Model parameter layers.11.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:57.025091 - 0:00:16 - Model parameter layers.11.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:57.025654 - 0:00:16 - Model parameter layers.11.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
1: WARNING 25-08-06 20:45:57.025654 - 0:00:16 - Model parameter layers.11.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:57.027620 - 0:00:15 - Model parameter layers.10.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:57.027620 - 0:00:15 - Model parameter layers.10.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:57.028221 - 0:00:15 - Model parameter layers.10.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:57.028221 - 0:00:15 - Model parameter layers.10.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:57.028819 - 0:00:15 - Model parameter layers.10.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:57.028819 - 0:00:15 - Model parameter layers.10.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:57.031771 - 0:00:15 - Model parameter layers.11.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:57.031771 - 0:00:15 - Model parameter layers.11.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:57.032394 - 0:00:15 - Model parameter layers.11.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:57.032394 - 0:00:15 - Model parameter layers.11.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:57.032992 - 0:00:15 - Model parameter layers.11.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
0: WARNING 25-08-06 20:45:57.032992 - 0:00:15 - Model parameter layers.11.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
2: INFO    25-08-06 20:45:57.034816 - 0:00:16 - [TRAIN] Rank 2 waiting for distillation model to load...
2: INFO    25-08-06 20:45:57.034883 - 0:00:16 - [InterProcessComm.sync_model_loaded] Rank 2 entering sync
2: INFO    25-08-06 20:45:57.035225 - 0:00:16 - [InterProcessComm.sync_model_loaded] Rank 2 sync complete
2: INFO    25-08-06 20:45:57.035282 - 0:00:16 - [TRAIN] Rank 2 received signal that distillation model is ready
2: INFO    25-08-06 20:45:57.035318 - 0:00:16 - Model size: 187,614,720 total parameters
1: INFO    25-08-06 20:45:57.035802 - 0:00:16 - [TRAIN] Rank 1 waiting for distillation model to load...
1: INFO    25-08-06 20:45:57.035870 - 0:00:16 - [InterProcessComm.sync_model_loaded] Rank 1 entering sync
1: INFO    25-08-06 20:45:57.036260 - 0:00:16 - [InterProcessComm.sync_model_loaded] Rank 1 sync complete
1: INFO    25-08-06 20:45:57.036316 - 0:00:16 - [TRAIN] Rank 1 received signal that distillation model is ready
1: INFO    25-08-06 20:45:57.036352 - 0:00:16 - Model size: 187,614,720 total parameters
0: INFO    25-08-06 20:45:57.042594 - 0:00:15 - [TRAIN] Rank 0 waiting for distillation model to load...
0: INFO    25-08-06 20:45:57.042740 - 0:00:15 - [InterProcessComm.sync_model_loaded] Rank 0 entering sync
0: INFO    25-08-06 20:45:57.043189 - 0:00:15 - [InterProcessComm.sync_model_loaded] Rank 0 sync complete
0: INFO    25-08-06 20:45:57.043287 - 0:00:15 - [TRAIN] Rank 0 received signal that distillation model is ready
0: INFO    25-08-06 20:45:57.043342 - 0:00:15 - Model size: 187,614,720 total parameters
2: INFO    25-08-06 20:45:57.048711 - 0:00:16 - GPU capacity: NVIDIA A100-SXM4-80GB (2) with 79.25GiB memory
1: INFO    25-08-06 20:45:57.048757 - 0:00:16 - GPU capacity: NVIDIA A100-SXM4-80GB (1) with 79.25GiB memory
0: INFO    25-08-06 20:45:57.048956 - 0:00:15 - GPU capacity: NVIDIA A100-SXM4-80GB (0) with 79.25GiB memory
2: INFO    25-08-06 20:45:57.051915 - 0:00:16 - GPU memory usage: NVIDIA A100-SXM4-80GB (2): 79.253662109375 GiB capacity, 1.072265625 GiB peak, 1.3529540420734205% peak
2: INFO    25-08-06 20:45:57.051976 - 0:00:16 - Starting build of optimizer...
2: INFO    25-08-06 20:45:57.052726 - 0:00:16 - Done with build of optimizer.
0: INFO    25-08-06 20:45:57.056966 - 0:00:15 - GPU memory usage: NVIDIA A100-SXM4-80GB (0): 79.253662109375 GiB capacity, 1.072265625 GiB peak, 1.3529540420734205% peak
1: INFO    25-08-06 20:45:57.056997 - 0:00:16 - GPU memory usage: NVIDIA A100-SXM4-80GB (1): 79.253662109375 GiB capacity, 1.072265625 GiB peak, 1.3529540420734205% peak
0: INFO    25-08-06 20:45:57.057065 - 0:00:15 - Starting build of optimizer...
1: INFO    25-08-06 20:45:57.057072 - 0:00:16 - Starting build of optimizer...
1: INFO    25-08-06 20:45:57.058107 - 0:00:16 - Done with build of optimizer.
0: INFO    25-08-06 20:45:57.058127 - 0:00:15 - Done with build of optimizer.
6: WARNING 25-08-06 20:45:57.153947 - 0:00:16 - Model parameter layers.0.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.153947 - 0:00:16 - Model parameter layers.0.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.154636 - 0:00:16 - Model parameter layers.0.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.154636 - 0:00:16 - Model parameter layers.0.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.155201 - 0:00:16 - Model parameter layers.0.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.155201 - 0:00:16 - Model parameter layers.0.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.158476 - 0:00:16 - Model parameter layers.1.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.158476 - 0:00:16 - Model parameter layers.1.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.159035 - 0:00:16 - Model parameter layers.1.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.159035 - 0:00:16 - Model parameter layers.1.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.159578 - 0:00:16 - Model parameter layers.1.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.159578 - 0:00:16 - Model parameter layers.1.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.162286 - 0:00:16 - Model parameter layers.2.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.162286 - 0:00:16 - Model parameter layers.2.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.162841 - 0:00:16 - Model parameter layers.2.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.162841 - 0:00:16 - Model parameter layers.2.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.163381 - 0:00:16 - Model parameter layers.2.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.163381 - 0:00:16 - Model parameter layers.2.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.166118 - 0:00:16 - Model parameter layers.3.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.166118 - 0:00:16 - Model parameter layers.3.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.166676 - 0:00:16 - Model parameter layers.3.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.166676 - 0:00:16 - Model parameter layers.3.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.167217 - 0:00:16 - Model parameter layers.3.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.167217 - 0:00:16 - Model parameter layers.3.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.169916 - 0:00:16 - Model parameter layers.4.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.169916 - 0:00:16 - Model parameter layers.4.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.170460 - 0:00:16 - Model parameter layers.4.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.170460 - 0:00:16 - Model parameter layers.4.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.171008 - 0:00:16 - Model parameter layers.4.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.171008 - 0:00:16 - Model parameter layers.4.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.173707 - 0:00:16 - Model parameter layers.5.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.173707 - 0:00:16 - Model parameter layers.5.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.174250 - 0:00:16 - Model parameter layers.5.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.174250 - 0:00:16 - Model parameter layers.5.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.174799 - 0:00:16 - Model parameter layers.5.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.174799 - 0:00:16 - Model parameter layers.5.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.177518 - 0:00:16 - Model parameter layers.6.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.177518 - 0:00:16 - Model parameter layers.6.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.178070 - 0:00:16 - Model parameter layers.6.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.178070 - 0:00:16 - Model parameter layers.6.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.178616 - 0:00:16 - Model parameter layers.6.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.178616 - 0:00:16 - Model parameter layers.6.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.181303 - 0:00:16 - Model parameter layers.7.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.181303 - 0:00:16 - Model parameter layers.7.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.181854 - 0:00:16 - Model parameter layers.7.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.181854 - 0:00:16 - Model parameter layers.7.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.182390 - 0:00:16 - Model parameter layers.7.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.182390 - 0:00:16 - Model parameter layers.7.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.185086 - 0:00:16 - Model parameter layers.8.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.185086 - 0:00:16 - Model parameter layers.8.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.185657 - 0:00:16 - Model parameter layers.8.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.185657 - 0:00:16 - Model parameter layers.8.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.186199 - 0:00:16 - Model parameter layers.8.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.186199 - 0:00:16 - Model parameter layers.8.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.188903 - 0:00:16 - Model parameter layers.9.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.188903 - 0:00:16 - Model parameter layers.9.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.189447 - 0:00:16 - Model parameter layers.9.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.189447 - 0:00:16 - Model parameter layers.9.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.189996 - 0:00:16 - Model parameter layers.9.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.189996 - 0:00:16 - Model parameter layers.9.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.192704 - 0:00:16 - Model parameter layers.10.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.192704 - 0:00:16 - Model parameter layers.10.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.193245 - 0:00:16 - Model parameter layers.10.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.193245 - 0:00:16 - Model parameter layers.10.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.193793 - 0:00:16 - Model parameter layers.10.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.193793 - 0:00:16 - Model parameter layers.10.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.196506 - 0:00:16 - Model parameter layers.11.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.196506 - 0:00:16 - Model parameter layers.11.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.197057 - 0:00:16 - Model parameter layers.11.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.197057 - 0:00:16 - Model parameter layers.11.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.197596 - 0:00:16 - Model parameter layers.11.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: WARNING 25-08-06 20:45:57.197596 - 0:00:16 - Model parameter layers.11.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.198636 - 0:00:16 - Model parameter layers.0.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.198636 - 0:00:16 - Model parameter layers.0.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.199295 - 0:00:16 - Model parameter layers.0.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.199295 - 0:00:16 - Model parameter layers.0.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.199862 - 0:00:16 - Model parameter layers.0.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.199862 - 0:00:16 - Model parameter layers.0.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.203332 - 0:00:16 - Model parameter layers.1.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.203332 - 0:00:16 - Model parameter layers.1.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.203903 - 0:00:16 - Model parameter layers.1.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.203903 - 0:00:16 - Model parameter layers.1.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.204447 - 0:00:16 - Model parameter layers.1.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.204447 - 0:00:16 - Model parameter layers.1.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: INFO    25-08-06 20:45:57.206610 - 0:00:16 - [TRAIN] Rank 6 waiting for distillation model to load...
6: INFO    25-08-06 20:45:57.206667 - 0:00:16 - [InterProcessComm.sync_model_loaded] Rank 6 entering sync
6: INFO    25-08-06 20:45:57.207004 - 0:00:16 - [InterProcessComm.sync_model_loaded] Rank 6 sync complete
3: WARNING 25-08-06 20:45:57.206934 - 0:00:16 - Model parameter layers.0.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: INFO    25-08-06 20:45:57.207060 - 0:00:16 - [TRAIN] Rank 6 received signal that distillation model is ready
3: WARNING 25-08-06 20:45:57.206934 - 0:00:16 - Model parameter layers.0.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
6: INFO    25-08-06 20:45:57.207098 - 0:00:16 - Model size: 187,614,720 total parameters
5: WARNING 25-08-06 20:45:57.207248 - 0:00:16 - Model parameter layers.2.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.207248 - 0:00:16 - Model parameter layers.2.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.207609 - 0:00:16 - Model parameter layers.0.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.207609 - 0:00:16 - Model parameter layers.0.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.207812 - 0:00:16 - Model parameter layers.2.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.207812 - 0:00:16 - Model parameter layers.2.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.208189 - 0:00:16 - Model parameter layers.0.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.208189 - 0:00:16 - Model parameter layers.0.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.208355 - 0:00:16 - Model parameter layers.2.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.208355 - 0:00:16 - Model parameter layers.2.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.209457 - 0:00:16 - Model parameter layers.0.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.209457 - 0:00:16 - Model parameter layers.0.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.210088 - 0:00:16 - Model parameter layers.0.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.210088 - 0:00:16 - Model parameter layers.0.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.210657 - 0:00:16 - Model parameter layers.0.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.210657 - 0:00:16 - Model parameter layers.0.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.211166 - 0:00:16 - Model parameter layers.3.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.211166 - 0:00:16 - Model parameter layers.3.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.211658 - 0:00:16 - Model parameter layers.1.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.211658 - 0:00:16 - Model parameter layers.1.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.211726 - 0:00:16 - Model parameter layers.3.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.211726 - 0:00:16 - Model parameter layers.3.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.212229 - 0:00:16 - Model parameter layers.1.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.212229 - 0:00:16 - Model parameter layers.1.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.212265 - 0:00:16 - Model parameter layers.3.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.212265 - 0:00:16 - Model parameter layers.3.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.212798 - 0:00:16 - Model parameter layers.1.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.212798 - 0:00:16 - Model parameter layers.1.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.213902 - 0:00:16 - Model parameter layers.1.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.213902 - 0:00:16 - Model parameter layers.1.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.214452 - 0:00:16 - Model parameter layers.1.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.214452 - 0:00:16 - Model parameter layers.1.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.215008 - 0:00:16 - Model parameter layers.1.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.215008 - 0:00:16 - Model parameter layers.1.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.215057 - 0:00:16 - Model parameter layers.4.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.215057 - 0:00:16 - Model parameter layers.4.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.215608 - 0:00:16 - Model parameter layers.4.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.215608 - 0:00:16 - Model parameter layers.4.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.215669 - 0:00:16 - Model parameter layers.2.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.215669 - 0:00:16 - Model parameter layers.2.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.216149 - 0:00:16 - Model parameter layers.4.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.216149 - 0:00:16 - Model parameter layers.4.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.216231 - 0:00:16 - Model parameter layers.2.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.216231 - 0:00:16 - Model parameter layers.2.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.216798 - 0:00:16 - Model parameter layers.2.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.216798 - 0:00:16 - Model parameter layers.2.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.217779 - 0:00:16 - Model parameter layers.2.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.217779 - 0:00:16 - Model parameter layers.2.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.218344 - 0:00:16 - Model parameter layers.2.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.218344 - 0:00:16 - Model parameter layers.2.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.218898 - 0:00:16 - Model parameter layers.2.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.218898 - 0:00:16 - Model parameter layers.2.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.218932 - 0:00:16 - Model parameter layers.5.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.218932 - 0:00:16 - Model parameter layers.5.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.219474 - 0:00:16 - Model parameter layers.5.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.219474 - 0:00:16 - Model parameter layers.5.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.219699 - 0:00:16 - Model parameter layers.3.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.219699 - 0:00:16 - Model parameter layers.3.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.220021 - 0:00:16 - Model parameter layers.5.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.220021 - 0:00:16 - Model parameter layers.5.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.220263 - 0:00:16 - Model parameter layers.3.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.220263 - 0:00:16 - Model parameter layers.3.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.220829 - 0:00:16 - Model parameter layers.3.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.220829 - 0:00:16 - Model parameter layers.3.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.221681 - 0:00:16 - Model parameter layers.3.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.221681 - 0:00:16 - Model parameter layers.3.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.222224 - 0:00:16 - Model parameter layers.3.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.222224 - 0:00:16 - Model parameter layers.3.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.222774 - 0:00:16 - Model parameter layers.3.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.222774 - 0:00:16 - Model parameter layers.3.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.222841 - 0:00:16 - Model parameter layers.6.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.222841 - 0:00:16 - Model parameter layers.6.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.223386 - 0:00:16 - Model parameter layers.6.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.223386 - 0:00:16 - Model parameter layers.6.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.223712 - 0:00:16 - Model parameter layers.4.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.223712 - 0:00:16 - Model parameter layers.4.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.223936 - 0:00:16 - Model parameter layers.6.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.223936 - 0:00:16 - Model parameter layers.6.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.224278 - 0:00:16 - Model parameter layers.4.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.224278 - 0:00:16 - Model parameter layers.4.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.224844 - 0:00:16 - Model parameter layers.4.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.224844 - 0:00:16 - Model parameter layers.4.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.225546 - 0:00:16 - Model parameter layers.4.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.225546 - 0:00:16 - Model parameter layers.4.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.226102 - 0:00:16 - Model parameter layers.4.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.226102 - 0:00:16 - Model parameter layers.4.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.226648 - 0:00:16 - Model parameter layers.4.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.226648 - 0:00:16 - Model parameter layers.4.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.226751 - 0:00:16 - Model parameter layers.7.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.226751 - 0:00:16 - Model parameter layers.7.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.227294 - 0:00:16 - Model parameter layers.7.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.227294 - 0:00:16 - Model parameter layers.7.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.227710 - 0:00:16 - Model parameter layers.5.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.227710 - 0:00:16 - Model parameter layers.5.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.227846 - 0:00:16 - Model parameter layers.7.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.227846 - 0:00:16 - Model parameter layers.7.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.228266 - 0:00:16 - Model parameter layers.5.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.228266 - 0:00:16 - Model parameter layers.5.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.228829 - 0:00:16 - Model parameter layers.5.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.228829 - 0:00:16 - Model parameter layers.5.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.229424 - 0:00:16 - Model parameter layers.5.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.229424 - 0:00:16 - Model parameter layers.5.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.229977 - 0:00:16 - Model parameter layers.5.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.229977 - 0:00:16 - Model parameter layers.5.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.230516 - 0:00:16 - Model parameter layers.5.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.230516 - 0:00:16 - Model parameter layers.5.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.230655 - 0:00:16 - Model parameter layers.8.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.230655 - 0:00:16 - Model parameter layers.8.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.231222 - 0:00:16 - Model parameter layers.8.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.231222 - 0:00:16 - Model parameter layers.8.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.231690 - 0:00:16 - Model parameter layers.6.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.231690 - 0:00:16 - Model parameter layers.6.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.231778 - 0:00:16 - Model parameter layers.8.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.231778 - 0:00:16 - Model parameter layers.8.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.232280 - 0:00:16 - Model parameter layers.6.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.232280 - 0:00:16 - Model parameter layers.6.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.232849 - 0:00:16 - Model parameter layers.6.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.232849 - 0:00:16 - Model parameter layers.6.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.233267 - 0:00:16 - Model parameter layers.6.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.233267 - 0:00:16 - Model parameter layers.6.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.233822 - 0:00:16 - Model parameter layers.6.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.233822 - 0:00:16 - Model parameter layers.6.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.234357 - 0:00:16 - Model parameter layers.6.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.234357 - 0:00:16 - Model parameter layers.6.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.234566 - 0:00:16 - Model parameter layers.9.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.234566 - 0:00:16 - Model parameter layers.9.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.235119 - 0:00:16 - Model parameter layers.9.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.235119 - 0:00:16 - Model parameter layers.9.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.235666 - 0:00:16 - Model parameter layers.9.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.235666 - 0:00:16 - Model parameter layers.9.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.235691 - 0:00:16 - Model parameter layers.7.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.235691 - 0:00:16 - Model parameter layers.7.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.236248 - 0:00:16 - Model parameter layers.7.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.236248 - 0:00:16 - Model parameter layers.7.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.236813 - 0:00:16 - Model parameter layers.7.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.236813 - 0:00:16 - Model parameter layers.7.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.237129 - 0:00:16 - Model parameter layers.7.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.237129 - 0:00:16 - Model parameter layers.7.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.237680 - 0:00:16 - Model parameter layers.7.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.237680 - 0:00:16 - Model parameter layers.7.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.238217 - 0:00:16 - Model parameter layers.7.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.238217 - 0:00:16 - Model parameter layers.7.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.238445 - 0:00:16 - Model parameter layers.10.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.238445 - 0:00:16 - Model parameter layers.10.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.239004 - 0:00:16 - Model parameter layers.10.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.239004 - 0:00:16 - Model parameter layers.10.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.239546 - 0:00:16 - Model parameter layers.10.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.239546 - 0:00:16 - Model parameter layers.10.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.239684 - 0:00:16 - Model parameter layers.8.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.239684 - 0:00:16 - Model parameter layers.8.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.240244 - 0:00:16 - Model parameter layers.8.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.240244 - 0:00:16 - Model parameter layers.8.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.240812 - 0:00:16 - Model parameter layers.8.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.240812 - 0:00:16 - Model parameter layers.8.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.240983 - 0:00:16 - Model parameter layers.8.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.240983 - 0:00:16 - Model parameter layers.8.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.241526 - 0:00:16 - Model parameter layers.8.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.241526 - 0:00:16 - Model parameter layers.8.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.242070 - 0:00:16 - Model parameter layers.8.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.242070 - 0:00:16 - Model parameter layers.8.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.242358 - 0:00:16 - Model parameter layers.11.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.242358 - 0:00:16 - Model parameter layers.11.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.242918 - 0:00:16 - Model parameter layers.11.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.242918 - 0:00:16 - Model parameter layers.11.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.243457 - 0:00:16 - Model parameter layers.11.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: WARNING 25-08-06 20:45:57.243457 - 0:00:16 - Model parameter layers.11.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.243700 - 0:00:16 - Model parameter layers.9.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.243700 - 0:00:16 - Model parameter layers.9.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.244262 - 0:00:16 - Model parameter layers.9.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.244262 - 0:00:16 - Model parameter layers.9.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.244824 - 0:00:16 - Model parameter layers.9.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.244833 - 0:00:16 - Model parameter layers.9.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.244824 - 0:00:16 - Model parameter layers.9.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.244833 - 0:00:16 - Model parameter layers.9.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.245375 - 0:00:16 - Model parameter layers.9.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.245375 - 0:00:16 - Model parameter layers.9.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.245921 - 0:00:16 - Model parameter layers.9.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.245921 - 0:00:16 - Model parameter layers.9.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.247682 - 0:00:16 - Model parameter layers.10.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.247682 - 0:00:16 - Model parameter layers.10.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.248242 - 0:00:16 - Model parameter layers.10.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.248242 - 0:00:16 - Model parameter layers.10.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.248683 - 0:00:16 - Model parameter layers.10.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.248683 - 0:00:16 - Model parameter layers.10.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.248805 - 0:00:16 - Model parameter layers.10.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.248805 - 0:00:16 - Model parameter layers.10.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.249236 - 0:00:16 - Model parameter layers.10.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.249236 - 0:00:16 - Model parameter layers.10.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.249787 - 0:00:16 - Model parameter layers.10.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.249787 - 0:00:16 - Model parameter layers.10.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.251655 - 0:00:16 - Model parameter layers.11.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.251655 - 0:00:16 - Model parameter layers.11.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.252213 - 0:00:16 - Model parameter layers.11.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.252213 - 0:00:16 - Model parameter layers.11.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.252541 - 0:00:16 - Model parameter layers.11.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.252541 - 0:00:16 - Model parameter layers.11.attention.wq_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: INFO    25-08-06 20:45:57.252679 - 0:00:16 - [TRAIN] Rank 5 waiting for distillation model to load...
5: INFO    25-08-06 20:45:57.252742 - 0:00:16 - [InterProcessComm.sync_model_loaded] Rank 5 entering sync
3: WARNING 25-08-06 20:45:57.252799 - 0:00:16 - Model parameter layers.11.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: WARNING 25-08-06 20:45:57.252799 - 0:00:16 - Model parameter layers.11.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.253092 - 0:00:16 - Model parameter layers.11.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: INFO    25-08-06 20:45:57.253092 - 0:00:16 - [InterProcessComm.sync_model_loaded] Rank 5 sync complete
4: WARNING 25-08-06 20:45:57.253092 - 0:00:16 - Model parameter layers.11.attention.wk_offset.w_b.weight is all zeros: it might be because of a missing initialization
5: INFO    25-08-06 20:45:57.253155 - 0:00:16 - [TRAIN] Rank 5 received signal that distillation model is ready
5: INFO    25-08-06 20:45:57.253192 - 0:00:16 - Model size: 187,614,720 total parameters
4: WARNING 25-08-06 20:45:57.253635 - 0:00:16 - Model parameter layers.11.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
4: WARNING 25-08-06 20:45:57.253635 - 0:00:16 - Model parameter layers.11.attention.wv_offset.w_b.weight is all zeros: it might be because of a missing initialization
3: INFO    25-08-06 20:45:57.262287 - 0:00:16 - [TRAIN] Rank 3 waiting for distillation model to load...
3: INFO    25-08-06 20:45:57.262344 - 0:00:16 - [InterProcessComm.sync_model_loaded] Rank 3 entering sync
3: INFO    25-08-06 20:45:57.262682 - 0:00:16 - [InterProcessComm.sync_model_loaded] Rank 3 sync complete
3: INFO    25-08-06 20:45:57.262739 - 0:00:16 - [TRAIN] Rank 3 received signal that distillation model is ready
3: INFO    25-08-06 20:45:57.262777 - 0:00:16 - Model size: 187,614,720 total parameters
4: INFO    25-08-06 20:45:57.262990 - 0:00:16 - [TRAIN] Rank 4 waiting for distillation model to load...
4: INFO    25-08-06 20:45:57.263048 - 0:00:16 - [InterProcessComm.sync_model_loaded] Rank 4 entering sync
4: INFO    25-08-06 20:45:57.263290 - 0:00:16 - [InterProcessComm.sync_model_loaded] Rank 4 sync complete
4: INFO    25-08-06 20:45:57.263346 - 0:00:16 - [TRAIN] Rank 4 received signal that distillation model is ready
4: INFO    25-08-06 20:45:57.263380 - 0:00:16 - Model size: 187,614,720 total parameters
3: INFO    25-08-06 20:45:57.269612 - 0:00:16 - GPU capacity: NVIDIA A100-SXM4-80GB (3) with 79.25GiB memory
5: INFO    25-08-06 20:45:57.272663 - 0:00:16 - GPU capacity: NVIDIA A100-SXM4-80GB (5) with 79.25GiB memory
4: INFO    25-08-06 20:45:57.272727 - 0:00:16 - GPU capacity: NVIDIA A100-SXM4-80GB (4) with 79.25GiB memory
6: INFO    25-08-06 20:45:57.272897 - 0:00:16 - GPU capacity: NVIDIA A100-SXM4-80GB (6) with 79.25GiB memory
3: INFO    25-08-06 20:45:57.272976 - 0:00:16 - GPU memory usage: NVIDIA A100-SXM4-80GB (3): 79.253662109375 GiB capacity, 1.072265625 GiB peak, 1.3529540420734205% peak
3: INFO    25-08-06 20:45:57.273030 - 0:00:16 - Starting build of optimizer...
3: INFO    25-08-06 20:45:57.273690 - 0:00:16 - Done with build of optimizer.
4: INFO    25-08-06 20:45:57.275750 - 0:00:16 - GPU memory usage: NVIDIA A100-SXM4-80GB (4): 79.253662109375 GiB capacity, 1.072265625 GiB peak, 1.3529540420734205% peak
4: INFO    25-08-06 20:45:57.275802 - 0:00:16 - Starting build of optimizer...
4: INFO    25-08-06 20:45:57.276417 - 0:00:16 - Done with build of optimizer.
5: INFO    25-08-06 20:45:57.278241 - 0:00:16 - GPU memory usage: NVIDIA A100-SXM4-80GB (5): 79.253662109375 GiB capacity, 1.072265625 GiB peak, 1.3529540420734205% peak
5: INFO    25-08-06 20:45:57.278298 - 0:00:16 - Starting build of optimizer...
6: INFO    25-08-06 20:45:57.278339 - 0:00:16 - GPU memory usage: NVIDIA A100-SXM4-80GB (6): 79.253662109375 GiB capacity, 1.072265625 GiB peak, 1.3529540420734205% peak
6: INFO    25-08-06 20:45:57.278394 - 0:00:16 - Starting build of optimizer...
5: INFO    25-08-06 20:45:57.279038 - 0:00:16 - Done with build of optimizer.
6: INFO    25-08-06 20:45:57.279052 - 0:00:16 - Done with build of optimizer.
[rank0]:[E806 20:55:57.565867775 ProcessGroupNCCL.cpp:616] [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=3, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600091 milliseconds before timing out.
[rank0]:[E806 20:55:57.566523068 ProcessGroupNCCL.cpp:1785] [PG ID 0 PG GUID 0(default_pg) Rank 0] Exception (either an error or timeout) detected by watchdog at work: 3, last enqueued NCCL work: 3, last completed NCCL work: 2.
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435250 [0] NCCL INFO [Service thread] Connection closed by localRank 0
[rank2]:[E806 20:55:57.757568621 ProcessGroupNCCL.cpp:616] [Rank 2] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=3, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600091 milliseconds before timing out.
[rank2]:[E806 20:55:57.759134751 ProcessGroupNCCL.cpp:1785] [PG ID 0 PG GUID 0(default_pg) Rank 2] Exception (either an error or timeout) detected by watchdog at work: 3, last enqueued NCCL work: 4, last completed NCCL work: 2.
[rank6]:[E806 20:55:57.764513763 ProcessGroupNCCL.cpp:616] [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=3, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600070 milliseconds before timing out.
[rank6]:[E806 20:55:57.765078233 ProcessGroupNCCL.cpp:1785] [PG ID 0 PG GUID 0(default_pg) Rank 6] Exception (either an error or timeout) detected by watchdog at work: 3, last enqueued NCCL work: 3, last completed NCCL work: 2.
[rank5]:[E806 20:55:57.765917999 ProcessGroupNCCL.cpp:616] [Rank 5] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=3, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600071 milliseconds before timing out.
[rank3]:[E806 20:55:57.766020251 ProcessGroupNCCL.cpp:616] [Rank 3] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=3, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600077 milliseconds before timing out.
[rank5]:[E806 20:55:57.766756415 ProcessGroupNCCL.cpp:1785] [PG ID 0 PG GUID 0(default_pg) Rank 5] Exception (either an error or timeout) detected by watchdog at work: 3, last enqueued NCCL work: 4, last completed NCCL work: 2.
[rank3]:[E806 20:55:57.767219674 ProcessGroupNCCL.cpp:1785] [PG ID 0 PG GUID 0(default_pg) Rank 3] Exception (either an error or timeout) detected by watchdog at work: 3, last enqueued NCCL work: 4, last completed NCCL work: 2.
[rank4]:[E806 20:55:57.767413467 ProcessGroupNCCL.cpp:616] [Rank 4] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=3, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600075 milliseconds before timing out.
[rank1]:[E806 20:55:57.767456298 ProcessGroupNCCL.cpp:616] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=3, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600082 milliseconds before timing out.
[rank4]:[E806 20:55:57.767921487 ProcessGroupNCCL.cpp:1785] [PG ID 0 PG GUID 0(default_pg) Rank 4] Exception (either an error or timeout) detected by watchdog at work: 3, last enqueued NCCL work: 4, last completed NCCL work: 2.
[rank1]:[E806 20:55:57.768335545 ProcessGroupNCCL.cpp:1785] [PG ID 0 PG GUID 0(default_pg) Rank 1] Exception (either an error or timeout) detected by watchdog at work: 3, last enqueued NCCL work: 4, last completed NCCL work: 2.
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434414:1435167 [0] NCCL INFO comm 0xe5db810 rank 0 nranks 8 cudaDev 0 busId f000 - Abort COMPLETE
[rank0]:[E806 20:55:57.776220804 ProcessGroupNCCL.cpp:1834] [PG ID 0 PG GUID 0(default_pg) Rank 0] Timeout at NCCL work: 3, last enqueued NCCL work: 3, last completed NCCL work: 2.
[rank0]:[E806 20:55:57.776234164 ProcessGroupNCCL.cpp:630] [Rank 0] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank0]:[E806 20:55:57.776241674 ProcessGroupNCCL.cpp:636] [Rank 0] To avoid data inconsistency, we are taking the entire process down.
[rank0]:[E806 20:55:57.777276884 ProcessGroupNCCL.cpp:1595] [PG ID 0 PG GUID 0(default_pg) Rank 0] Process group watchdog thread terminated with exception: [Rank 0] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=3, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600091 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:618 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7ff87c4b9446 in /opt/saturncloud/envs/lingua_250731/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x282 (0x7ff831dbea82 in /opt/saturncloud/envs/lingua_250731/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x7ff831dc5ec3 in /opt/saturncloud/envs/lingua_250731/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7ff831dc792d in /opt/saturncloud/envs/lingua_250731/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7ff87c92a5c0 in /opt/saturncloud/envs/lingua_250731/lib/python3.11/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7ff8804ddac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7ff88056f850 in /usr/lib/x86_64-linux-gnu/libc.so.6)

w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435252 [6] NCCL INFO [Service thread] Connection closed by localRank 6
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434420:1435180 [6] NCCL INFO comm 0xf8a4840 rank 6 nranks 8 cudaDev 6 busId d6000 - Abort COMPLETE
[rank6]:[E806 20:55:57.879408856 ProcessGroupNCCL.cpp:1834] [PG ID 0 PG GUID 0(default_pg) Rank 6] Timeout at NCCL work: 3, last enqueued NCCL work: 3, last completed NCCL work: 2.
[rank6]:[E806 20:55:57.879419796 ProcessGroupNCCL.cpp:630] [Rank 6] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank6]:[E806 20:55:57.879427736 ProcessGroupNCCL.cpp:636] [Rank 6] To avoid data inconsistency, we are taking the entire process down.
[rank6]:[E806 20:55:57.880451425 ProcessGroupNCCL.cpp:1595] [PG ID 0 PG GUID 0(default_pg) Rank 6] Process group watchdog thread terminated with exception: [Rank 6] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=3, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600070 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:618 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x96 (0x7feeb076c446 in /opt/saturncloud/envs/lingua_250731/lib/python3.11/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x282 (0x7fee65fbea82 in /opt/saturncloud/envs/lingua_250731/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x233 (0x7fee65fc5ec3 in /opt/saturncloud/envs/lingua_250731/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x14d (0x7fee65fc792d in /opt/saturncloud/envs/lingua_250731/lib/python3.11/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0x145c0 (0x7feeb0c145c0 in /opt/saturncloud/envs/lingua_250731/lib/python3.11/site-packages/torch/lib/libtorch.so)
frame #5: <unknown function> + 0x94ac3 (0x7feeb47c9ac3 in /usr/lib/x86_64-linux-gnu/libc.so.6)
frame #6: <unknown function> + 0x126850 (0x7feeb485b850 in /usr/lib/x86_64-linux-gnu/libc.so.6)

w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434418:1435254 [4] NCCL INFO [Service thread] Connection closed by localRank 4
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434417:1435255 [3] NCCL INFO [Service thread] Connection closed by localRank 3
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434416:1435246 [2] NCCL INFO [Service thread] Connection closed by localRank 2
4: INFO    25-08-06 20:55:57.501728 - 0:10:16 - Async dataloader started
3: INFO    25-08-06 20:55:57.501797 - 0:10:16 - Async dataloader started
4: INFO    25-08-06 20:55:57.503173 - 0:10:16 - Profiling active.  Traces will be saved at /home/jovyan/workspace/efficient_llms_dumps/distill_exps/debug_0/profiling
3: INFO    25-08-06 20:55:57.503351 - 0:10:16 - Profiling active.  Traces will be saved at /home/jovyan/workspace/efficient_llms_dumps/distill_exps/debug_0/profiling
2: INFO    25-08-06 20:55:57.504833 - 0:10:16 - Async dataloader started
2: INFO    25-08-06 20:55:57.506723 - 0:10:16 - Profiling active.  Traces will be saved at /home/jovyan/workspace/efficient_llms_dumps/distill_exps/debug_0/profiling
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434415:1435243 [1] NCCL INFO [Service thread] Connection closed by localRank 1
1: INFO    25-08-06 20:55:57.584801 - 0:10:16 - Async dataloader started
1: INFO    25-08-06 20:55:57.587106 - 0:10:16 - Profiling active.  Traces will be saved at /home/jovyan/workspace/efficient_llms_dumps/distill_exps/debug_0/profiling
w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v:1434419:1435248 [5] NCCL INFO [Service thread] Connection closed by localRank 5
5: INFO    25-08-06 20:55:57.666847 - 0:10:16 - Async dataloader started
5: INFO    25-08-06 20:55:57.668653 - 0:10:16 - Profiling active.  Traces will be saved at /home/jovyan/workspace/efficient_llms_dumps/distill_exps/debug_0/profiling
W0806 20:55:59.629000 1434340 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1434415 closing signal SIGTERM
W0806 20:55:59.632000 1434340 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1434416 closing signal SIGTERM
W0806 20:55:59.635000 1434340 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1434417 closing signal SIGTERM
W0806 20:55:59.640000 1434340 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1434418 closing signal SIGTERM
W0806 20:55:59.644000 1434340 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1434419 closing signal SIGTERM
W0806 20:55:59.646000 1434340 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1434420 closing signal SIGTERM
W0806 20:55:59.648000 1434340 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1434421 closing signal SIGTERM
/opt/saturncloud/envs/lingua_250731/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 8 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/opt/saturncloud/envs/lingua_250731/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 8 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/opt/saturncloud/envs/lingua_250731/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 8 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/opt/saturncloud/envs/lingua_250731/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 8 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
/opt/saturncloud/envs/lingua_250731/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 8 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
E0806 20:56:03.433000 1434340 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: -6) local_rank: 0 (pid: 1434414) of binary: /opt/saturncloud/envs/lingua_250731/bin/python3.11
Traceback (most recent call last):
  File "/opt/saturncloud/envs/lingua_250731/bin/torchrun", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/opt/saturncloud/envs/lingua_250731/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/opt/saturncloud/envs/lingua_250731/lib/python3.11/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/opt/saturncloud/envs/lingua_250731/lib/python3.11/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/opt/saturncloud/envs/lingua_250731/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/saturncloud/envs/lingua_250731/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError:
============================================================
apps.main.train FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-06_20:55:59
  host      : w-mucha-lingua-run-a86ecbb431d948118e583e5a9e6b89df-5ccdf5nt86v
  rank      : 0 (local_rank: 0)
  exitcode  : -6 (pid: 1434414)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 1434414
============================================================
